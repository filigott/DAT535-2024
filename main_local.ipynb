{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: simulate scrambling the original data\n",
    "# TODO: Could maybe scramble more?\n",
    "\n",
    "def scramble_data(df: pd.DataFrame):\n",
    "    scrambled_lines = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Extract each column's value\n",
    "        type_field = row['type']\n",
    "        id_field = row['id']\n",
    "        subreddit_id = row['subreddit.id']\n",
    "        subreddit_name = row['subreddit.name']\n",
    "        subreddit_nsfw = row['subreddit.nsfw']\n",
    "        created_utc = row['created_utc']\n",
    "        permalink = row['permalink']\n",
    "        # Replace '|' in body to avoid conflicts with new delimiter\n",
    "        body = row['body'].replace('|', '__PIPE__')  \n",
    "        sentiment = row['sentiment']\n",
    "        score = row['score']\n",
    "\n",
    "        # Some scramble logic here:\n",
    "        type_field = (type_field == \"comment\")  # Convert to boolean\n",
    "\n",
    "        # Create a \"scrambled\" line with the \"|\" as delimiter and moving some parts around\n",
    "        row_str = f\"{type_field}|{id_field}|{subreddit_id}|{subreddit_name}|{subreddit_nsfw}|{created_utc}|{sentiment}|{permalink}|{body}|{score}\\n\"\n",
    "\n",
    "        # Randomly break the row into two parts\n",
    "        if random.random() > 0.7:  # 30% chance to split a row into two parts\n",
    "            split_point = len(row_str) // 2\n",
    "            row_str = row_str[:split_point] + '\\n' + row_str[split_point:]\n",
    "\n",
    "        scrambled_lines.append(row_str)\n",
    "\n",
    "    scrambled_data = \"\".join(scrambled_lines)\n",
    "    return scrambled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean body text by removing both single and double quotes from the beginning and end\n",
    "def clean_body_text(body):\n",
    "    # Restore original pipe symbol\n",
    "    body = body.replace('__PIPE__', '|')\n",
    "\n",
    "    # Remove single and double quotation marks from the beginning and end of the body text\n",
    "    body_cleaned = re.sub(r\"^[\\s,]*['\\\"]|['\\\"][\\s,]*$\", '', body)\n",
    "    \n",
    "    # Optionally, collapse multiple spaces into a single space\n",
    "    body_cleaned = re.sub(r'\\s+', ' ', body_cleaned).strip()\n",
    "\n",
    "    return body_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: cleaning the scrambled data\n",
    "# TODO: This should be done in SPARK (Lecturer did something to the body field here for body text in emails)\n",
    "\n",
    "def clean_data(scrambled_data: str):\n",
    "    # Split the unstructured data into individual lines\n",
    "    lines = scrambled_data.split('\\n')\n",
    "\n",
    "    # Step 1: Rejoin broken lines\n",
    "    cleaned_lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip():  # Ignore empty lines\n",
    "            current_line += line.strip() + \" \"  # Append with a space to avoid issues\n",
    "            # Check if we have a complete line\n",
    "            if current_line.count('|') == 9:  # Exactly 10 fields\n",
    "                cleaned_lines.append(current_line.strip())  # Add completed line\n",
    "                current_line = \"\"  # Reset for the next line\n",
    "\n",
    "    # Handle any remaining line that wasn't appended\n",
    "    if current_line:\n",
    "        cleaned_lines.append(current_line.strip())\n",
    "\n",
    "    # Step 2: Parse each cleaned line back into columns\n",
    "    parsed_data = []\n",
    "    error_data = []  # List to hold error information\n",
    "\n",
    "    for i, line in enumerate(cleaned_lines):\n",
    "        fields = line.split('|')\n",
    "\n",
    "        # Ensure we have at least 10 fields before processing\n",
    "        if len(fields) < 10:\n",
    "            error_info = {\n",
    "                'line_index': i,\n",
    "                'fields_found': fields,\n",
    "                'original_line': line\n",
    "            }\n",
    "            error_data.append(error_info)\n",
    "            continue\n",
    "        \n",
    "        # Extract fields\n",
    "        type_field = \"comment\" if fields[0] == \"True\" else \"post\"  # Convert back to string\n",
    "        id_field = fields[1]\n",
    "        subreddit_id = fields[2]\n",
    "        subreddit_name = fields[3]\n",
    "        subreddit_nsfw = \"nsfw\" if fields[4] == \"True\" else \"not_nsfw\"  # Convert to string\n",
    "        created_utc = fields[5]\n",
    "        sentiment = fields[6]\n",
    "        permalink = fields[7]\n",
    "        body = clean_body_text(fields[8])\n",
    "        score = fields[9]\n",
    "\n",
    "        # Convert the timestamp to a human-readable format\n",
    "        created_utc_human = pd.to_datetime(int(created_utc), unit='s').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Create the row dictionary\n",
    "        row_dict = {\n",
    "            'type': type_field,\n",
    "            'id': id_field,\n",
    "            'subreddit_id': subreddit_id,\n",
    "            'subreddit_name': subreddit_name,\n",
    "            'subreddit_nsfw': subreddit_nsfw,\n",
    "            'created_utc': created_utc_human,\n",
    "            'sentiment': sentiment,\n",
    "            'permalink': permalink,\n",
    "            'body': body,\n",
    "            'score': score\n",
    "        }\n",
    "        parsed_data.append(row_dict)\n",
    "\n",
    "    # Optionally, return both parsed data and error data\n",
    "    return pd.DataFrame(parsed_data), error_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "NUM_ROWS = 5\n",
    "\n",
    "# Load a small subset of the CSV (for testing)\n",
    "input_file = 'data/subset/100000-reddit-covid-comments.csv'\n",
    "df = pd.read_csv(input_file).head(NUM_ROWS)  # Load first N rows for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrambled text (first 5 lines):\n",
      "\n",
      "True|hi0xdct|2qh7q|florida|False|1635191579|-0.5666|https://old.reddit.com/r/florida/comments/qf3xp4/desantis_recruiting_unvaccinated_out_of_state/hi0xdct/|&gt; COVID-19 is legitimately dangerous*\n",
      "\n",
      "*for the old and/or unhealthy.\n",
      "\n",
      "Average age of death is like 80. 76% of all deaths are 65+.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Scramble the dataset and save to a .txt file\n",
    "scrambled_text = scramble_data(df)\n",
    "scrambled_file = \"data/scrambled/scrambled-reddit-covid-comments.txt\"\n",
    "\n",
    "with open(scrambled_file, 'w') as file:\n",
    "    file.write(scrambled_text)\n",
    "\n",
    "print(f\"Scrambled text (first {NUM_ROWS} lines):\\n\")\n",
    "with open(scrambled_file, 'r') as file:\n",
    "    scrambled_data = file.readlines()\n",
    "    for line in scrambled_data[:NUM_ROWS]:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Data (first 5 rows):\n",
      "      type       id subreddit_id     subreddit_name subreddit_nsfw  \\\n",
      "0  comment  hi0xdct        2qh7q            florida       not_nsfw   \n",
      "1  comment  hi16118        2y77d           antiwork       not_nsfw   \n",
      "2  comment  hi1mkh7        2qhsa  interestingasfuck       not_nsfw   \n",
      "3  comment  hi15pqu        2z2wm      cryptomarkets       not_nsfw   \n",
      "4  comment  hi16y0z        2qqd2    greenbaypackers       not_nsfw   \n",
      "\n",
      "           created_utc sentiment  \\\n",
      "0  2021-10-25 19:52:59   -0.5666   \n",
      "1  2021-10-25 20:50:40   -0.8634   \n",
      "2  2021-10-25 22:49:17    0.4329   \n",
      "3  2021-10-25 20:48:34       0.0   \n",
      "4  2021-10-25 20:56:56       0.0   \n",
      "\n",
      "                                           permalink  \\\n",
      "0  https://old.reddit.com/r/florida/comments/qf3x...   \n",
      "1  https://old.reddit.com/r/antiwork/comments/qfg...   \n",
      "2  https://old.reddit.com/r/interestingasfuck/com...   \n",
      "3  https://old.reddit.com/r/CryptoMarkets/comment...   \n",
      "4  https://old.reddit.com/r/GreenBayPackers/comme...   \n",
      "\n",
      "                                                body score  \n",
      "0  &gt; COVID-19 is legitimately dangerous* *for ...    -1  \n",
      "1  Wtf ? What is wrong with that manager? For alm...     1  \n",
      "2  I thought the sherpas were working on that thr...     1  \n",
      "3  I mean…yea it was a gamble. But it’s paying my...     1  \n",
      "4  Fact is the “vaccine” doesn’t do anything. Fir...     0  \n",
      "Cleaned data saved to data/cleaned/cleaned-reddit-covid-comments.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the scrambled .txt file (to simulate real-world scenario)\n",
    "with open(scrambled_file, 'r') as file:\n",
    "    scrambled_data = file.read()\n",
    "\n",
    "# Clean the dataset (bring it back to a clean state)\n",
    "df_cleaned, error_data = clean_data(scrambled_data)\n",
    "\n",
    "print(f\"\\nCleaned Data (first {NUM_ROWS} rows):\")\n",
    "print(df_cleaned.head(NUM_ROWS))\n",
    "\n",
    "# Step 3: Save the cleaned data back to a Parquet file\n",
    "cleaned_output_file_parquet = \"data/cleaned/cleaned-reddit-covid-comments.parquet\"\n",
    "df_cleaned.to_parquet(cleaned_output_file_parquet, index=False)\n",
    "\n",
    "# Also save the cleaned data back to a CSV for easier comparison\n",
    "cleaned_output_file = \"data/cleaned/cleaned-reddit-covid-comments.csv\"\n",
    "df_cleaned.to_csv(cleaned_output_file, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found.\n"
     ]
    }
   ],
   "source": [
    "# Check for any errors\n",
    "if len(error_data) > 0:\n",
    "    print(f\"\\nTotal Errors Encountered: {len(error_data)}\")\n",
    "    for error in error_data:\n",
    "        print(f\"Error in line {error['line_index']}: {error['fields_found']}\")\n",
    "else:\n",
    "    print(\"No errors found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
