{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: simulate scrambling the original data\n",
    "# TODO: Could maybe scramble more?\n",
    "\n",
    "def scramble_data(df: pd.DataFrame):\n",
    "    scrambled_lines = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Extract each column's value\n",
    "        type_field = row['type']\n",
    "        id_field = row['id']\n",
    "        subreddit_id = row['subreddit.id']\n",
    "        subreddit_name = row['subreddit.name']\n",
    "        subreddit_nsfw = row['subreddit.nsfw']\n",
    "        created_utc = row['created_utc']\n",
    "        permalink = row['permalink']\n",
    "        # Replace '|' in body to avoid conflicts with new delimiter\n",
    "        body = row['body'].replace('|', '__PIPE__')  \n",
    "        sentiment = row['sentiment']\n",
    "        score = row['score']\n",
    "\n",
    "        # Some scramble logic here:\n",
    "        type_field = (type_field == \"comment\")  # Convert to boolean\n",
    "\n",
    "        # Create a \"scrambled\" line with the \"|\" as delimiter and moving some parts around\n",
    "        row_str = f\"{type_field}|{id_field}|{subreddit_id}|{subreddit_name}|{subreddit_nsfw}|{created_utc}|{sentiment}|{permalink}|{body}|{score}\\n\"\n",
    "\n",
    "        # Randomly break the row into two parts\n",
    "        if random.random() > 0.7:  # 30% chance to split a row into two parts\n",
    "            split_point = len(row_str) // 2\n",
    "            row_str = row_str[:split_point] + '\\n' + row_str[split_point:]\n",
    "\n",
    "        scrambled_lines.append(row_str)\n",
    "\n",
    "    scrambled_data = \"\".join(scrambled_lines)\n",
    "    return scrambled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean body text by removing both single and double quotes from the beginning and end\n",
    "def clean_body_text(body):\n",
    "    # Restore original pipe symbol\n",
    "    body = body.replace('__PIPE__', '|')\n",
    "\n",
    "    # Remove single and double quotation marks from the beginning and end of the body text\n",
    "    body_cleaned = re.sub(r\"^[\\s,]*['\\\"]|['\\\"][\\s,]*$\", '', body)\n",
    "    \n",
    "    # Optionally, collapse multiple spaces into a single space\n",
    "    body_cleaned = re.sub(r'\\s+', ' ', body_cleaned).strip()\n",
    "\n",
    "    return body_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: cleaning the scrambled data\n",
    "# TODO: This should be done in SPARK (Lecturer did something to the body field here for body text in emails)\n",
    "\n",
    "def clean_data(scrambled_data: str):\n",
    "    # Split the unstructured data into individual lines\n",
    "    lines = scrambled_data.split('\\n')\n",
    "\n",
    "    # Step 1: Rejoin broken lines\n",
    "    cleaned_lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip():  # Ignore empty lines\n",
    "            current_line += line.strip() + \" \"  # Append with a space to avoid issues\n",
    "            # Check if we have a complete line\n",
    "            if current_line.count('|') == 9:  # Exactly 10 fields\n",
    "                cleaned_lines.append(current_line.strip())  # Add completed line\n",
    "                current_line = \"\"  # Reset for the next line\n",
    "\n",
    "    # Handle any remaining line that wasn't appended\n",
    "    if current_line:\n",
    "        cleaned_lines.append(current_line.strip())\n",
    "\n",
    "    # Step 2: Parse each cleaned line back into columns\n",
    "    parsed_data = []\n",
    "    error_data = []  # List to hold error information\n",
    "\n",
    "    for i, line in enumerate(cleaned_lines):\n",
    "        fields = line.split('|')\n",
    "\n",
    "        # Ensure we have at least 10 fields before processing\n",
    "        if len(fields) < 10:\n",
    "            error_info = {\n",
    "                'line_index': i,\n",
    "                'fields_found': fields,\n",
    "                'original_line': line\n",
    "            }\n",
    "            error_data.append(error_info)\n",
    "            continue\n",
    "        \n",
    "        # Extract fields\n",
    "        type_field = \"comment\" if fields[0] == \"True\" else \"post\"  # Convert back to string\n",
    "        id_field = fields[1]\n",
    "        subreddit_id = fields[2]\n",
    "        subreddit_name = fields[3]\n",
    "        subreddit_nsfw = \"nsfw\" if fields[4] == \"True\" else \"not_nsfw\"  # Convert to string\n",
    "        created_utc = fields[5]\n",
    "        sentiment = fields[6]\n",
    "        permalink = fields[7]\n",
    "        body = clean_body_text(fields[8])\n",
    "        score = fields[9]\n",
    "\n",
    "        # Convert the timestamp to a human-readable format\n",
    "        created_utc_human = pd.to_datetime(int(created_utc), unit='s').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Create the row dictionary\n",
    "        row_dict = {\n",
    "            'type': type_field,\n",
    "            'id': id_field,\n",
    "            'subreddit_id': subreddit_id,\n",
    "            'subreddit_name': subreddit_name,\n",
    "            'subreddit_nsfw': subreddit_nsfw,\n",
    "            'created_utc': created_utc_human,\n",
    "            'sentiment': sentiment,\n",
    "            'permalink': permalink,\n",
    "            'body': body,\n",
    "            'score': score\n",
    "        }\n",
    "        parsed_data.append(row_dict)\n",
    "\n",
    "    # Optionally, return both parsed data and error data\n",
    "    return pd.DataFrame(parsed_data), error_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "NUM_ROWS = 20\n",
    "\n",
    "# Load a small subset of the CSV (for testing)\n",
    "input_file = 'data/subset/100000-reddit-covid-comments.csv'\n",
    "df = pd.read_csv(input_file).head(NUM_ROWS)  # Load first N rows for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrambled text (first 20 lines):\n",
      "\n",
      "True|hi0xdct|2qh7q|florida|False|1635191579|-0.5666|https://old.reddit.com/r/florida/comments/qf3xp4/desantis_recruiting_unvaccinated_out_of_state/hi0xdct/|&gt; COVID-19 is legitimately dangerous*\n",
      "\n",
      "*for the old and/or unhealthy.\n",
      "\n",
      "Average age of death is like 80. 76% of all deaths are 65+.\n",
      "\n",
      "&gt; we STILL don't know what the long-term effects are\n",
      "\n",
      "[The CDC estimates 120.2 million cases from 2/20 to 5/21 with 6.2 million hospitalizations.](https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/burden.html)\n",
      "\n",
      "How many people do you think are walking around with \"long-term effects\"? How long is \"long-term\"?\n",
      "\n",
      "What if, much like every other disease, there are none, except from severe cases or being absurdly unlucky?\n",
      "\n",
      "&gt; ~750k wasn't enough\n",
      "\n",
      "Considering all the ignored red flags raised about [the PCR test](https://www.nytimes.com/2020/08/29/health/coronavirus-testing.html), we have very good reason to think that number is way too high.\n",
      "\n",
      "&gt; \"In Massachusetts, **from 85 to 90 percent** of people who tested positive in July with a cycle threshold of 40 would have been deemed negative if the threshold were 30 cycles, Dr. Mina said. “I would say that none of those people should be contact-traced, not one,” he said.\"|-1\n",
      "True|hi16118|2y77d|antiwork|False|1635195040|-0.8634|https://old.reddit.com/r/antiwork/comments/qfgkqn/my_friend_has_covid_and_complained_to_the_hr/hi16118/|Wtf ? What is wrong with that manager? For almost 2 years covid has been in the news everyday and he said that to a covid positive worker??!! Something seriously wrong with people nowadays! Haha !!! Crazy.|1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Scramble the dataset and save to a .txt file\n",
    "scrambled_text = scramble_data(df)\n",
    "scrambled_file = \"data/scrambled/scrambled-reddit-covid-comments.txt\"\n",
    "\n",
    "with open(scrambled_file, 'w') as file:\n",
    "    file.write(scrambled_text)\n",
    "\n",
    "print(f\"Scrambled text (first {NUM_ROWS} lines):\\n\")\n",
    "with open(scrambled_file, 'r') as file:\n",
    "    scrambled_data = file.readlines()\n",
    "    for line in scrambled_data[:NUM_ROWS]:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Data (first 20 rows):\n",
      "       type       id subreddit_id         subreddit_name subreddit_nsfw  \\\n",
      "0   comment  hi0xdct        2qh7q                florida       not_nsfw   \n",
      "1   comment  hi16118        2y77d               antiwork       not_nsfw   \n",
      "2   comment  hi1mkh7        2qhsa      interestingasfuck       not_nsfw   \n",
      "3   comment  hi15pqu        2z2wm          cryptomarkets       not_nsfw   \n",
      "4   comment  hi16y0z        2qqd2        greenbaypackers       not_nsfw   \n",
      "5   comment  hi0wt1o        2wtmm             edcorlando       not_nsfw   \n",
      "6   comment  hi1sjao        2yrq6         publicfreakout       not_nsfw   \n",
      "7   comment  hi132e0        2qh1i              askreddit       not_nsfw   \n",
      "8   comment  hi0wo8l        2qtwb              childfree       not_nsfw   \n",
      "9   comment  hi1t85o       2n4vyh          anime_titties       not_nsfw   \n",
      "10  comment  hi1gjs2        2tasy  personalfinancecanada       not_nsfw   \n",
      "11  comment  hi0q4vl        2qh33                  funny       not_nsfw   \n",
      "12  comment  hi12n1p        2r77k                halifax       not_nsfw   \n",
      "13  comment  hi10jol        2qh4r             conspiracy       not_nsfw   \n",
      "14  comment  hi19upp        2qm35                romania       not_nsfw   \n",
      "15  comment  hi0yj65        2y77d               antiwork       not_nsfw   \n",
      "16  comment  hi1si5f        3ax4r        camgirlproblems       not_nsfw   \n",
      "17  comment  hi0d34w        2w844      nostupidquestions       not_nsfw   \n",
      "18  comment  hi0sper        2qh4r             conspiracy       not_nsfw   \n",
      "19  comment  hi0mccb        2qh2t                chicago       not_nsfw   \n",
      "\n",
      "            created_utc sentiment  \\\n",
      "0   2021-10-25 19:52:59   -0.5666   \n",
      "1   2021-10-25 20:50:40   -0.8634   \n",
      "2   2021-10-25 22:49:17    0.4329   \n",
      "3   2021-10-25 20:48:34       0.0   \n",
      "4   2021-10-25 20:56:56       0.0   \n",
      "5   2021-10-25 19:49:20    0.8077   \n",
      "6   2021-10-25 23:34:46       0.0   \n",
      "7   2021-10-25 20:30:56    0.5106   \n",
      "8   2021-10-25 19:48:27    0.9752   \n",
      "9   2021-10-25 23:40:04   -0.8689   \n",
      "10  2021-10-25 22:04:54     -0.34   \n",
      "11  2021-10-25 19:05:22    0.6124   \n",
      "12  2021-10-25 20:28:05   -0.6598   \n",
      "13  2021-10-25 20:14:13   -0.4143   \n",
      "14  2021-10-25 21:17:09       nan   \n",
      "15  2021-10-25 20:00:42     0.797   \n",
      "16  2021-10-25 23:34:30    0.8659   \n",
      "17  2021-10-25 17:37:02   -0.2023   \n",
      "18  2021-10-25 19:22:08   -0.0258   \n",
      "19  2021-10-25 18:40:02    0.5859   \n",
      "\n",
      "                                            permalink  \\\n",
      "0   https://old.reddit.com/r/florida/comments/qf3x...   \n",
      "1   https://old.reddit.com/r/antiwork/comments/qfg...   \n",
      "2   https://old.reddit.com/r/interestingasfuck/com...   \n",
      "3   https://old.reddit.com/r/CryptoMarkets/comment...   \n",
      "4   https://old.reddit.com/r/GreenBayPackers/comme...   \n",
      "5   https://old.reddit.com/r/EDCOrlando/comments/q...   \n",
      "6   https://old.reddit.com/r/PublicFreakout/commen...   \n",
      "7   https://old.reddit.com/r/AskReddit/comments/qf...   \n",
      "8   https://old.reddit.com/r/childfree/comments/qf...   \n",
      "9   https://old.reddit.com/r/anime_titties/comment...   \n",
      "10  https://old.reddit.com/r/PersonalFinanceCanada...   \n",
      "11  https://old.reddit.com/r/funny/comments/qfnv3e...   \n",
      "12  https://old.reddit.com/r/halifax/comments/qfng...   \n",
      "13  https://old.reddit.com/r/conspiracy/comments/q...   \n",
      "14  https://old.reddit.com/r/Romania/comments/qfqi...   \n",
      "15  https://old.reddit.com/r/antiwork/comments/qfg...   \n",
      "16  https://old.reddit.com/r/CamGirlProblems/comme...   \n",
      "17  https://old.reddit.com/r/NoStupidQuestions/com...   \n",
      "18  https://old.reddit.com/r/conspiracy/comments/q...   \n",
      "19  https://old.reddit.com/r/chicago/comments/qfmo...   \n",
      "\n",
      "                                                 body score  \n",
      "0   &gt; COVID-19 is legitimately dangerous* *for ...    -1  \n",
      "1   Wtf ? What is wrong with that manager? For alm...     1  \n",
      "2   I thought the sherpas were working on that thr...     1  \n",
      "3   I mean…yea it was a gamble. But it’s paying my...     1  \n",
      "4   Fact is the “vaccine” doesn’t do anything. Fir...     0  \n",
      "5   Lol I’m vaccinated. But u sound like a dumbass...     3  \n",
      "6   This explains why he's also against the COVID ...     2  \n",
      "7   Go to ww w.vigiaccess.org scroll down and acce...    -2  \n",
      "8   Fair question and true. Wanting is a enough of...    24  \n",
      "9   This pill: https://www.drugs.com/imprints/806-...     0  \n",
      "10  BMO has two Customer Contact Centres. One in M...     6  \n",
      "11  A message to all users: Please be aware that s...     1  \n",
      "12  I so feel you with this. Lost work from COVID,...     1  \n",
      "13  There are so many parents with no fucking clue...    12  \n",
      "14  Baga-mias pula in ea treaba, eu paranoicul pul...    18  \n",
      "15  I don't have COVID. But I am very sick today, ...     1  \n",
      "16  Trolls will attack you with anything they sens...     1  \n",
      "17  Any math on that? About 5x as many cops died f...    -1  \n",
      "18  There was already that story that came out a m...     2  \n",
      "19  steppenwolf is doing BUG again, it was amazing...    12  \n",
      "Cleaned data saved to data/cleaned/cleaned-reddit-covid-comments.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the scrambled .txt file (to simulate real-world scenario)\n",
    "with open(scrambled_file, 'r') as file:\n",
    "    scrambled_data = file.read()\n",
    "\n",
    "# Clean the dataset (bring it back to a clean state)\n",
    "df_cleaned, error_data = clean_data(scrambled_data)\n",
    "\n",
    "print(f\"\\nCleaned Data (first {NUM_ROWS} rows):\")\n",
    "print(df_cleaned.head(NUM_ROWS))\n",
    "\n",
    "# Step 3: Save the cleaned data back to a Parquet file\n",
    "cleaned_output_file_parquet = \"data/cleaned/cleaned-reddit-covid-comments.parquet\"\n",
    "df_cleaned.to_parquet(cleaned_output_file_parquet, index=False)\n",
    "\n",
    "# Also save the cleaned data back to a CSV for easier comparison\n",
    "cleaned_output_file = \"data/cleaned/cleaned-reddit-covid-comments.csv\"\n",
    "df_cleaned.to_csv(cleaned_output_file, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found.\n"
     ]
    }
   ],
   "source": [
    "# Check for any errors\n",
    "if len(error_data) > 0:\n",
    "    print(f\"\\nTotal Errors Encountered: {len(error_data)}\")\n",
    "    for error in error_data:\n",
    "        print(f\"Error in line {error['line_index']}: {error['fields_found']}\")\n",
    "else:\n",
    "    print(\"No errors found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
