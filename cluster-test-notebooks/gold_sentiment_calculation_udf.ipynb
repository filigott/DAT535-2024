{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Calculation UDF\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"3\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"9\") \\\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.extraPythonPackages\", \"pandas,vaderSentiment\") \\\n",
    "    .config(\"spark.driver.extraPythonPackages\", \"pandas,vaderSentiment\") \\\n",
    "    .config(\"spark.executorEnv.PYTHONPATH\", \":\".join(sys.path)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level for cleaner outputs\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:36.723 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "12:04:36.730 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/ubuntu/project/cluster-notebooks/spark-warehouse'.\n",
      "12:04:36.740 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:04:36.742 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@164a6dac{/SQL,null,AVAILABLE,@Spark}\n",
      "12:04:36.742 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:04:36.743 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a7b5f23{/SQL/json,null,AVAILABLE,@Spark}\n",
      "12:04:36.743 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:04:36.744 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fc2409d{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "12:04:36.744 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:04:36.744 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2080dcd7{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "12:04:36.745 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:04:36.746 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3da39e29{/static/sql,null,AVAILABLE,@Spark}\n",
      "12:04:37.474 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 71 ms to list leaf files for 1 paths.\n",
      "12:04:37.723 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "12:04:37.742 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:04:37.743 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "12:04:37.743 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:04:37.744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:04:37.786 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:04:37.861 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 127.3 KiB, free 2004.5 MiB)\n",
      "12:04:37.886 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 2004.4 MiB)\n",
      "12:04:37.892 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on namenode:37805 (size: 46.9 KiB, free: 2004.6 MiB)\n",
      "12:04:37.897 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "12:04:37.914 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:04:37.914 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "12:04:37.945 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (datanode3, executor 1, partition 0, PROCESS_LOCAL, 9222 bytes) \n",
      "12:04:38.216 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on datanode3:42353 (size: 46.9 KiB, free: 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:39.901 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1968 ms on datanode3 (executor 1) (1/1)\n",
      "12:04:39.903 [task-result-getter-0] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "12:04:39.908 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.095 s\n",
      "12:04:39.912 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:04:39.913 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 0: Stage finished\n",
      "12:04:39.915 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.191466 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:40.160 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on namenode:37805 in memory (size: 46.9 KiB, free: 2004.6 MiB)\n",
      "12:04:40.170 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on datanode3:42353 in memory (size: 46.9 KiB, free: 912.3 MiB)\n",
      "root\n",
      " |-- link: string (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- sub_reddit: string (nullable = true)\n",
      " |-- post_id: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      "\n",
      "12:04:41.022 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:04:41.023 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:04:41.408 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 230.862231 ms\n",
      "12:04:41.439 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 409.4 KiB, free 2004.2 MiB)\n",
      "12:04:41.451 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 45.0 KiB, free 2004.2 MiB)\n",
      "12:04:41.453 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on namenode:37805 (size: 45.0 KiB, free: 2004.6 MiB)\n",
      "12:04:41.463 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0\n",
      "12:04:41.480 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "12:04:41.538 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "12:04:41.540 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:04:41.540 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "12:04:41.540 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:04:41.546 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:04:41.553 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:04:41.596 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 18.2 KiB, free 2004.1 MiB)\n",
      "12:04:41.599 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 2004.1 MiB)\n",
      "12:04:41.600 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on namenode:37805 (size: 6.9 KiB, free: 2004.5 MiB)\n",
      "12:04:41.601 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "12:04:41.603 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:04:41.603 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "12:04:41.616 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (datanode3, executor 1, partition 0, NODE_LOCAL, 9690 bytes) \n",
      "12:04:41.677 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on datanode3:42353 (size: 6.9 KiB, free: 912.3 MiB)\n",
      "12:04:42.116 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on datanode3:42353 (size: 45.0 KiB, free: 912.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:42.804 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1192 ms on datanode3 (executor 1) (1/1)\n",
      "12:04:42.804 [task-result-getter-1] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "12:04:42.805 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 1.241 s\n",
      "12:04:42.806 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:04:42.806 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 1: Stage finished\n",
      "12:04:42.806 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.268451 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:43.817 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.704362 ms\n",
      "+----------------------------------------------------------------------------------------------------------------+-----------+-----+------------+-------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|link                                                                                                            |created_utc|score|sub_reddit  |post_id|comment_id|body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+----------------------------------------------------------------------------------------------------------------+-----------+-----+------------+-------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|https://old.reddit.com/r/AmalaNetwork/comments/lv6zt4/joe_biden_says_his_hands_are_tied_on_a_15_minimum/gpbtnii/|1614624690 |3    |AmalaNetwork|lv6zt4 |gpbtnii   |might be an unpopular opinion but i dont think its worth holding up the covid relief bill over this any longer. people are going hungry and getting evicted right now enhanced ui and assistance should be the priority in the moment.                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|https://old.reddit.com/r/Seattle/comments/lvbojq/seattle_to_operate_mass_vaccination_site_at_lumen/gpbtmvu/     |1614624683 |17   |Seattle     |lvbojq |gpbtmvu   |king county has used 90 of their doses way higher than our state avg. its not seattle that is the issue. also it is 9 if you look at the king county numbers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|https://old.reddit.com/r/samharris/comments/lv58pu/politics_and_current_events_megathread_march_2021/gpbtmqv/   |1614624681 |7    |samharris   |lv58pu |gpbtmqv   |sam seems to think theres a significant problem of rich people and their tax money leaving california and moving to other states from people i know in my network i can account for probably 100s of millions of dollars in the tax base evaporating in california. from episode 232. however according to this httpswww.nytimes.com20210301businesscovidstatetaxrevenue.htmlhttpswww.nytimes.com20210301businesscovidstatetaxrevenue.html california state tax revenue actually increased 1.2 from 2019 to 2020.                                                                                                                                                   |\n",
      "|https://old.reddit.com/r/NBASpurs/comments/lvfm3v/fans_at_home_games_starting_mar_12/gpbtmh9/                   |1614624678 |4    |NBASpurs    |lvfm3v |gpbtmh9   |man im really thinking of driving 7 hours to see a game. covid cancelling a game is really scary though                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|https://old.reddit.com/r/Reformed/comments/lvfncl/attending_a_church_if_skeptical_of_the_pastors/gpbtlsk/       |1614624669 |13   |Reformed    |lvfncl |gpbtlsk   |judging a church and a person by their response s to a worldwide pandemic one which we thought was a biological attack at one point then settled into the dullknife dailydrama of fear and more death than anyone imagined lets not imagine that anyone has handled this great. i dont know anyone who has. including the churches who closed march 17 and are still closedthats not great either. if you dont want to attend this church dont base it on how they scored on their covid19 response bingo card. do it based on doctrine dogma and if they have cute boys there like always. not on a certain response to this exceptional weird time in our culture.|\n",
      "+----------------------------------------------------------------------------------------------------------------+-----------+-----+------------+-------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HDFS path for the dataset\n",
    "file_path = \"hdfs://namenode:9000/data/cleaned_dataset.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# Display schema and sample rows\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean comments\n",
    "def clean_comment_spark(df, column):\n",
    "    \"\"\"Clean comments in the specified column.\"\"\"\n",
    "    return df.withColumn(\n",
    "        f\"{column}_clean\",\n",
    "        F.trim(\n",
    "            F.regexp_replace(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.lower(F.col(column)),  # Convert to lowercase\n",
    "                        r\"http\\S+|www\\S+|https\\S+\", \"\"),  # Remove URLs\n",
    "                    r\"@\\w+|#\", \"\"),  # Remove mentions and hashtags\n",
    "                r\"[^\\w\\s]\", \"\"),  # Remove special characters and punctuation\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:44.032 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:04:44.033 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:04:44.119 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 55.662525 ms\n",
      "12:04:44.125 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 408.2 KiB, free 2003.7 MiB)\n",
      "12:04:44.140 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 44.9 KiB, free 2003.7 MiB)\n",
      "12:04:44.141 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on namenode:37805 (size: 44.9 KiB, free: 2004.5 MiB)\n",
      "12:04:44.144 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "12:04:44.146 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "12:04:44.174 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "12:04:44.177 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:04:44.177 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "12:04:44.177 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:04:44.179 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:04:44.180 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:04:44.191 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 20.4 KiB, free 2003.7 MiB)\n",
      "12:04:44.198 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 2003.7 MiB)\n",
      "12:04:44.199 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on namenode:37805 (size: 7.9 KiB, free: 2004.5 MiB)\n",
      "12:04:44.200 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "12:04:44.200 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:04:44.201 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "12:04:44.202 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (datanode1, executor 3, partition 0, NODE_LOCAL, 9690 bytes) \n",
      "12:04:44.417 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on datanode1:41313 (size: 7.9 KiB, free: 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:45.299 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on datanode1:41313 (size: 44.9 KiB, free: 912.2 MiB)\n",
      "12:04:45.459 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on namenode:37805 in memory (size: 6.9 KiB, free: 2004.5 MiB)\n",
      "12:04:45.463 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on datanode3:42353 in memory (size: 6.9 KiB, free: 912.3 MiB)\n",
      "12:04:45.473 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on namenode:37805 in memory (size: 45.0 KiB, free: 2004.5 MiB)\n",
      "12:04:45.476 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on datanode3:42353 in memory (size: 45.0 KiB, free: 912.3 MiB)\n",
      "12:04:46.837 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 2635 ms on datanode1 (executor 3) (1/1)\n",
      "12:04:46.837 [task-result-getter-2] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "12:04:46.838 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 2.656 s\n",
      "12:04:46.840 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:04:46.840 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 2: Stage finished\n",
      "12:04:46.842 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 2.666805 s\n",
      "12:04:46.863 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.376505 ms\n",
      "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|comment_id|body_clean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|gpbtnii   |might be an unpopular opinion but i dont think its worth holding up the covid relief bill over this any longer people are going hungry and getting evicted right now enhanced ui and assistance should be the priority in the moment                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|gpbtmvu   |king county has used 90 of their doses way higher than our state avg its not seattle that is the issue also it is 9 if you look at the king county numbers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|gpbtmqv   |sam seems to think theres a significant problem of rich people and their tax money leaving california and moving to other states from people i know in my network i can account for probably 100s of millions of dollars in the tax base evaporating in california from episode 232 however according to this  california state tax revenue actually increased 12 from 2019 to 2020                                                                                                                                                                                                                                                                           |\n",
      "|gpbtmh9   |man im really thinking of driving 7 hours to see a game covid cancelling a game is really scary though                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|gpbtlsk   |judging a church and a person by their response s to a worldwide pandemic one which we thought was a biological attack at one point then settled into the dullknife dailydrama of fear and more death than anyone imagined lets not imagine that anyone has handled this great i dont know anyone who has including the churches who closed march 17 and are still closedthats not great either if you dont want to attend this church dont base it on how they scored on their covid19 response bingo card do it based on doctrine dogma and if they have cute boys there like always not on a certain response to this exceptional weird time in our culture|\n",
      "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Clean the comments and select relevant columns\n",
    "df = clean_comment_spark(df, \"body\").select(\"comment_id\", \"body_clean\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:46.920 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 312.0 B, free 2004.1 MiB)\n",
      "12:04:46.931 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 379.2 KiB, free 2003.8 MiB)\n",
      "12:04:46.933 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on namenode:37805 (size: 379.2 KiB, free: 2004.2 MiB)\n",
      "12:04:46.934 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "# Create and broadcast the SentimentIntensityAnalyzer\n",
    "analyzer_broadcast = sc.broadcast(SentimentIntensityAnalyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF for sentiment analysis\n",
    "def calculate_sentiment_udf(text):\n",
    "    analyzer = analyzer_broadcast.value\n",
    "    if text:\n",
    "        return analyzer.polarity_scores(text)['compound']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the UDF\n",
    "sentiment_udf = F.udf(calculate_sentiment_udf, FloatType())\n",
    "\n",
    "# Apply the UDF to calculate sentiment\n",
    "df = df.withColumn(\"sentiment\", sentiment_udf(F.col(\"body_clean\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:47.143 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:04:47.143 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:04:47.199 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "12:04:47.216 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "12:04:47.216 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "12:04:47.216 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "12:04:47.217 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "12:04:47.217 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "12:04:47.217 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "12:04:47.237 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.272266 ms\n",
      "12:04:47.247 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.545689 ms\n",
      "12:04:47.251 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 408.2 KiB, free 2003.4 MiB)\n",
      "12:04:47.258 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 44.9 KiB, free 2003.3 MiB)\n",
      "12:04:47.259 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on namenode:37805 (size: 44.9 KiB, free: 2004.1 MiB)\n",
      "12:04:47.259 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 6 from parquet at NativeMethodAccessorImpl.java:0\n",
      "12:04:47.261 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "12:04:47.300 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "12:04:47.303 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 45 output partitions\n",
      "12:04:47.303 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "12:04:47.303 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:04:47.308 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:04:47.312 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:04:47.353 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 280.6 KiB, free 2003.0 MiB)\n",
      "12:04:47.355 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 104.9 KiB, free 2002.9 MiB)\n",
      "12:04:47.356 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on namenode:37805 (size: 104.9 KiB, free: 2004.0 MiB)\n",
      "12:04:47.357 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "12:04:47.358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 45 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "12:04:47.358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 3.0 with 45 tasks resource profile 0\n",
      "12:04:47.361 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3) (datanode3, executor 1, partition 0, NODE_LOCAL, 9690 bytes) \n",
      "12:04:47.361 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 4) (datanode1, executor 3, partition 1, NODE_LOCAL, 9690 bytes) \n",
      "12:04:47.362 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 5) (datanode2, executor 2, partition 2, NODE_LOCAL, 9690 bytes) \n",
      "12:04:47.362 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 6) (datanode3, executor 1, partition 3, NODE_LOCAL, 9690 bytes) \n",
      "12:04:47.366 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 3.0 (TID 7) (datanode1, executor 3, partition 4, NODE_LOCAL, 9690 bytes) \n",
      "12:04:47.367 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 3.0 (TID 8) (datanode2, executor 2, partition 5, NODE_LOCAL, 9690 bytes) \n",
      "12:04:47.393 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode3:42353 (size: 104.9 KiB, free: 912.2 MiB)\n",
      "12:04:47.406 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode1:41313 (size: 104.9 KiB, free: 912.1 MiB)\n",
      "12:04:47.667 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode2:36269 (size: 104.9 KiB, free: 912.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                         (0 + 6) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:48.199 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode3:42353 (size: 379.2 KiB, free: 911.8 MiB)\n",
      "12:04:48.221 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_python on disk on datanode3:42353 (size: 822.8 KiB)\n",
      "12:04:48.243 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode3:42353 (size: 44.9 KiB, free: 911.8 MiB)\n",
      "12:04:48.386 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 1 new executor because tasks are backlogged (new desired total will be 4 for resource profile id: 0)\n",
      "12:04:48.402 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode1:41313 (size: 379.2 KiB, free: 911.8 MiB)\n",
      "12:04:48.435 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_python on disk on datanode1:41313 (size: 822.8 KiB)\n",
      "12:04:48.459 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode1:41313 (size: 44.9 KiB, free: 911.7 MiB)\n",
      "12:04:49.400 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 2 new executors because tasks are backlogged (new desired total will be 6 for resource profile id: 0)\n",
      "12:04:49.865 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode2:36269 (size: 379.2 KiB, free: 911.8 MiB)\n",
      "12:04:49.906 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_python on disk on datanode2:36269 (size: 822.8 KiB)\n",
      "12:04:49.943 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode2:36269 (size: 44.9 KiB, free: 911.8 MiB)\n",
      "12:04:50.422 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 3 new executors because tasks are backlogged (new desired total will be 9 for resource profile id: 0)\n",
      "12:04:51.703 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.12.222:46396) with ID 4,  ResourceProfileId 0\n",
      "12:04:51.705 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 4 has registered (new total is 4)\n",
      "12:04:51.860 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager datanode2:35337 with 912.3 MiB RAM, BlockManagerId(4, datanode2, 35337, None)\n",
      "12:04:51.928 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 3.0 (TID 9) (datanode2, executor 4, partition 6, NODE_LOCAL, 9690 bytes) \n",
      "12:04:51.929 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 3.0 (TID 10) (datanode2, executor 4, partition 7, NODE_LOCAL, 9690 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                         (0 + 8) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:52.275 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode2:35337 (size: 104.9 KiB, free: 912.2 MiB)\n",
      "12:04:53.495 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.12.118:39120) with ID 6,  ResourceProfileId 0\n",
      "12:04:53.497 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 6 has registered (new total is 5)\n",
      "12:04:53.583 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager datanode3:40487 with 912.3 MiB RAM, BlockManagerId(6, datanode3, 40487, None)\n",
      "12:04:53.637 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 3.0 (TID 11) (datanode3, executor 6, partition 8, NODE_LOCAL, 9690 bytes) \n",
      "12:04:53.638 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 3.0 (TID 12) (datanode3, executor 6, partition 9, NODE_LOCAL, 9690 bytes) \n",
      "12:04:53.776 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.12.196:48482) with ID 5,  ResourceProfileId 0\n",
      "12:04:53.778 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 5 has registered (new total is 6)\n",
      "12:04:53.852 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager datanode1:35917 with 912.3 MiB RAM, BlockManagerId(5, datanode1, 35917, None)\n",
      "12:04:53.894 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 3.0 (TID 13) (datanode1, executor 5, partition 10, NODE_LOCAL, 9690 bytes) \n",
      "12:04:53.895 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 3.0 (TID 14) (datanode1, executor 5, partition 11, NODE_LOCAL, 9690 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:04:54.034 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode3:40487 (size: 104.9 KiB, free: 912.2 MiB)\n",
      "12:04:54.232 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode1:35917 (size: 104.9 KiB, free: 912.2 MiB)\n",
      "12:04:54.815 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode2:35337 (size: 379.2 KiB, free: 911.8 MiB)\n",
      "12:04:54.873 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_python on disk on datanode2:35337 (size: 822.8 KiB)\n",
      "12:04:55.082 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode2:35337 (size: 44.9 KiB, free: 911.8 MiB)\n",
      "12:04:56.349 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode3:40487 (size: 379.2 KiB, free: 911.8 MiB)\n",
      "12:04:56.606 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_python on disk on datanode3:40487 (size: 822.8 KiB)\n",
      "12:04:56.669 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode3:40487 (size: 44.9 KiB, free: 911.8 MiB)\n",
      "12:04:56.681 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode1:35917 (size: 379.2 KiB, free: 911.8 MiB)\n",
      "12:04:56.736 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_python on disk on datanode1:35917 (size: 822.8 KiB)\n",
      "12:04:56.789 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode1:35917 (size: 44.9 KiB, free: 911.8 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:25.200 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 3.0 (TID 15) (datanode2, executor 2, partition 12, NODE_LOCAL, 9690 bytes) \n",
      "12:06:25.208 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 3.0 (TID 8) in 97841 ms on datanode2 (executor 2) (1/45)\n",
      "12:06:25.212 [dag-scheduler-event-loop] INFO  org.apache.spark.api.python.PythonAccumulatorV2 - Connected to AccumulatorServer at host: 127.0.0.1 port: 60387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=>                                                       (1 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:25.966 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 3.0 (TID 16) (datanode1, executor 3, partition 13, NODE_LOCAL, 9690 bytes) \n",
      "12:06:25.967 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 3.0 (TID 7) in 98601 ms on datanode1 (executor 3) (2/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==>                                                      (2 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:27.042 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 3.0 (TID 17) (datanode2, executor 2, partition 14, NODE_LOCAL, 9690 bytes) \n",
      "12:06:27.042 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 5) in 99680 ms on datanode2 (executor 2) (3/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===>                                                     (3 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:27.992 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 3.0 (TID 18) (datanode3, executor 1, partition 15, NODE_LOCAL, 9690 bytes) \n",
      "12:06:27.993 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 100632 ms on datanode3 (executor 1) (4/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====>                                                   (4 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:28.647 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 3.0 (TID 19) (datanode1, executor 3, partition 16, NODE_LOCAL, 9690 bytes) \n",
      "12:06:28.648 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 4) in 101287 ms on datanode1 (executor 3) (5/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======>                                                  (5 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:30.243 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 3.0 (TID 20) (datanode2, executor 4, partition 17, NODE_LOCAL, 9690 bytes) \n",
      "12:06:30.244 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 3.0 (TID 10) in 98316 ms on datanode2 (executor 4) (6/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======>                                                 (6 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:30.967 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 3.0 (TID 21) (datanode3, executor 1, partition 18, NODE_LOCAL, 9690 bytes) \n",
      "12:06:30.968 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 6) in 103606 ms on datanode3 (executor 1) (7/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:========>                                                (7 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:31.849 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 3.0 (TID 22) (datanode2, executor 4, partition 19, NODE_LOCAL, 9690 bytes) \n",
      "12:06:31.850 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 3.0 (TID 9) in 99926 ms on datanode2 (executor 4) (8/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==========>                                              (8 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:33.903 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 3.0 (TID 23) (datanode3, executor 6, partition 20, NODE_LOCAL, 9690 bytes) \n",
      "12:06:33.904 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 3.0 (TID 11) in 100267 ms on datanode3 (executor 6) (9/45)\n",
      "12:06:33.907 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 3.0 (TID 24) (datanode1, executor 5, partition 21, NODE_LOCAL, 9690 bytes) \n",
      "12:06:33.908 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 3.0 (TID 14) in 100014 ms on datanode1 (executor 5) (10/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============>                                           (10 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:34.908 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 3.0 (TID 25) (datanode3, executor 6, partition 22, NODE_LOCAL, 9690 bytes) \n",
      "12:06:34.909 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 3.0 (TID 12) in 101272 ms on datanode3 (executor 6) (11/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============>                                          (11 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:06:36.969 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 3.0 (TID 26) (datanode1, executor 5, partition 23, NODE_LOCAL, 9690 bytes) \n",
      "12:06:36.970 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 3.0 (TID 13) in 103077 ms on datanode1 (executor 5) (12/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==============>                                         (12 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:07:57.393 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 3.0 (TID 27) (datanode2, executor 2, partition 24, NODE_LOCAL, 9690 bytes) \n",
      "12:07:57.395 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 3.0 (TID 15) in 92196 ms on datanode2 (executor 2) (13/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:================>                                       (13 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:02.101 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 3.0 (TID 28) (datanode2, executor 2, partition 25, NODE_LOCAL, 9690 bytes) \n",
      "12:08:02.101 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 3.0 (TID 17) in 95060 ms on datanode2 (executor 2) (14/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=================>                                      (14 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:02.801 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 3.0 (TID 29) (datanode1, executor 3, partition 26, NODE_LOCAL, 9690 bytes) \n",
      "12:08:02.802 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 3.0 (TID 19) in 94155 ms on datanode1 (executor 3) (15/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==================>                                     (15 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:04.100 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 3.0 (TID 30) (datanode3, executor 1, partition 27, NODE_LOCAL, 9690 bytes) \n",
      "12:08:04.103 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 3.0 (TID 18) in 96111 ms on datanode3 (executor 1) (16/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===================>                                    (16 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:05.846 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 3.0 (TID 31) (datanode3, executor 1, partition 28, NODE_LOCAL, 9690 bytes) \n",
      "12:08:05.847 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 3.0 (TID 21) in 94880 ms on datanode3 (executor 1) (17/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====================>                                  (17 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:06.182 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 3.0 (TID 32) (datanode2, executor 4, partition 29, NODE_LOCAL, 9690 bytes) \n",
      "12:08:06.183 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 3.0 (TID 20) in 95940 ms on datanode2 (executor 4) (18/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================>                                 (18 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:06.887 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 3.0 (TID 33) (datanode1, executor 3, partition 30, NODE_LOCAL, 9690 bytes) \n",
      "12:08:06.888 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 3.0 (TID 16) in 100923 ms on datanode1 (executor 3) (19/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================>                                (19 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:07.524 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 31.0 in stage 3.0 (TID 34) (datanode1, executor 5, partition 31, NODE_LOCAL, 9690 bytes) \n",
      "12:08:07.525 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 3.0 (TID 24) in 93619 ms on datanode1 (executor 5) (20/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:========================>                               (20 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:08.711 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 32.0 in stage 3.0 (TID 35) (datanode3, executor 6, partition 32, NODE_LOCAL, 9690 bytes) \n",
      "12:08:08.713 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 3.0 (TID 25) in 93806 ms on datanode3 (executor 6) (21/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==========================>                             (21 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:12.609 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 33.0 in stage 3.0 (TID 36) (datanode3, executor 6, partition 33, NODE_LOCAL, 9690 bytes) \n",
      "12:08:12.610 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 3.0 (TID 23) in 98708 ms on datanode3 (executor 6) (22/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===========================>                            (22 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:13.092 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 3.0 (TID 22) in 101242 ms on datanode2 (executor 4) (23/45)\n",
      "12:08:13.093 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 34.0 in stage 3.0 (TID 37) (datanode2, executor 4, partition 34, NODE_LOCAL, 9690 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                          (24 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:08:13.473 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 35.0 in stage 3.0 (TID 38) (datanode1, executor 5, partition 35, NODE_LOCAL, 9690 bytes) \n",
      "12:08:13.475 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 3.0 (TID 26) in 96506 ms on datanode1 (executor 5) (24/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                          (24 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:09:31.797 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 36.0 in stage 3.0 (TID 39) (datanode2, executor 2, partition 36, NODE_LOCAL, 9690 bytes) \n",
      "12:09:31.800 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 24.0 in stage 3.0 (TID 27) in 94407 ms on datanode2 (executor 2) (25/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===============================>                        (25 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:09:36.129 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 37.0 in stage 3.0 (TID 40) (datanode2, executor 2, partition 37, NODE_LOCAL, 9690 bytes) \n",
      "12:09:36.130 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 3.0 (TID 28) in 94030 ms on datanode2 (executor 2) (26/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:================================>                       (26 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:09:38.088 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 38.0 in stage 3.0 (TID 41) (datanode1, executor 5, partition 38, NODE_LOCAL, 9690 bytes) \n",
      "12:09:38.089 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 31.0 in stage 3.0 (TID 34) in 90566 ms on datanode1 (executor 5) (27/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=================================>                      (27 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:09:38.938 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 39.0 in stage 3.0 (TID 42) (datanode2, executor 4, partition 39, NODE_LOCAL, 9690 bytes) \n",
      "12:09:38.940 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 29.0 in stage 3.0 (TID 32) in 92757 ms on datanode2 (executor 4) (28/45)\n",
      "12:09:39.088 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 40.0 in stage 3.0 (TID 43) (datanode3, executor 1, partition 40, NODE_LOCAL, 9690 bytes) \n",
      "12:09:39.090 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 3.0 (TID 31) in 93244 ms on datanode3 (executor 1) (29/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:====================================>                   (29 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:09:40.871 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 41.0 in stage 3.0 (TID 44) (datanode3, executor 1, partition 41, NODE_LOCAL, 9690 bytes) \n",
      "12:09:40.872 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 27.0 in stage 3.0 (TID 30) in 96771 ms on datanode3 (executor 1) (30/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====================================>                  (30 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:09:42.670 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 42.0 in stage 3.0 (TID 45) (datanode3, executor 6, partition 42, NODE_LOCAL, 9690 bytes) \n",
      "12:09:42.671 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 33.0 in stage 3.0 (TID 36) in 90063 ms on datanode3 (executor 6) (31/45)\n",
      "12:09:42.773 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 43.0 in stage 3.0 (TID 46) (datanode1, executor 5, partition 43, NODE_LOCAL, 9690 bytes) \n",
      "12:09:42.775 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 35.0 in stage 3.0 (TID 38) in 89302 ms on datanode1 (executor 5) (32/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================>                (32 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:09:43.944 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 44.0 in stage 3.0 (TID 47) (datanode1, executor 3, partition 44, NODE_LOCAL, 9864 bytes) \n",
      "12:09:43.946 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 3.0 (TID 33) in 97059 ms on datanode1 (executor 3) (33/45)\n",
      "12:09:44.033 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 34.0 in stage 3.0 (TID 37) in 90940 ms on datanode2 (executor 4) (34/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==========================================>             (34 + 11) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:09:45.007 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 3.0 (TID 29) in 102207 ms on datanode1 (executor 3) (35/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===========================================>            (35 + 10) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:09:46.737 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 32.0 in stage 3.0 (TID 35) in 98026 ms on datanode3 (executor 6) (36/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================================>           (36 + 9) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:10:54.478 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 36.0 in stage 3.0 (TID 39) in 82682 ms on datanode2 (executor 2) (37/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==============================================>          (37 + 8) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:01.511 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 37.0 in stage 3.0 (TID 40) in 85382 ms on datanode2 (executor 2) (38/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:================================================>        (38 + 7) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:02.827 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 38.0 in stage 3.0 (TID 41) in 84740 ms on datanode1 (executor 5) (39/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=================================================>       (39 + 6) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:03.472 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 41.0 in stage 3.0 (TID 44) in 82602 ms on datanode3 (executor 1) (40/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==================================================>      (40 + 5) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:05.726 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 40.0 in stage 3.0 (TID 43) in 86638 ms on datanode3 (executor 1) (41/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===================================================>     (41 + 4) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:06.149 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 42.0 in stage 3.0 (TID 45) in 83479 ms on datanode3 (executor 6) (42/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====================================================>   (42 + 3) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:07.391 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 43.0 in stage 3.0 (TID 46) in 84618 ms on datanode1 (executor 5) (43/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>  (43 + 2) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:22.107 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 39.0 in stage 3.0 (TID 42) in 103169 ms on datanode2 (executor 4) (44/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================================> (44 + 1) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:12:01.519 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Requesting to kill executor(s) 2\n",
      "12:12:01.523 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Actual list of executor(s) to be killed is 2\n",
      "12:12:01.566 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Executors 2 removed due to idle timeout.\n",
      "12:12:03.598 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Disabling executor 2.\n",
      "12:12:03.607 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Executor lost: 2 (epoch 0)\n",
      "12:12:03.610 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor 2 from BlockManagerMaster.\n",
      "12:12:03.611 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(2, datanode2, 36269, None)\n",
      "12:12:03.611 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.BlockManagerMaster - Removed 2 successfully in removeExecutor\n",
      "12:12:03.612 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Shuffle files lost for executor: 2 (epoch 0)\n",
      "12:12:03.632 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Executor 2 on datanode2 killed by driver.\n",
      "12:12:03.639 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - Executor 2 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 1, unexpectedly exited: 0).\n",
      "12:12:05.776 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Requesting to kill executor(s) 1\n",
      "12:12:05.776 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Actual list of executor(s) to be killed is 1\n",
      "12:12:05.786 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Executors 1 removed due to idle timeout.\n",
      "12:12:06.188 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Requesting to kill executor(s) 6\n",
      "12:12:06.188 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Actual list of executor(s) to be killed is 6\n",
      "12:12:06.197 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Executors 6 removed due to idle timeout.\n",
      "12:12:07.345 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Disabling executor 6.\n",
      "12:12:07.346 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Executor lost: 6 (epoch 1)\n",
      "12:12:07.346 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor 6 from BlockManagerMaster.\n",
      "12:12:07.346 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(6, datanode3, 40487, None)\n",
      "12:12:07.346 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.BlockManagerMaster - Removed 6 successfully in removeExecutor\n",
      "12:12:07.347 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Shuffle files lost for executor: 6 (epoch 1)\n",
      "12:12:07.349 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Disabling executor 1.\n",
      "12:12:07.351 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Executor lost: 1 (epoch 2)\n",
      "12:12:07.351 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Executor 6 on datanode3 killed by driver.\n",
      "12:12:07.351 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor 1 from BlockManagerMaster.\n",
      "12:12:07.352 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(1, datanode3, 42353, None)\n",
      "12:12:07.352 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.BlockManagerMaster - Removed 1 successfully in removeExecutor\n",
      "12:12:07.352 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Shuffle files lost for executor: 1 (epoch 2)\n",
      "12:12:07.352 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - Executor 6 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 2, unexpectedly exited: 0).\n",
      "12:12:07.354 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Executor 1 on datanode3 killed by driver.\n",
      "12:12:07.354 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - Executor 1 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 3, unexpectedly exited: 0).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================================> (44 + 1) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:12:25.252 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 44.0 in stage 3.0 (TID 47) in 161309 ms on datanode1 (executor 3) (45/45)\n",
      "12:12:25.252 [task-result-getter-3] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "12:12:25.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 457.929 s\n",
      "12:12:25.256 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:12:25.256 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 3: Stage finished\n",
      "12:12:25.259 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 457.959577 s\n",
      "12:12:25.267 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 12bc96c1-6666-4d0b-b5b6-474bfd07717c.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:12:25.465 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 12bc96c1-6666-4d0b-b5b6-474bfd07717c committed. Elapsed time: 196 ms.\n",
      "12:12:25.474 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 12bc96c1-6666-4d0b-b5b6-474bfd07717c.\n"
     ]
    }
   ],
   "source": [
    "# Write the results to HDFS in Parquet format\n",
    "output_path = \"hdfs://namenode:9000/data/results/comment_sentiment.parquet\"\n",
    "df.select(\"comment_id\", \"sentiment\").write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:12:25.532 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 13 ms to list leaf files for 1 paths.\n",
      "12:12:25.562 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "12:12:25.563 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:12:25.563 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "12:12:25.563 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:12:25.563 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:12:25.564 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[18] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:12:25.576 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 127.3 KiB, free 2002.8 MiB)\n",
      "12:12:25.580 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 2002.8 MiB)\n",
      "12:12:25.581 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on namenode:37805 (size: 46.9 KiB, free: 2004.0 MiB)\n",
      "12:12:25.581 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "12:12:25.582 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:12:25.582 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 4.0 with 1 tasks resource profile 0\n",
      "12:12:25.583 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 48) (datanode1, executor 3, partition 0, PROCESS_LOCAL, 9232 bytes) \n",
      "12:12:25.615 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on datanode1:41313 (size: 46.9 KiB, free: 911.7 MiB)\n",
      "12:12:25.666 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 48) in 83 ms on datanode1 (executor 3) (1/1)\n",
      "12:12:25.667 [task-result-getter-0] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "12:12:25.668 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.103 s\n",
      "12:12:25.668 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:12:25.668 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 4: Stage finished\n",
      "12:12:25.669 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.106394 s\n",
      "12:12:25.699 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:12:25.699 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:12:25.723 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.887913 ms\n",
      "12:12:25.726 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 408.2 KiB, free 2002.4 MiB)\n",
      "12:12:25.735 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 45.0 KiB, free 2002.3 MiB)\n",
      "12:12:25.738 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on namenode:37805 (size: 45.0 KiB, free: 2003.9 MiB)\n",
      "12:12:25.740 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 9 from showString at NativeMethodAccessorImpl.java:0\n",
      "12:12:25.741 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 42827369 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "12:12:25.755 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "12:12:25.756 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:12:25.756 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\n",
      "12:12:25.756 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:12:25.760 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:12:25.763 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:12:25.770 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 14.9 KiB, free 2002.3 MiB)\n",
      "12:12:25.773 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 2002.3 MiB)\n",
      "12:12:25.773 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on namenode:37805 (size: 6.4 KiB, free: 2003.9 MiB)\n",
      "12:12:25.774 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "12:12:25.774 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:12:25.774 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 5.0 with 1 tasks resource profile 0\n",
      "12:12:25.776 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 49) (datanode1, executor 3, partition 0, NODE_LOCAL, 10804 bytes) \n",
      "12:12:25.798 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on datanode1:41313 (size: 6.4 KiB, free: 911.7 MiB)\n",
      "12:12:25.836 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on datanode1:41313 (size: 45.0 KiB, free: 911.6 MiB)\n",
      "12:12:25.879 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 49) in 104 ms on datanode1 (executor 3) (1/1)\n",
      "12:12:25.879 [task-result-getter-1] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "12:12:25.881 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 0.116 s\n",
      "12:12:25.882 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:12:25.882 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 5: Stage finished\n",
      "12:12:25.883 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 0.127737 s\n",
      "+----------+---------+\n",
      "|comment_id|sentiment|\n",
      "+----------+---------+\n",
      "|fn65r67   |-0.5267  |\n",
      "|fn65qx3   |0.4019   |\n",
      "|fn65qqp   |0.9482   |\n",
      "|fn65qi9   |-0.5267  |\n",
      "|fn65qdm   |0.7717   |\n",
      "|fn65q2w   |0.0      |\n",
      "|fn65pzv   |-0.882   |\n",
      "|fn65p3h   |0.34     |\n",
      "|fn65ovn   |-0.6324  |\n",
      "|fn65o5n   |-0.1675  |\n",
      "+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "12:12:25.957 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:12:25.957 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:12:26.056 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.378923 ms\n",
      "12:12:26.060 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 407.7 KiB, free 2001.9 MiB)\n",
      "12:12:26.070 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 44.8 KiB, free 2001.9 MiB)\n",
      "12:12:26.071 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on namenode:37805 (size: 44.8 KiB, free: 2003.9 MiB)\n",
      "12:12:26.071 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 11 from count at NativeMethodAccessorImpl.java:0\n",
      "12:12:26.072 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 42827369 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "12:12:26.101 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 26 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "12:12:26.107 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "12:12:26.107 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
      "12:12:26.108 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:12:26.109 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:12:26.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:12:26.127 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 16.9 KiB, free 2001.8 MiB)\n",
      "12:12:26.130 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 2001.8 MiB)\n",
      "12:12:26.131 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on namenode:37805 (size: 7.7 KiB, free: 2003.9 MiB)\n",
      "12:12:26.131 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
      "12:12:26.133 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "12:12:26.133 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 6.0 with 6 tasks resource profile 0\n",
      "12:12:26.135 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 50) (datanode1, executor 3, partition 0, NODE_LOCAL, 10793 bytes) \n",
      "12:12:26.136 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 51) (datanode1, executor 5, partition 1, NODE_LOCAL, 10977 bytes) \n",
      "12:12:26.136 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 52) (datanode2, executor 4, partition 2, NODE_LOCAL, 10977 bytes) \n",
      "12:12:26.136 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 53) (datanode1, executor 3, partition 3, NODE_LOCAL, 10977 bytes) \n",
      "12:12:26.136 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 6.0 (TID 54) (datanode1, executor 5, partition 4, NODE_LOCAL, 10977 bytes) \n",
      "12:12:26.136 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 6.0 (TID 55) (datanode2, executor 4, partition 5, NODE_LOCAL, 10609 bytes) \n",
      "12:12:26.161 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on datanode1:41313 (size: 7.7 KiB, free: 911.6 MiB)\n",
      "12:12:26.170 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on datanode2:35337 (size: 7.7 KiB, free: 911.8 MiB)\n",
      "12:12:26.181 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on datanode1:35917 (size: 7.7 KiB, free: 911.8 MiB)\n",
      "12:12:26.239 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on datanode2:35337 (size: 44.8 KiB, free: 911.7 MiB)\n",
      "12:12:26.333 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on datanode1:35917 (size: 44.8 KiB, free: 911.7 MiB)\n",
      "12:12:26.334 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on datanode1:41313 (size: 44.8 KiB, free: 911.6 MiB)\n",
      "12:12:26.388 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 6.0 (TID 55) in 252 ms on datanode2 (executor 4) (1/6)\n",
      "12:12:26.388 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 52) in 252 ms on datanode2 (executor 4) (2/6)\n",
      "12:12:26.540 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 53) in 403 ms on datanode1 (executor 3) (3/6)\n",
      "12:12:26.543 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 50) in 408 ms on datanode1 (executor 3) (4/6)\n",
      "12:12:26.584 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 6.0 (TID 54) in 448 ms on datanode1 (executor 5) (5/6)\n",
      "12:12:26.586 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 51) in 451 ms on datanode1 (executor 5) (6/6)\n",
      "12:12:26.587 [task-result-getter-3] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "12:12:26.587 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.472 s\n",
      "12:12:26.588 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "12:12:26.589 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "12:12:26.591 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "12:12:26.591 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "12:12:26.629 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.631876 ms\n",
      "12:12:26.646 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "12:12:26.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:12:26.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)\n",
      "12:12:26.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7)\n",
      "12:12:26.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:12:26.655 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:12:26.662 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 12.5 KiB, free 2001.8 MiB)\n",
      "12:12:26.665 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 2001.8 MiB)\n",
      "12:12:26.666 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on namenode:37805 (size: 6.0 KiB, free: 2003.9 MiB)\n",
      "12:12:26.667 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "12:12:26.667 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:12:26.667 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 8.0 with 1 tasks resource profile 0\n",
      "12:12:26.669 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 56) (datanode1, executor 3, partition 0, NODE_LOCAL, 9010 bytes) \n",
      "12:12:26.691 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on datanode1:41313 (size: 6.0 KiB, free: 911.6 MiB)\n",
      "12:12:26.712 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - Asked to send map output locations for shuffle 0 to 192.168.12.196:36642\n",
      "12:12:26.847 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 56) in 178 ms on datanode1 (executor 3) (1/1)\n",
      "12:12:26.847 [task-result-getter-0] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "12:12:26.849 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.188 s\n",
      "12:12:26.850 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:12:26.850 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 8: Stage finished\n",
      "12:12:26.851 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.203733 s\n",
      "Total records processed: 8506526\n"
     ]
    }
   ],
   "source": [
    "# Read back the results and display a preview\n",
    "result_df = spark.read.parquet(output_path)\n",
    "result_df.show(10, truncate=False)\n",
    "result_count = result_df.count()\n",
    "print(f\"Total records processed: {result_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:12:26.863 [Thread-4] INFO  org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.\n",
      "12:12:26.875 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector - Stopped Spark@367c2f5d{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "12:12:26.878 [Thread-4] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://namenode:4040\n",
      "12:12:26.885 [YARN application state monitor] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Interrupting monitor thread\n",
      "12:12:26.896 [Thread-4] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Shutting down all executors\n",
      "12:12:26.897 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Asking each executor to shut down\n",
      "12:12:26.901 [Thread-4] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - YARN client scheduler backend Stopped\n",
      "12:12:26.941 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!\n",
      "12:12:26.967 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared\n",
      "12:12:26.967 [Thread-4] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped\n",
      "12:12:26.973 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped\n",
      "12:12:26.976 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!\n",
      "12:12:26.986 [Thread-4] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
