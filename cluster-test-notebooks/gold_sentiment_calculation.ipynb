{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Calculation RDD\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"3\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"9\") \\\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.extraPythonPackages\", \"vaderSentiment\") \\\n",
    "    .config(\"spark.driver.extraPythonPackages\", \"vaderSentiment\") \\\n",
    "    .config(\"spark.executorEnv.PYTHONPATH\", \":\".join(sys.path)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level for cleaner outputs\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:20:49.232 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "12:20:49.239 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/ubuntu/project/cluster-notebooks/spark-warehouse'.\n",
      "12:20:49.248 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:20:49.250 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5178ac23{/SQL,null,AVAILABLE,@Spark}\n",
      "12:20:49.250 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:20:49.251 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50e9ad63{/SQL/json,null,AVAILABLE,@Spark}\n",
      "12:20:49.251 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:20:49.252 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@415c8308{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "12:20:49.252 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:20:49.253 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ec78f2b{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "12:20:49.253 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "12:20:49.254 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13085a5d{/static/sql,null,AVAILABLE,@Spark}\n",
      "12:20:49.980 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 68 ms to list leaf files for 1 paths.\n",
      "12:20:50.195 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "12:20:50.243 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:20:50.243 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "12:20:50.243 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:20:50.244 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:20:50.248 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:20:50.307 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 127.3 KiB, free 2004.5 MiB)\n",
      "12:20:50.333 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 2004.4 MiB)\n",
      "12:20:50.335 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on namenode:40137 (size: 46.9 KiB, free: 2004.6 MiB)\n",
      "12:20:50.342 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "12:20:50.358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:20:50.359 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "12:20:50.386 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (datanode2, executor 1, partition 0, PROCESS_LOCAL, 9222 bytes) \n",
      "12:20:50.656 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on datanode2:39793 (size: 46.9 KiB, free: 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:20:52.180 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1805 ms on datanode2 (executor 1) (1/1)\n",
      "12:20:52.183 [task-result-getter-0] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "12:20:52.188 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.923 s\n",
      "12:20:52.192 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:20:52.193 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 0: Stage finished\n",
      "12:20:52.195 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.000233 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:20:52.422 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on namenode:40137 in memory (size: 46.9 KiB, free: 2004.6 MiB)\n",
      "12:20:52.438 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on datanode2:39793 in memory (size: 46.9 KiB, free: 912.3 MiB)\n",
      "root\n",
      " |-- link: string (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- sub_reddit: string (nullable = true)\n",
      " |-- post_id: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      "\n",
      "12:20:53.210 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:20:53.211 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:20:53.554 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 185.934852 ms\n",
      "12:20:53.577 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 409.4 KiB, free 2004.2 MiB)\n",
      "12:20:53.586 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 45.1 KiB, free 2004.2 MiB)\n",
      "12:20:53.586 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on namenode:40137 (size: 45.1 KiB, free: 2004.6 MiB)\n",
      "12:20:53.587 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0\n",
      "12:20:53.598 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "12:20:53.636 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "12:20:53.640 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:20:53.640 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "12:20:53.640 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:20:53.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:20:53.644 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:20:53.682 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 18.2 KiB, free 2004.1 MiB)\n",
      "12:20:53.683 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 2004.1 MiB)\n",
      "12:20:53.683 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on namenode:40137 (size: 6.9 KiB, free: 2004.5 MiB)\n",
      "12:20:53.684 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "12:20:53.685 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:20:53.685 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "12:20:53.694 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (datanode2, executor 1, partition 0, NODE_LOCAL, 9690 bytes) \n",
      "12:20:53.739 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on datanode2:39793 (size: 6.9 KiB, free: 912.3 MiB)\n",
      "12:20:54.136 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on datanode2:39793 (size: 45.1 KiB, free: 912.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:20:55.049 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1358 ms on datanode2 (executor 1) (1/1)\n",
      "12:20:55.049 [task-result-getter-1] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "12:20:55.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 1.399 s\n",
      "12:20:55.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:20:55.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 1: Stage finished\n",
      "12:20:55.052 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.415584 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:20:55.953 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.469073 ms\n",
      "+----------------------------------------------------------------------------------------------------------------+-----------+-----+------------+-------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|link                                                                                                            |created_utc|score|sub_reddit  |post_id|comment_id|body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+----------------------------------------------------------------------------------------------------------------+-----------+-----+------------+-------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|https://old.reddit.com/r/AmalaNetwork/comments/lv6zt4/joe_biden_says_his_hands_are_tied_on_a_15_minimum/gpbtnii/|1614624690 |3    |AmalaNetwork|lv6zt4 |gpbtnii   |might be an unpopular opinion but i dont think its worth holding up the covid relief bill over this any longer. people are going hungry and getting evicted right now enhanced ui and assistance should be the priority in the moment.                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|https://old.reddit.com/r/Seattle/comments/lvbojq/seattle_to_operate_mass_vaccination_site_at_lumen/gpbtmvu/     |1614624683 |17   |Seattle     |lvbojq |gpbtmvu   |king county has used 90 of their doses way higher than our state avg. its not seattle that is the issue. also it is 9 if you look at the king county numbers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|https://old.reddit.com/r/samharris/comments/lv58pu/politics_and_current_events_megathread_march_2021/gpbtmqv/   |1614624681 |7    |samharris   |lv58pu |gpbtmqv   |sam seems to think theres a significant problem of rich people and their tax money leaving california and moving to other states from people i know in my network i can account for probably 100s of millions of dollars in the tax base evaporating in california. from episode 232. however according to this httpswww.nytimes.com20210301businesscovidstatetaxrevenue.htmlhttpswww.nytimes.com20210301businesscovidstatetaxrevenue.html california state tax revenue actually increased 1.2 from 2019 to 2020.                                                                                                                                                   |\n",
      "|https://old.reddit.com/r/NBASpurs/comments/lvfm3v/fans_at_home_games_starting_mar_12/gpbtmh9/                   |1614624678 |4    |NBASpurs    |lvfm3v |gpbtmh9   |man im really thinking of driving 7 hours to see a game. covid cancelling a game is really scary though                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|https://old.reddit.com/r/Reformed/comments/lvfncl/attending_a_church_if_skeptical_of_the_pastors/gpbtlsk/       |1614624669 |13   |Reformed    |lvfncl |gpbtlsk   |judging a church and a person by their response s to a worldwide pandemic one which we thought was a biological attack at one point then settled into the dullknife dailydrama of fear and more death than anyone imagined lets not imagine that anyone has handled this great. i dont know anyone who has. including the churches who closed march 17 and are still closedthats not great either. if you dont want to attend this church dont base it on how they scored on their covid19 response bingo card. do it based on doctrine dogma and if they have cute boys there like always. not on a certain response to this exceptional weird time in our culture.|\n",
      "+----------------------------------------------------------------------------------------------------------------+-----------+-----+------------+-------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HDFS path for the dataset\n",
    "file_path = \"hdfs://namenode:9000/data/cleaned_dataset.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# Display schema and sample rows\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean comments\n",
    "def clean_comment_spark(df, column):\n",
    "    \"\"\"Clean comments in the specified column.\"\"\"\n",
    "    return df.withColumn(\n",
    "        f\"{column}_clean\",\n",
    "        F.trim(\n",
    "            F.regexp_replace(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.lower(F.col(column)),  # Convert to lowercase\n",
    "                        r\"http\\S+|www\\S+|https\\S+\", \"\"),  # Remove URLs\n",
    "                    r\"@\\w+|#\", \"\"),  # Remove mentions and hashtags\n",
    "                r\"[^\\w\\s]\", \"\"),  # Remove special characters and punctuation\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:20:56.132 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:20:56.133 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:20:56.205 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 43.896746 ms\n",
      "12:20:56.209 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 408.1 KiB, free 2003.7 MiB)\n",
      "12:20:56.220 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 44.8 KiB, free 2003.7 MiB)\n",
      "12:20:56.220 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on namenode:40137 (size: 44.8 KiB, free: 2004.5 MiB)\n",
      "12:20:56.222 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "12:20:56.223 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "12:20:56.234 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "12:20:56.235 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:20:56.235 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "12:20:56.235 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:20:56.237 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:20:56.239 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:20:56.249 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 20.4 KiB, free 2003.7 MiB)\n",
      "12:20:56.282 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 2003.7 MiB)\n",
      "12:20:56.283 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on namenode:40137 (size: 7.9 KiB, free: 2004.5 MiB)\n",
      "12:20:56.284 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "12:20:56.285 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:20:56.285 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "12:20:56.290 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on namenode:40137 in memory (size: 6.9 KiB, free: 2004.5 MiB)\n",
      "12:20:56.292 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (datanode2, executor 1, partition 0, NODE_LOCAL, 9690 bytes) \n",
      "12:20:56.295 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on datanode2:39793 in memory (size: 6.9 KiB, free: 912.3 MiB)\n",
      "12:20:56.306 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on namenode:40137 in memory (size: 45.1 KiB, free: 2004.5 MiB)\n",
      "12:20:56.310 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on datanode2:39793 in memory (size: 45.1 KiB, free: 912.3 MiB)\n",
      "12:20:56.319 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on datanode2:39793 (size: 7.9 KiB, free: 912.3 MiB)\n",
      "12:20:56.387 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on datanode2:39793 (size: 44.8 KiB, free: 912.2 MiB)\n",
      "12:20:56.515 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 223 ms on datanode2 (executor 1) (1/1)\n",
      "12:20:56.516 [task-result-getter-2] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "12:20:56.517 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.274 s\n",
      "12:20:56.518 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:20:56.518 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 2: Stage finished\n",
      "12:20:56.521 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.286726 s\n",
      "12:20:56.536 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.107991 ms\n",
      "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|comment_id|body_clean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|gpbtnii   |might be an unpopular opinion but i dont think its worth holding up the covid relief bill over this any longer people are going hungry and getting evicted right now enhanced ui and assistance should be the priority in the moment                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|gpbtmvu   |king county has used 90 of their doses way higher than our state avg its not seattle that is the issue also it is 9 if you look at the king county numbers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|gpbtmqv   |sam seems to think theres a significant problem of rich people and their tax money leaving california and moving to other states from people i know in my network i can account for probably 100s of millions of dollars in the tax base evaporating in california from episode 232 however according to this  california state tax revenue actually increased 12 from 2019 to 2020                                                                                                                                                                                                                                                                           |\n",
      "|gpbtmh9   |man im really thinking of driving 7 hours to see a game covid cancelling a game is really scary though                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|gpbtlsk   |judging a church and a person by their response s to a worldwide pandemic one which we thought was a biological attack at one point then settled into the dullknife dailydrama of fear and more death than anyone imagined lets not imagine that anyone has handled this great i dont know anyone who has including the churches who closed march 17 and are still closedthats not great either if you dont want to attend this church dont base it on how they scored on their covid19 response bingo card do it based on doctrine dogma and if they have cute boys there like always not on a certain response to this exceptional weird time in our culture|\n",
      "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean the comments and select relevant columns\n",
    "df = clean_comment_spark(df, \"body\").select(\"comment_id\", \"body_clean\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:20:56.555 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:20:56.555 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:20:56.590 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.564544 ms\n",
      "12:20:56.594 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 408.1 KiB, free 2003.7 MiB)\n",
      "12:20:56.601 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 44.8 KiB, free 2003.7 MiB)\n",
      "12:20:56.602 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on namenode:40137 (size: 44.8 KiB, free: 2004.5 MiB)\n",
      "12:20:56.605 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 5 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "12:20:56.606 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to RDD for sentiment analysis\n",
    "data_rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate sentiment for a partition\n",
    "def calculate_sentiment_partition(rows):\n",
    "    analyzer = SentimentIntensityAnalyzer()  # Initialize the analyzer once per partition\n",
    "    results = []\n",
    "    for row in rows:\n",
    "        comment_id = row[\"comment_id\"]\n",
    "        text = row[\"body_clean\"]\n",
    "        sentiment_score = analyzer.polarity_scores(text)['compound'] if text else None\n",
    "        results.append((comment_id, sentiment_score))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:20:56.681 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: runJob at PythonRDD.scala:181\n",
      "12:20:56.685 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "12:20:56.686 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (runJob at PythonRDD.scala:181)\n",
      "12:20:56.686 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:20:56.688 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:20:56.689 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (PythonRDD[16] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "12:20:56.710 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 30.1 KiB, free 2003.7 MiB)\n",
      "12:20:56.713 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 2003.6 MiB)\n",
      "12:20:56.714 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on namenode:40137 (size: 12.7 KiB, free: 2004.5 MiB)\n",
      "12:20:56.715 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "12:20:56.717 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (PythonRDD[16] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "12:20:56.717 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 3.0 with 1 tasks resource profile 0\n",
      "12:20:56.721 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3) (datanode1, executor 3, partition 0, NODE_LOCAL, 9690 bytes) \n",
      "12:20:56.927 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode1:44859 (size: 12.7 KiB, free: 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:20:58.528 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode1:44859 (size: 44.8 KiB, free: 912.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:22:27.631 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 90911 ms on datanode1 (executor 3) (1/1)\n",
      "12:22:27.632 [task-result-getter-3] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "12:22:27.637 [dag-scheduler-event-loop] INFO  org.apache.spark.api.python.PythonAccumulatorV2 - Connected to AccumulatorServer at host: 127.0.0.1 port: 51923\n",
      "12:22:27.639 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (runJob at PythonRDD.scala:181) finished in 90.944 s\n",
      "12:22:27.639 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:22:27.639 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 3: Stage finished\n",
      "12:22:27.640 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: runJob at PythonRDD.scala:181, took 90.958697 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Apply the function to each partition\n",
    "sentiment_rdd = data_rdd.mapPartitions(calculate_sentiment_partition)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "sentiment_df = sentiment_rdd.toDF([\"comment_id\", \"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:22:27.798 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "12:22:27.811 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "12:22:27.811 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "12:22:27.812 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "12:22:27.812 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "12:22:27.812 [Thread-4] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "12:22:27.812 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "12:22:27.824 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.714935 ms\n",
      "12:22:27.830 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "12:22:27.831 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 45 output partitions\n",
      "12:22:27.831 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "12:22:27.831 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:22:27.833 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:22:27.834 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[22] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:22:27.867 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 284.7 KiB, free 2003.4 MiB)\n",
      "12:22:27.869 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 106.3 KiB, free 2003.3 MiB)\n",
      "12:22:27.870 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on namenode:40137 (size: 106.3 KiB, free: 2004.4 MiB)\n",
      "12:22:27.870 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "12:22:27.871 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 45 missing tasks from ResultStage 4 (MapPartitionsRDD[22] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "12:22:27.871 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 4.0 with 45 tasks resource profile 0\n",
      "12:22:27.872 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4) (datanode2, executor 1, partition 0, NODE_LOCAL, 9690 bytes) \n",
      "12:22:27.872 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 5) (datanode1, executor 3, partition 1, NODE_LOCAL, 9690 bytes) \n",
      "12:22:27.873 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 4.0 (TID 6) (datanode3, executor 2, partition 2, NODE_LOCAL, 9690 bytes) \n",
      "12:22:27.873 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 4.0 (TID 7) (datanode2, executor 1, partition 3, NODE_LOCAL, 9690 bytes) \n",
      "12:22:27.873 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 4.0 (TID 8) (datanode1, executor 3, partition 4, NODE_LOCAL, 9690 bytes) \n",
      "12:22:27.873 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 4.0 (TID 9) (datanode3, executor 2, partition 5, NODE_LOCAL, 9690 bytes) \n",
      "12:22:27.898 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode2:39793 (size: 106.3 KiB, free: 912.1 MiB)\n",
      "12:22:27.906 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode1:44859 (size: 106.3 KiB, free: 912.1 MiB)\n",
      "12:22:28.113 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode3:33879 (size: 106.3 KiB, free: 912.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                         (0 + 6) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:22:28.782 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode2:39793 (size: 44.8 KiB, free: 912.1 MiB)\n",
      "12:22:28.989 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 1 new executor because tasks are backlogged (new desired total will be 4 for resource profile id: 0)\n",
      "12:22:30.004 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 2 new executors because tasks are backlogged (new desired total will be 6 for resource profile id: 0)\n",
      "12:22:30.010 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode3:33879 (size: 44.8 KiB, free: 912.2 MiB)\n",
      "12:22:31.018 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Requesting 3 new executors because tasks are backlogged (new desired total will be 9 for resource profile id: 0)\n",
      "12:22:32.766 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.12.222:60106) with ID 4,  ResourceProfileId 0\n",
      "12:22:32.767 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 4 has registered (new total is 4)\n",
      "12:22:32.870 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager datanode2:38315 with 912.3 MiB RAM, BlockManagerId(4, datanode2, 38315, None)\n",
      "12:22:32.912 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 4.0 (TID 10) (datanode2, executor 4, partition 6, NODE_LOCAL, 9690 bytes) \n",
      "12:22:32.913 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 4.0 (TID 11) (datanode2, executor 4, partition 7, NODE_LOCAL, 9690 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                         (0 + 8) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:22:33.266 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode2:38315 (size: 106.3 KiB, free: 912.2 MiB)\n",
      "12:22:33.351 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.12.196:48010) with ID 6,  ResourceProfileId 0\n",
      "12:22:33.352 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 6 has registered (new total is 5)\n",
      "12:22:33.408 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager datanode1:42309 with 912.3 MiB RAM, BlockManagerId(6, datanode1, 42309, None)\n",
      "12:22:33.448 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 4.0 (TID 12) (datanode1, executor 6, partition 8, NODE_LOCAL, 9690 bytes) \n",
      "12:22:33.448 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 4.0 (TID 13) (datanode1, executor 6, partition 9, NODE_LOCAL, 9690 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (0 + 10) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:22:33.725 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode1:42309 (size: 106.3 KiB, free: 912.2 MiB)\n",
      "12:22:34.577 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.12.118:60710) with ID 5,  ResourceProfileId 0\n",
      "12:22:34.579 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - New executor 5 has registered (new total is 6)\n",
      "12:22:34.738 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager datanode3:36221 with 912.3 MiB RAM, BlockManagerId(5, datanode3, 36221, None)\n",
      "12:22:34.821 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 4.0 (TID 14) (datanode3, executor 5, partition 10, NODE_LOCAL, 9690 bytes) \n",
      "12:22:34.821 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 4.0 (TID 15) (datanode3, executor 5, partition 11, NODE_LOCAL, 9690 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (0 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:22:35.231 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode3:36221 (size: 106.3 KiB, free: 912.2 MiB)\n",
      "12:22:35.492 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode2:38315 (size: 44.8 KiB, free: 912.2 MiB)\n",
      "12:22:35.716 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode1:42309 (size: 44.8 KiB, free: 912.2 MiB)\n",
      "12:22:37.602 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode3:36221 (size: 44.8 KiB, free: 912.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (0 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:01.963 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 4.0 (TID 16) (datanode1, executor 3, partition 12, NODE_LOCAL, 9690 bytes) \n",
      "12:24:01.967 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 4.0 (TID 8) in 94094 ms on datanode1 (executor 3) (1/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=>                                                       (1 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:04.214 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 4.0 (TID 17) (datanode1, executor 3, partition 13, NODE_LOCAL, 9690 bytes) \n",
      "12:24:04.215 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 5) in 96343 ms on datanode1 (executor 3) (2/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==>                                                      (2 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:04.622 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 4.0 (TID 18) (datanode2, executor 1, partition 14, NODE_LOCAL, 9690 bytes) \n",
      "12:24:04.622 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 96750 ms on datanode2 (executor 1) (3/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===>                                                     (3 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:06.073 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 4.0 (TID 19) (datanode3, executor 2, partition 15, NODE_LOCAL, 9690 bytes) \n",
      "12:24:06.074 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 4.0 (TID 9) in 98201 ms on datanode3 (executor 2) (4/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=====>                                                   (4 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:07.007 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 4.0 (TID 20) (datanode2, executor 1, partition 16, NODE_LOCAL, 9690 bytes) \n",
      "12:24:07.008 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 4.0 (TID 7) in 99135 ms on datanode2 (executor 1) (5/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======>                                                  (5 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:08.245 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 4.0 (TID 21) (datanode2, executor 4, partition 17, NODE_LOCAL, 9690 bytes) \n",
      "12:24:08.246 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 4.0 (TID 11) in 95333 ms on datanode2 (executor 4) (6/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======>                                                 (6 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:09.066 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 4.0 (TID 22) (datanode3, executor 2, partition 18, NODE_LOCAL, 9690 bytes) \n",
      "12:24:09.067 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 4.0 (TID 6) in 101195 ms on datanode3 (executor 2) (7/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:========>                                                (7 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:10.558 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 4.0 (TID 23) (datanode2, executor 4, partition 19, NODE_LOCAL, 9690 bytes) \n",
      "12:24:10.559 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 4.0 (TID 10) in 97647 ms on datanode2 (executor 4) (8/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==========>                                              (8 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:11.136 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 4.0 (TID 24) (datanode1, executor 6, partition 20, NODE_LOCAL, 9690 bytes) \n",
      "12:24:11.137 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 4.0 (TID 12) in 97690 ms on datanode1 (executor 6) (9/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========>                                             (9 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:12.008 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 4.0 (TID 25) (datanode1, executor 6, partition 21, NODE_LOCAL, 9690 bytes) \n",
      "12:24:12.010 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 4.0 (TID 13) in 98562 ms on datanode1 (executor 6) (10/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============>                                          (11 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:12.256 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 4.0 (TID 26) (datanode3, executor 5, partition 22, NODE_LOCAL, 9690 bytes) \n",
      "12:24:12.257 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 4.0 (TID 15) in 97436 ms on datanode3 (executor 5) (11/45)\n",
      "12:24:15.367 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 4.0 (TID 27) (datanode3, executor 5, partition 23, NODE_LOCAL, 9690 bytes) \n",
      "12:24:15.367 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 4.0 (TID 14) in 100547 ms on datanode3 (executor 5) (12/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==============>                                         (12 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:32.056 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 4.0 (TID 28) (datanode1, executor 3, partition 24, NODE_LOCAL, 9690 bytes) \n",
      "12:25:32.057 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 4.0 (TID 16) in 90095 ms on datanode1 (executor 3) (13/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:================>                                       (13 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:37.397 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 4.0 (TID 29) (datanode2, executor 1, partition 25, NODE_LOCAL, 9690 bytes) \n",
      "12:25:37.399 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 4.0 (TID 20) in 90392 ms on datanode2 (executor 1) (14/45)\n",
      "12:25:37.586 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 4.0 (TID 30) (datanode2, executor 1, partition 26, NODE_LOCAL, 9690 bytes) \n",
      "12:25:37.590 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 4.0 (TID 18) in 92968 ms on datanode2 (executor 1) (15/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================>                                     (15 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:40.886 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 4.0 (TID 31) (datanode2, executor 4, partition 27, NODE_LOCAL, 9690 bytes) \n",
      "12:25:40.889 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 4.0 (TID 21) in 92645 ms on datanode2 (executor 4) (16/45)\n",
      "12:25:40.909 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 4.0 (TID 32) (datanode1, executor 3, partition 28, NODE_LOCAL, 9690 bytes) \n",
      "12:25:40.910 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 4.0 (TID 17) in 96697 ms on datanode1 (executor 3) (17/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=====================>                                  (17 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:41.315 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 4.0 (TID 33) (datanode1, executor 6, partition 29, NODE_LOCAL, 9690 bytes) \n",
      "12:25:41.317 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 4.0 (TID 25) in 89309 ms on datanode1 (executor 6) (18/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======================>                                 (18 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:41.741 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 4.0 (TID 34) (datanode3, executor 2, partition 30, NODE_LOCAL, 9690 bytes) \n",
      "12:25:41.742 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 4.0 (TID 19) in 95670 ms on datanode3 (executor 2) (19/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================>                                (19 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:42.091 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 31.0 in stage 4.0 (TID 35) (datanode3, executor 2, partition 31, NODE_LOCAL, 9690 bytes) \n",
      "12:25:42.092 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 4.0 (TID 22) in 93026 ms on datanode3 (executor 2) (20/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:========================>                               (20 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:44.572 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 32.0 in stage 4.0 (TID 36) (datanode3, executor 5, partition 32, NODE_LOCAL, 9690 bytes) \n",
      "12:25:44.573 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 4.0 (TID 26) in 92317 ms on datanode3 (executor 5) (21/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==========================>                             (21 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:45.353 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 33.0 in stage 4.0 (TID 37) (datanode1, executor 6, partition 33, NODE_LOCAL, 9690 bytes) \n",
      "12:25:45.354 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 4.0 (TID 24) in 94219 ms on datanode1 (executor 6) (22/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========================>                            (22 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:46.590 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 34.0 in stage 4.0 (TID 38) (datanode2, executor 4, partition 34, NODE_LOCAL, 9690 bytes) \n",
      "12:25:46.591 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 4.0 (TID 23) in 96033 ms on datanode2 (executor 4) (23/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============================>                           (23 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:25:50.584 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 35.0 in stage 4.0 (TID 39) (datanode3, executor 5, partition 35, NODE_LOCAL, 9690 bytes) \n",
      "12:25:50.586 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 4.0 (TID 27) in 95220 ms on datanode3 (executor 5) (24/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                          (24 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:06.585 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 36.0 in stage 4.0 (TID 40) (datanode1, executor 3, partition 36, NODE_LOCAL, 9690 bytes) \n",
      "12:27:06.586 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 24.0 in stage 4.0 (TID 28) in 94531 ms on datanode1 (executor 3) (25/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===============================>                        (25 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:08.874 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 37.0 in stage 4.0 (TID 41) (datanode2, executor 1, partition 37, NODE_LOCAL, 9690 bytes) \n",
      "12:27:08.875 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 4.0 (TID 29) in 91478 ms on datanode2 (executor 1) (26/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:================================>                       (26 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:09.825 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 38.0 in stage 4.0 (TID 42) (datanode1, executor 3, partition 38, NODE_LOCAL, 9690 bytes) \n",
      "12:27:09.825 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 4.0 (TID 32) in 88916 ms on datanode1 (executor 3) (27/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=================================>                      (27 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:10.562 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 39.0 in stage 4.0 (TID 43) (datanode3, executor 2, partition 39, NODE_LOCAL, 9690 bytes) \n",
      "12:27:10.563 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 31.0 in stage 4.0 (TID 35) in 88473 ms on datanode3 (executor 2) (28/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================>                     (28 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:12.108 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 40.0 in stage 4.0 (TID 44) (datanode1, executor 6, partition 40, NODE_LOCAL, 9690 bytes) \n",
      "12:27:12.109 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 29.0 in stage 4.0 (TID 33) in 90794 ms on datanode1 (executor 6) (29/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================>                   (29 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:12.528 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 41.0 in stage 4.0 (TID 45) (datanode2, executor 4, partition 41, NODE_LOCAL, 9690 bytes) \n",
      "12:27:12.529 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 34.0 in stage 4.0 (TID 38) in 85940 ms on datanode2 (executor 4) (30/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=====================================>                  (30 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:13.338 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 42.0 in stage 4.0 (TID 46) (datanode2, executor 4, partition 42, NODE_LOCAL, 9690 bytes) \n",
      "12:27:13.339 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 27.0 in stage 4.0 (TID 31) in 92454 ms on datanode2 (executor 4) (31/45)\n",
      "12:27:13.370 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 43.0 in stage 4.0 (TID 47) (datanode2, executor 1, partition 43, NODE_LOCAL, 9690 bytes) \n",
      "12:27:13.371 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 4.0 (TID 30) in 95785 ms on datanode2 (executor 1) (32/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================>                (32 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:14.411 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 44.0 in stage 4.0 (TID 48) (datanode1, executor 6, partition 44, NODE_LOCAL, 9864 bytes) \n",
      "12:27:14.412 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 33.0 in stage 4.0 (TID 37) in 89059 ms on datanode1 (executor 6) (33/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=========================================>              (33 + 12) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:17.768 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 35.0 in stage 4.0 (TID 39) in 87184 ms on datanode3 (executor 5) (34/45)\n",
      "12:27:17.797 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 4.0 (TID 34) in 96056 ms on datanode3 (executor 2) (35/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========================================>            (35 + 10) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:27:19.818 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 32.0 in stage 4.0 (TID 36) in 95246 ms on datanode3 (executor 5) (36/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================================>           (36 + 9) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:28:19.860 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Requesting to kill executor(s) 5\n",
      "12:28:19.866 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Actual list of executor(s) to be killed is 5\n",
      "12:28:19.924 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Executors 5 removed due to idle timeout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================================>           (36 + 9) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:28:21.242 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Disabling executor 5.\n",
      "12:28:21.250 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Executor lost: 5 (epoch 0)\n",
      "12:28:21.251 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor 5 from BlockManagerMaster.\n",
      "12:28:21.252 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(5, datanode3, 36221, None)\n",
      "12:28:21.253 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.BlockManagerMaster - Removed 5 successfully in removeExecutor\n",
      "12:28:21.253 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Shuffle files lost for executor: 5 (epoch 0)\n",
      "12:28:21.273 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Executor 5 on datanode3 killed by driver.\n",
      "12:28:21.279 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - Executor 5 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 1, unexpectedly exited: 0).\n",
      "12:28:32.627 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 36.0 in stage 4.0 (TID 40) in 86041 ms on datanode1 (executor 3) (37/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==============================================>          (37 + 8) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:28:37.185 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 38.0 in stage 4.0 (TID 42) in 87361 ms on datanode1 (executor 3) (38/45)\n",
      "12:28:37.291 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 41.0 in stage 4.0 (TID 45) in 84764 ms on datanode2 (executor 4) (39/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=================================================>       (39 + 6) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:28:37.525 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 37.0 in stage 4.0 (TID 41) in 88652 ms on datanode2 (executor 1) (40/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>      (40 + 5) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:28:38.194 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 42.0 in stage 4.0 (TID 46) in 84856 ms on datanode2 (executor 4) (41/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================================>     (41 + 4) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:28:40.417 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 43.0 in stage 4.0 (TID 47) in 87047 ms on datanode2 (executor 1) (42/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=====================================================>   (42 + 3) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:28:41.655 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 40.0 in stage 4.0 (TID 44) in 89547 ms on datanode1 (executor 6) (43/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======================================================>  (43 + 2) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:28:51.447 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 39.0 in stage 4.0 (TID 43) in 100885 ms on datanode3 (executor 2) (44/45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================================> (44 + 1) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:29:37.279 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Requesting to kill executor(s) 3\n",
      "12:29:37.280 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Actual list of executor(s) to be killed is 3\n",
      "12:29:37.301 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Executors 3 removed due to idle timeout.\n",
      "12:29:37.836 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Disabling executor 3.\n",
      "12:29:37.837 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Executor lost: 3 (epoch 1)\n",
      "12:29:37.838 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor 3 from BlockManagerMaster.\n",
      "12:29:37.838 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(3, datanode1, 44859, None)\n",
      "12:29:37.839 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.BlockManagerMaster - Removed 3 successfully in removeExecutor\n",
      "12:29:37.839 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Shuffle files lost for executor: 3 (epoch 1)\n",
      "12:29:37.841 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Executor 3 on datanode1 killed by driver.\n",
      "12:29:37.842 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - Executor 3 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 2, unexpectedly exited: 0).\n",
      "12:29:38.204 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Requesting to kill executor(s) 4\n",
      "12:29:38.204 [spark-dynamic-executor-allocation] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Actual list of executor(s) to be killed is 4\n",
      "12:29:38.215 [spark-dynamic-executor-allocation] INFO  org.apache.spark.ExecutorAllocationManager - Executors 4 removed due to idle timeout.\n",
      "12:29:41.304 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Disabling executor 4.\n",
      "12:29:41.305 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Executor lost: 4 (epoch 2)\n",
      "12:29:41.306 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor 4 from BlockManagerMaster.\n",
      "12:29:41.306 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(4, datanode2, 38315, None)\n",
      "12:29:41.306 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.BlockManagerMaster - Removed 4 successfully in removeExecutor\n",
      "12:29:41.307 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Shuffle files lost for executor: 4 (epoch 2)\n",
      "12:29:41.310 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Executor 4 on datanode2 killed by driver.\n",
      "12:29:41.311 [spark-listener-group-executorManagement] INFO  org.apache.spark.scheduler.dynalloc.ExecutorMonitor - Executor 4 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 3, unexpectedly exited: 0).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================================> (44 + 1) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:29:54.519 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 44.0 in stage 4.0 (TID 48) in 160109 ms on datanode1 (executor 6) (45/45)\n",
      "12:29:54.520 [task-result-getter-0] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "12:29:54.523 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 446.681 s\n",
      "12:29:54.524 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:29:54.524 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 4: Stage finished\n",
      "12:29:54.525 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 446.694885 s\n",
      "12:29:54.532 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 2fce8e30-8a0b-4f49-84f0-124ad39ef127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:29:54.773 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 2fce8e30-8a0b-4f49-84f0-124ad39ef127 committed. Elapsed time: 238 ms.\n",
      "12:29:54.778 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 2fce8e30-8a0b-4f49-84f0-124ad39ef127.\n"
     ]
    }
   ],
   "source": [
    "# Write the results to HDFS in Parquet format\n",
    "output_path = \"hdfs://namenode:9000/data/results/comment_sentiment.parquet\"\n",
    "sentiment_df.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:29:54.831 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 16 ms to list leaf files for 1 paths.\n",
      "12:29:54.862 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "12:29:54.863 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:29:54.863 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "12:29:54.863 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:29:54.863 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:29:54.864 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[24] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:29:54.872 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 127.3 KiB, free 2003.1 MiB)\n",
      "12:29:54.874 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 2003.1 MiB)\n",
      "12:29:54.874 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on namenode:40137 (size: 46.9 KiB, free: 2004.3 MiB)\n",
      "12:29:54.875 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "12:29:54.875 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:29:54.875 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 5.0 with 1 tasks resource profile 0\n",
      "12:29:54.877 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 49) (datanode1, executor 6, partition 0, PROCESS_LOCAL, 9232 bytes) \n",
      "12:29:54.911 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on datanode1:42309 (size: 46.9 KiB, free: 912.1 MiB)\n",
      "12:29:55.032 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 49) in 155 ms on datanode1 (executor 6) (1/1)\n",
      "12:29:55.032 [task-result-getter-1] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "12:29:55.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.167 s\n",
      "12:29:55.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:29:55.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 5: Stage finished\n",
      "12:29:55.033 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.171074 s\n",
      "12:29:55.053 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:29:55.053 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:29:55.076 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.046 ms\n",
      "12:29:55.080 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 408.1 KiB, free 2002.7 MiB)\n",
      "12:29:55.093 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 44.9 KiB, free 2002.7 MiB)\n",
      "12:29:55.095 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on namenode:40137 (size: 44.9 KiB, free: 2004.3 MiB)\n",
      "12:29:55.096 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 9 from showString at NativeMethodAccessorImpl.java:0\n",
      "12:29:55.098 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 42978197 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "12:29:55.108 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "12:29:55.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:29:55.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\n",
      "12:29:55.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:29:55.111 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:29:55.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[28] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:29:55.117 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 14.9 KiB, free 2002.6 MiB)\n",
      "12:29:55.119 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 2002.6 MiB)\n",
      "12:29:55.121 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on namenode:40137 (size: 6.4 KiB, free: 2004.3 MiB)\n",
      "12:29:55.122 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "12:29:55.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[28] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:29:55.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 6.0 with 1 tasks resource profile 0\n",
      "12:29:55.125 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 50) (datanode2, executor 1, partition 0, NODE_LOCAL, 10804 bytes) \n",
      "12:29:55.144 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on datanode2:39793 (size: 6.4 KiB, free: 912.1 MiB)\n",
      "12:29:55.178 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on datanode2:39793 (size: 44.9 KiB, free: 912.1 MiB)\n",
      "12:29:55.216 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 50) in 92 ms on datanode2 (executor 1) (1/1)\n",
      "12:29:55.216 [task-result-getter-2] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "12:29:55.217 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 0.103 s\n",
      "12:29:55.217 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:29:55.218 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 6: Stage finished\n",
      "12:29:55.218 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.109332 s\n",
      "+----------+---------+\n",
      "|comment_id|sentiment|\n",
      "+----------+---------+\n",
      "|fn65r67   |-0.5267  |\n",
      "|fn65qx3   |0.4019   |\n",
      "|fn65qqp   |0.9482   |\n",
      "|fn65qi9   |-0.5267  |\n",
      "|fn65qdm   |0.7717   |\n",
      "|fn65q2w   |0.0      |\n",
      "|fn65pzv   |-0.882   |\n",
      "|fn65p3h   |0.34     |\n",
      "|fn65ovn   |-0.6324  |\n",
      "|fn65o5n   |-0.1675  |\n",
      "+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "12:29:55.315 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "12:29:55.315 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "12:29:55.425 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.89374 ms\n",
      "12:29:55.430 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 407.6 KiB, free 2002.2 MiB)\n",
      "12:29:55.440 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 44.7 KiB, free 2002.2 MiB)\n",
      "12:29:55.441 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on namenode:40137 (size: 44.7 KiB, free: 2004.2 MiB)\n",
      "12:29:55.442 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 11 from count at NativeMethodAccessorImpl.java:0\n",
      "12:29:55.443 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 42978197 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "12:29:55.478 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 32 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "12:29:55.483 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 7 (count at NativeMethodAccessorImpl.java:0) with 6 output partitions\n",
      "12:29:55.484 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 7 (count at NativeMethodAccessorImpl.java:0)\n",
      "12:29:55.484 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "12:29:55.485 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:29:55.487 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 7 (MapPartitionsRDD[32] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:29:55.507 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 16.9 KiB, free 2002.2 MiB)\n",
      "12:29:55.509 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 2002.2 MiB)\n",
      "12:29:55.510 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on namenode:40137 (size: 7.7 KiB, free: 2004.2 MiB)\n",
      "12:29:55.511 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
      "12:29:55.512 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[32] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "12:29:55.512 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 7.0 with 6 tasks resource profile 0\n",
      "12:29:55.514 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 51) (datanode1, executor 6, partition 0, NODE_LOCAL, 10793 bytes) \n",
      "12:29:55.514 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 52) (datanode3, executor 2, partition 1, NODE_LOCAL, 10977 bytes) \n",
      "12:29:55.515 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 53) (datanode2, executor 1, partition 2, NODE_LOCAL, 10977 bytes) \n",
      "12:29:55.515 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 7.0 (TID 54) (datanode1, executor 6, partition 3, NODE_LOCAL, 10977 bytes) \n",
      "12:29:55.515 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 7.0 (TID 55) (datanode3, executor 2, partition 4, NODE_LOCAL, 10977 bytes) \n",
      "12:29:55.515 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 7.0 (TID 56) (datanode2, executor 1, partition 5, NODE_LOCAL, 10609 bytes) \n",
      "12:29:55.534 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on datanode2:39793 (size: 7.7 KiB, free: 912.0 MiB)\n",
      "12:29:55.544 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on datanode1:42309 (size: 7.7 KiB, free: 912.1 MiB)\n",
      "12:29:55.562 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on datanode3:33879 (size: 7.7 KiB, free: 912.1 MiB)\n",
      "12:29:55.601 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on datanode2:39793 (size: 44.7 KiB, free: 912.0 MiB)\n",
      "12:29:55.658 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on datanode1:42309 (size: 44.7 KiB, free: 912.1 MiB)\n",
      "12:29:55.676 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on datanode3:33879 (size: 44.7 KiB, free: 912.1 MiB)\n",
      "12:29:55.721 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 7.0 (TID 56) in 205 ms on datanode2 (executor 1) (1/6)\n",
      "12:29:55.723 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 53) in 209 ms on datanode2 (executor 1) (2/6)\n",
      "12:29:55.794 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 7.0 (TID 54) in 279 ms on datanode1 (executor 6) (3/6)\n",
      "12:29:55.795 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 51) in 282 ms on datanode1 (executor 6) (4/6)\n",
      "12:29:55.839 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 52) in 325 ms on datanode3 (executor 2) (5/6)\n",
      "12:29:55.840 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 7.0 (TID 55) in 325 ms on datanode3 (executor 2) (6/6)\n",
      "12:29:55.840 [task-result-getter-3] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "12:29:55.841 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.347 s\n",
      "12:29:55.842 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "12:29:55.843 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "12:29:55.843 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "12:29:55.844 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "12:29:55.899 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.875738 ms\n",
      "12:29:55.924 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "12:29:55.926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "12:29:55.926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (count at NativeMethodAccessorImpl.java:0)\n",
      "12:29:55.926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)\n",
      "12:29:55.927 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "12:29:55.928 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[35] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "12:29:55.939 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 12.5 KiB, free 2002.2 MiB)\n",
      "12:29:55.943 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 2002.1 MiB)\n",
      "12:29:55.945 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on namenode:40137 (size: 6.0 KiB, free: 2004.2 MiB)\n",
      "12:29:55.946 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "12:29:55.947 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[35] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "12:29:55.947 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 9.0 with 1 tasks resource profile 0\n",
      "12:29:55.952 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 57) (datanode2, executor 1, partition 0, NODE_LOCAL, 9010 bytes) \n",
      "12:29:55.973 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on datanode2:39793 (size: 6.0 KiB, free: 912.0 MiB)\n",
      "12:29:55.990 [dispatcher-event-loop-3] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - Asked to send map output locations for shuffle 0 to 192.168.12.222:35378\n",
      "12:29:56.089 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 57) in 139 ms on datanode2 (executor 1) (1/1)\n",
      "12:29:56.090 [task-result-getter-1] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "12:29:56.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.153 s\n",
      "12:29:56.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "12:29:56.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 9: Stage finished\n",
      "12:29:56.092 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: count at NativeMethodAccessorImpl.java:0, took 0.167521 s\n",
      "Total records processed: 8506526\n"
     ]
    }
   ],
   "source": [
    "# Verify results\n",
    "result_df = spark.read.parquet(output_path)\n",
    "result_df.show(10, truncate=False)\n",
    "result_count = result_df.count()\n",
    "print(f\"Total records processed: {result_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:29:56.110 [Thread-4] INFO  org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.\n",
      "12:29:56.131 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector - Stopped Spark@1768c80a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "12:29:56.138 [Thread-4] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://namenode:4040\n",
      "12:29:56.157 [YARN application state monitor] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Interrupting monitor thread\n",
      "12:29:56.179 [Thread-4] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Shutting down all executors\n",
      "12:29:56.179 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Asking each executor to shut down\n",
      "12:29:56.187 [Thread-4] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - YARN client scheduler backend Stopped\n",
      "12:29:56.231 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!\n",
      "12:29:56.249 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared\n",
      "12:29:56.249 [Thread-4] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped\n",
      "12:29:56.252 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped\n",
      "12:29:56.255 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!\n",
      "12:29:56.268 [Thread-4] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
