{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with Dynamic Allocation\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis and Clustering\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"3\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"9\") \\\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file\n",
    "file_path = \"hdfs://namenode:9000/data/cleaned_dataset.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean comments\n",
    "def clean_comment_spark(df, column):\n",
    "    return df.withColumn(\n",
    "        f\"{column}_clean\",\n",
    "        F.trim(\n",
    "            F.regexp_replace(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.lower(F.col(column)),\n",
    "                        r\"http\\S+|www\\S+|https\\S+\", \"\"),\n",
    "                    r\"@\\w+|#\", \"\"),\n",
    "                r\"[^\\w\\s]\", \"\")\n",
    "            )\n",
    "        )\n",
    "    \n",
    "df = clean_comment_spark(df, \"body\").select(\"comment_id\", \"body_clean\", \"created_utc\", \"subreddit_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast Sentiment Analyzer\n",
    "analyzer_broadcast = sc.broadcast(SentimentIntensityAnalyzer())\n",
    "\n",
    "# Sentiment calculation using RDDs\n",
    "def calculate_sentiment(row):\n",
    "    analyzer = analyzer_broadcast.value\n",
    "    comment_id = row['comment_id']\n",
    "    text = row['body_clean']\n",
    "    sentiment_score = analyzer.polarity_scores(text)['compound'] if text else None\n",
    "    return (comment_id, sentiment_score, row['created_utc'], row['subreddit_name'])\n",
    "\n",
    "sentiment_rdd = df.rdd.map(calculate_sentiment)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"comment_id\", StringType(), True),\n",
    "    StructField(\"sentiment\", FloatType(), True),\n",
    "    StructField(\"created_utc\", StringType(), True),\n",
    "    StructField(\"subreddit_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "sentiment_df = spark.createDataFrame(sentiment_rdd, schema)\n",
    "sentiment_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to date\n",
    "df = sentiment_df.withColumn(\"date\", F.from_unixtime(F.col(\"created_utc\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall daily sentiment\n",
    "df_daily_sentiment_all = df.groupBy(\"date\").agg(F.avg(\"sentiment\").alias(\"avg_daily_sentiment_all\")).orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily sentiment per subreddit\n",
    "df_daily_sentiment_subreddit = df.groupBy(\"date\", \"subreddit_name\") \\\n",
    "    .agg(F.avg(\"sentiment\").alias(\"avg_daily_sentiment_subreddit\")) \\\n",
    "    .orderBy(\"date\", \"subreddit_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join subreddit and overall trends\n",
    "df_trend_comparison = df_daily_sentiment_subreddit.join(\n",
    "    df_daily_sentiment_all, on=\"date\", how=\"left\"\n",
    ").withColumn(\n",
    "    \"sentiment_diff\",\n",
    "    F.col(\"avg_daily_sentiment_subreddit\") - F.col(\"avg_daily_sentiment_all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and stopword removal\n",
    "tokenizer = Tokenizer(inputCol=\"body_clean\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf])\n",
    "model = pipeline.fit(df)\n",
    "tfidf_df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans clustering\n",
    "kmeans = KMeans(k=5, seed=123)\n",
    "model = kmeans.fit(tfidf_df)\n",
    "predictions = model.transform(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis by cluster\n",
    "sentiment_analysis = predictions.groupBy(\"prediction\").agg(\n",
    "    F.mean(\"sentiment\").alias(\"average_sentiment\"),\n",
    "    F.count(\"subreddit_name\").alias(\"subreddit_count\")\n",
    ").orderBy(\"average_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_path = \"hdfs://namenode:9000/data/results/sentiment_analysis.parquet\"\n",
    "predictions.write.mode(\"overwrite\").parquet(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
