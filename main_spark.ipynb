{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import StringType, BooleanType, IntegerType, FloatType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 1: Initialize Spark\n",
    "# -------------------------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Comments Scrambling\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # Access SparkContext for RDD operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 2: Data Ingestion (Bronze Layer)\n",
    "# -------------------------------------\n",
    "\n",
    "# TODO: Needs to have files on hdfs\n",
    "# raw_data_path = \"hdfs://your_hdfs_path/data/subset/100000-reddit-covid-comments.csv\"  # Path on HDFS\n",
    "raw_data_path = \"data/subset/100000-reddit-covid-comments.csv\"  # local path\n",
    "\n",
    "# Set CSV options to handle multiline fields\n",
    "csv_options = {\n",
    "    \"header\": \"true\",            # CSV contains a header row\n",
    "    \"multiLine\": \"true\",         # Enable reading multiline fields\n",
    "    \"escape\": \"\\\"\",              # Use double quotes as the escape character for special characters\n",
    "    \"quote\": \"\\\"\",               # Ensure that quotes in fields are correctly handled\n",
    "    \"mode\": \"DROPMALFORMED\"      # Drop any malformed rows to avoid parser errors\n",
    "}\n",
    "\n",
    "# Read the CSV file with the new options\n",
    "raw_df = spark.read.options(**csv_options).csv(raw_data_path)\n",
    "\n",
    "# Display raw data sample\n",
    "print(\"Raw Data Sample:\")\n",
    "raw_df.show(5)\n",
    "\n",
    "# Save raw data into the Bronze layer on HDFS\n",
    "# raw_data_bronze_path = \"hdfs://your_hdfs_path/data/bronze_layer/\"\n",
    "# raw_df.write.mode(\"overwrite\").parquet(raw_data_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to RDD for MapReduce-like operations\n",
    "def extract_fields(row):\n",
    "    \"\"\"Extract relevant fields from each row of the DataFrame.\"\"\"\n",
    "    return (\n",
    "        row['type'], row['id'], row['subreddit.id'], \n",
    "        row['subreddit.name'], row['subreddit.nsfw'], \n",
    "        row['created_utc'], row['permalink'], \n",
    "        row['body'], row['sentiment'], row['score']\n",
    "    )\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "raw_rdd = raw_df.rdd.map(extract_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 3: Data Scrambling\n",
    "# -------------------------------------\n",
    "def scramble_row(row):\n",
    "    \"\"\"\n",
    "    Scramble the row by replacing commas, inserting random line breaks, \n",
    "    and converting the row into an unstructured key-value format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting fields\n",
    "    type_and_id_field = f\"{row[0]}: {row[1]}\" \n",
    "    # id_field = f\"id: {row[1]}\"\n",
    "    subreddit_id = f\"subreddit.id: {row[2]}\"\n",
    "    subreddit_name = f\"subreddit.name: {row[3]}\"\n",
    "    subreddit_nsfw = f\"subreddit.nsfw {row[4]}:\"\n",
    "    created_utc = f\"created_utc: {row[5]}\"\n",
    "    permalink = f\"permalink: {row[6]}\"\n",
    "    sentiment = f\"sentiment: {row[7]}\"\n",
    "\n",
    "    body = row[8]\n",
    "\n",
    "    # Randomly insert a line break in the body to make it messy\n",
    "    if random.random() > 0.7:  # 30% chance to break the body\n",
    "        split_point = random.randint(0, len(body) // 2)  # Random split location\n",
    "        body = body[:split_point] + \"\\n\" + body[split_point:]\n",
    "\n",
    "    body_field = f\"body: {body}\"\n",
    "    score = f\"score: {row[9]}\"\n",
    "\n",
    "    fields = [\n",
    "        type_and_id_field, subreddit_id, subreddit_name, subreddit_nsfw,\n",
    "        created_utc, permalink, sentiment, body_field, score\n",
    "    ]\n",
    "\n",
    "    # Join the fields with newlines to create an unstructured format\n",
    "    row_str = \"\\n\".join(fields)\n",
    "\n",
    "    return row_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scramble data using Map function (MapReduce style)\n",
    "scrambled_rdd = raw_rdd.map(scramble_row)\n",
    "\n",
    "# Debug: Check the counts and a sample\n",
    "print(\"Count of raw RDD:\", raw_rdd.count())\n",
    "print(\"Count of scrambled RDD:\", scrambled_rdd.count())\n",
    "print(\"Sample rows from scrambled RDD:\", scrambled_rdd.take(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scrambled data to HDFS as a text file\n",
    "# scrambled_data_path = \"hdfs://your_hdfs_path/data/scrambled_layer/scrambled-reddit-covid-comments.txt\"\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "scrambled_data_path = \"data/scrambled/spark-scrambled-reddit-covid-comments.txt\"\n",
    "\n",
    "# Clean up old output directory if it exists\n",
    "scrambled_data_path = \"data/scrambled/spark-scrambled-reddit-covid-comments\"\n",
    "if os.path.exists(scrambled_data_path):\n",
    "    shutil.rmtree(scrambled_data_path)  # Delete the existing directory\n",
    "\n",
    "try:\n",
    "    # Save scrambled data to local filesystem as a text file\n",
    "    scrambled_rdd.saveAsTextFile(scrambled_data_path)\n",
    "    print(f\"Scrambled data saved to: {scrambled_data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving scrambled data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 4: Data Cleaning (Silver Layer)\n",
    "# -------------------------------------\n",
    "def clean_data(unstructured_row):\n",
    "    \"\"\"\n",
    "    Clean the unstructured row by extracting the key-value pairs and reconstructing the original row.\n",
    "    Handles extra spaces, duplicated fields, and missing data.\n",
    "    \"\"\"\n",
    "    # Split the row by newlines and remove empty lines\n",
    "    fields = [f.strip() for f in unstructured_row.split(\"\\n\") if f.strip()]\n",
    "    \n",
    "    # Dictionary to store the extracted key-value pairs\n",
    "    field_dict = {}\n",
    "\n",
    "    for field in fields:\n",
    "        if \": \" in field:\n",
    "            key, value = field.split(\": \", 1)  # Split on the first occurrence of \": \"\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            \n",
    "            # Handle duplicate keys by ignoring subsequent ones\n",
    "            if key not in field_dict:\n",
    "                field_dict[key] = value\n",
    "\n",
    "    # Reconstruct fields, using defaults where necessary\n",
    "    type_field = field_dict.get(\"comment\", field_dict.get(\"post\", \"post\"))\n",
    "    id_field = field_dict.get(\"id\", \"\")\n",
    "    subreddit_id = field_dict.get(\"subreddit.id\", \"\")\n",
    "    subreddit_name = field_dict.get(\"subreddit.name\", \"\")\n",
    "    subreddit_nsfw = field_dict.get(\"subreddit.nsfw\", \"False\")  # Default to 'False'\n",
    "    created_utc = field_dict.get(\"created_utc\", \"\")\n",
    "    permalink = field_dict.get(\"permalink\", \"\")\n",
    "    sentiment = field_dict.get(\"sentiment\", \"NULL\")\n",
    "    body = field_dict.get(\"body\", \"\").replace(\"\\n\", \" \").strip()  # Join body\n",
    "    score = field_dict.get(\"score\", \"NULL\")\n",
    "\n",
    "    # Return as a structured tuple\n",
    "    return (type_field, id_field, subreddit_id, subreddit_name, subreddit_nsfw, created_utc, permalink, body, sentiment, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the scrambled data from HDFS (this is now an RDD)\n",
    "scrambled_rdd = sc.textFile(scrambled_data_path)\n",
    "\n",
    "# Clean the data using map and flatMap (to handle multiple lines returned by clean_data)\n",
    "cleaned_rdd = scrambled_rdd.flatMap(lambda line: clean_data(line))\n",
    "\n",
    "# Save cleaned data to the Silver layer on HDFS\n",
    "# cleaned_data_silver_path = \"hdfs://your_hdfs_path/data/silver_layer/\"\n",
    "\n",
    "cleaned_data_silver_path = \"/data/cleaned/\"\n",
    "\n",
    "cleaned_rdd.saveAsTextFile(cleaned_data_silver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 5: DataFrame for Gold Layer\n",
    "# -------------------------------------\n",
    "# Convert cleaned RDD back to DataFrame for potential SQL-based operations\n",
    "cleaned_df = cleaned_rdd.toDF([\"type\", \"id\", \"subreddit.id\", \"subreddit.name\", \"subreddit.nsfw\", \"created_utc\", \"permalink\", \"body\", \"score\"])\n",
    "\n",
    "# Save cleaned data to the Silver layer in Parquet format\n",
    "cleaned_data_silver_parquet_path = \"hdfs://your_hdfs_path/data/silver_layer_parquet/\"\n",
    "cleaned_df.write.mode(\"overwrite\").parquet(cleaned_data_silver_parquet_path)\n",
    "\n",
    "# Now the DataFrame is ready for future SQL or analytics in the Gold layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Part 6: Data Serving (Gold Layer)\n",
    "# -------------------------------------\n",
    "# You can load the cleaned Parquet data and perform advanced queries/analytics using Spark SQL for the Gold layer\n",
    "\n",
    "# Example: Analyze subreddit statistics\n",
    "cleaned_df.createOrReplaceTempView(\"reddit_comments\")\n",
    "# Example SQL operation: Sentiment analysis or any business logic\n",
    "\n",
    "# TODO: This is probably too simple, as it just counts the score (thumps up/down)\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT subreddit_name, COUNT(*) as comment_count\n",
    "    FROM reddit_comments\n",
    "    GROUP BY subreddit_name\n",
    "    HAVING COUNT(*) > 50\n",
    "    ORDER BY comment_count DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "result_df.show()\n",
    "\n",
    "# Save the final results to the Gold layer (for dashboard or further analysis)\n",
    "final_data_gold_path = \"hdfs://your_hdfs_path/data/gold_layer/\"\n",
    "result_df.write.mode(\"overwrite\").parquet(final_data_gold_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
