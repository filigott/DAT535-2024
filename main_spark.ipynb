{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import StringType, BooleanType, IntegerType, FloatType\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/23 09:38:24 WARN Utils: Your hostname, FiligottLaptop resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/23 09:38:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/23 09:38:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Reddit Comments Scrambling>\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 1: Initialize Spark\n",
    "# -------------------------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Comments Scrambling\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # Access SparkContext for RDD operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Sample:\n",
      "+-------+-------+------------+-----------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "|   type|     id|subreddit.id|   subreddit.name|subreddit.nsfw|created_utc|           permalink|                body|sentiment|score|\n",
      "+-------+-------+------------+-----------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "|comment|hi0xdct|       2qh7q|          florida|         False| 1635191579|https://old.reddi...|&gt; COVID-19 is ...|  -0.5666|   -1|\n",
      "|comment|hi16118|       2y77d|         antiwork|         False| 1635195040|https://old.reddi...|Wtf ? What is wro...|  -0.8634|    1|\n",
      "|comment|hi1mkh7|       2qhsa|interestingasfuck|         False| 1635202157|https://old.reddi...|I thought the she...|   0.4329|    1|\n",
      "|comment|hi15pqu|       2z2wm|    cryptomarkets|         False| 1635194914|https://old.reddi...|I mean…yea it was...|      0.0|    1|\n",
      "|comment|hi16y0z|       2qqd2|  greenbaypackers|         False| 1635195416|https://old.reddi...|Fact is the “vacc...|      0.0|    0|\n",
      "+-------+-------+------------+-----------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Step 2: Data Ingestion (Bronze Layer)\n",
    "# -------------------------------------\n",
    "\n",
    "# TODO: Needs to have files on hdfs\n",
    "# raw_data_path = \"hdfs://your_hdfs_path/data/subset/100000-reddit-covid-comments.csv\"  # Path on HDFS\n",
    "raw_data_path = \"data/subset/100000-reddit-covid-comments.csv\"  # local path\n",
    "\n",
    "# Set CSV options to handle multiline fields\n",
    "csv_options = {\n",
    "    \"header\": \"true\",            # CSV contains a header row\n",
    "    \"multiLine\": \"true\",         # Enable reading multiline fields\n",
    "    \"escape\": \"\\\"\",              # Use double quotes as the escape character for special characters\n",
    "    \"quote\": \"\\\"\",               # Ensure that quotes in fields are correctly handled\n",
    "    \"mode\": \"DROPMALFORMED\"      # Drop any malformed rows to avoid parser errors\n",
    "}\n",
    "\n",
    "# Read the CSV file with the new options\n",
    "raw_df = spark.read.options(**csv_options).csv(raw_data_path)\n",
    "\n",
    "# Display raw data sample\n",
    "print(\"Raw Data Sample:\")\n",
    "raw_df.show(5)\n",
    "\n",
    "# Save raw data into the Bronze layer on HDFS\n",
    "# raw_data_bronze_path = \"hdfs://your_hdfs_path/data/bronze_layer/\"\n",
    "# raw_df.write.mode(\"overwrite\").parquet(raw_data_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to RDD for MapReduce-like operations\n",
    "def extract_fields(row):\n",
    "    \"\"\"Extract relevant fields from each row of the DataFrame.\"\"\"\n",
    "    return (\n",
    "        row['type'], row['id'], row['subreddit.id'], \n",
    "        row['subreddit.name'], row['subreddit.nsfw'], \n",
    "        row['created_utc'], row['permalink'], \n",
    "        row['body'], row['sentiment'], row['score']\n",
    "    )\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "raw_rdd = raw_df.rdd.map(extract_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 3: Data Scrambling\n",
    "# -------------------------------------\n",
    "def scramble_row(row):\n",
    "    \"\"\"\n",
    "    Scramble the row by replacing commas, inserting random line breaks, \n",
    "    and converting the row into an unstructured key-value format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting fields\n",
    "    type_and_id_field = f\"{row[0]}: {row[1]}\" \n",
    "    # id_field = f\"id: {row[1]}\"\n",
    "    subreddit_id = f\"subreddit.id: {row[2]}\"\n",
    "    subreddit_name = f\"subreddit.name: {row[3]}\"\n",
    "    subreddit_nsfw = f\"subreddit.nsfw {row[4]}:\"\n",
    "    created_utc = f\"created_utc: {row[5]}\"\n",
    "    permalink = f\"permalink: {row[6]}\"\n",
    "    sentiment = f\"sentiment: {row[7]}\"\n",
    "\n",
    "    body = row[8]\n",
    "\n",
    "    # Randomly insert a line break in the body to make it messy\n",
    "    if random.random() > 0.7:  # 30% chance to break the body\n",
    "        split_point = random.randint(0, len(body) // 2)  # Random split location\n",
    "        body = body[:split_point] + \"\\n\" + body[split_point:]\n",
    "\n",
    "    body_field = f\"body: {body}\"\n",
    "    score = f\"score: {row[9]}\"\n",
    "\n",
    "    fields = [\n",
    "        type_and_id_field, subreddit_id, subreddit_name, subreddit_nsfw,\n",
    "        created_utc, permalink, sentiment, body_field, score\n",
    "    ]\n",
    "\n",
    "    # Join the fields with newlines to create an unstructured format\n",
    "    row_str = \"\\n\".join(fields)\n",
    "\n",
    "    return row_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from scrambled RDD: ['comment: hi0xdct\\nsubreddit.id: 2qh7q\\nsubreddit.name: florida\\nsubreddit.nsfw False:\\ncreated_utc: 1635191579\\npermalink: https://old.reddit.com/r/florida/comments/qf3xp4/desantis_recruiting_unvaccinated_out_of_state/hi0xdct/\\nsentiment: &gt; COVID-19 is legitimately dangerous*\\n\\n*for the old and/or unhealthy.\\n\\nAverage age of death is like 80. 76% of all deaths are 65+.\\n\\n&gt; we STILL don\\'t know what the long-term effects are\\n\\n[The CDC estimates 120.2 million cases from 2/20 to 5/21 with 6.2 million hospitalizations.](https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/burden.html) \\n\\nHow many people do you think are walking around with \"long-term effects\"? How long is \"long-term\"?\\n\\nWhat if, much like every other disease, there are none, except from severe cases or being absurdly unlucky?\\n\\n&gt; ~750k wasn\\'t enough\\n\\nConsidering all the ignored red flags raised about [the PCR test](https://www.nytimes.com/2020/08/29/health/coronavirus-testing.html), we have very good reason to think that number is way too high.\\n\\n&gt; \"In Massachusetts, **from 85 to 90 percent** of people who tested positive in July with a cycle threshold of 40 would have been deemed negative if the threshold were 30 cycles, Dr. Mina said. “I would say that none of those people should be contact-traced, not one,” he said.\"\\nbody: -0.5666\\nscore: -1', 'comment: hi16118\\nsubreddit.id: 2y77d\\nsubreddit.name: antiwork\\nsubreddit.nsfw False:\\ncreated_utc: 1635195040\\npermalink: https://old.reddit.com/r/antiwork/comments/qfgkqn/my_friend_has_covid_and_complained_to_the_hr/hi16118/\\nsentiment: Wtf ? What is wrong with that manager? For almost 2 years covid has been in the news everyday and he said that to a covid positive worker??!! Something seriously wrong with people nowadays! Haha !!! Crazy.\\nbody: -0\\n.8634\\nscore: 1', 'comment: hi1mkh7\\nsubreddit.id: 2qhsa\\nsubreddit.name: interestingasfuck\\nsubreddit.nsfw False:\\ncreated_utc: 1635202157\\npermalink: https://old.reddit.com/r/interestingasfuck/comments/qfl05t/all_of_the_garbage_on_everest_to_go_with_the_new/hi1mkh7/\\nsentiment: I thought the sherpas were working on that through Covid? Like didn’t Nepal shut down permits and pack a bunch of that out?\\nbody: 0.4329\\nscore: 1', 'comment: hi15pqu\\nsubreddit.id: 2z2wm\\nsubreddit.name: cryptomarkets\\nsubreddit.nsfw False:\\ncreated_utc: 1635194914\\npermalink: https://old.reddit.com/r/CryptoMarkets/comments/qf4fjl/i_hope_i_didnt_fuck_up/hi15pqu/\\nsentiment: I mean…yea it was a gamble. But it’s paying my rent this month. Out of work with covid.\\nbody: \\n0.0\\nscore: 1', 'comment: hi16y0z\\nsubreddit.id: 2qqd2\\nsubreddit.name: greenbaypackers\\nsubreddit.nsfw False:\\ncreated_utc: 1635195416\\npermalink: https://old.reddit.com/r/GreenBayPackers/comments/qfp1my/davante_adams_placed_on_covid_list/hi16y0z/\\nsentiment: Fact is the “vaccine” doesn’t do anything. First it was 100% you won’t get covid, then you just won’t go to the hospital, and now it isn’t even that.\\nbody: 0.0\\nscore: 0']\n"
     ]
    }
   ],
   "source": [
    "# Scramble data using Map function (MapReduce style)\n",
    "scrambled_rdd = raw_rdd.map(scramble_row)\n",
    "\n",
    "# Debug: Check the counts and a sample\n",
    "print(\"Count of raw RDD:\", raw_rdd.count())\n",
    "print(\"Count of scrambled RDD:\", scrambled_rdd.count())\n",
    "print(\"Sample rows from scrambled RDD:\", scrambled_rdd.take(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/21 20:27:29 ERROR Utils: Aborting task\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 3408, in func\n",
      "    for x in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_24523/3264421549.py\", line 24, in scramble_row\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/10/21 20:27:29 ERROR SparkHadoopWriter: Task attempt_20241021202729781195260035381429_0091_m_000000_0 aborted.\n",
      "24/10/21 20:27:29 ERROR Executor: Exception in task 0.0 in stage 17.0 (TID 17)\n",
      "org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 3408, in func\n",
      "    for x in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_24523/3264421549.py\", line 24, in scramble_row\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\t... 12 more\n",
      "24/10/21 20:27:29 WARN TaskSetManager: Lost task 0.0 in stage 17.0 (TID 17) (10.255.255.254 executor driver): org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 3408, in func\n",
      "    for x in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_24523/3264421549.py\", line 24, in scramble_row\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\t... 12 more\n",
      "\n",
      "24/10/21 20:27:29 ERROR TaskSetManager: Task 0 in stage 17.0 failed 1 times; aborting job\n",
      "24/10/21 20:27:29 ERROR SparkHadoopWriter: Aborting job job_20241021202729781195260035381429_0091.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 17) (10.255.255.254 executor driver): org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 3408, in func\n",
      "    for x in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_24523/3264421549.py\", line 24, in scramble_row\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\t... 12 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 3408, in func\n",
      "    for x in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_24523/3264421549.py\", line 24, in scramble_row\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\t... 12 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving scrambled data: An error occurred while calling o320.saveAsTextFile.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 17) (10.255.255.254 executor driver): org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 3408, in func\n",
      "    for x in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_24523/3264421549.py\", line 24, in scramble_row\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\t... 12 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2446)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\t... 51 more\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 3408, in func\n",
      "    for x in iterator:\n",
      "  File \"/home/filigott/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_24523/3264421549.py\", line 24, in scramble_row\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n",
      "\t... 12 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save scrambled data to HDFS as a text file\n",
    "# scrambled_data_path = \"hdfs://your_hdfs_path/data/scrambled_layer/scrambled-reddit-covid-comments.txt\"\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "scrambled_data_path = \"data/scrambled/spark-scrambled-reddit-covid-comments.txt\"\n",
    "\n",
    "# Clean up old output directory if it exists\n",
    "scrambled_data_path = \"data/scrambled/spark-scrambled-reddit-covid-comments\"\n",
    "if os.path.exists(scrambled_data_path):\n",
    "    shutil.rmtree(scrambled_data_path)  # Delete the existing directory\n",
    "\n",
    "try:\n",
    "    # Save scrambled data to local filesystem as a text file\n",
    "    scrambled_rdd.saveAsTextFile(scrambled_data_path)\n",
    "    print(f\"Scrambled data saved to: {scrambled_data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving scrambled data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 4: Data Cleaning (Silver Layer)\n",
    "# -------------------------------------\n",
    "def clean_data(unstructured_row):\n",
    "    \"\"\"\n",
    "    Clean the unstructured row by extracting the key-value pairs and reconstructing the original row.\n",
    "    Handles extra spaces, duplicated fields, and missing data.\n",
    "    \"\"\"\n",
    "    # Split the row by newlines and remove empty lines\n",
    "    fields = [f.strip() for f in unstructured_row.split(\"\\n\") if f.strip()]\n",
    "    \n",
    "    # Dictionary to store the extracted key-value pairs\n",
    "    field_dict = {}\n",
    "\n",
    "    for field in fields:\n",
    "        if \": \" in field:\n",
    "            key, value = field.split(\": \", 1)  # Split on the first occurrence of \": \"\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            \n",
    "            # Handle duplicate keys by ignoring subsequent ones\n",
    "            if key not in field_dict:\n",
    "                field_dict[key] = value\n",
    "\n",
    "    # Reconstruct fields, using defaults where necessary\n",
    "    type_field = field_dict.get(\"comment\", field_dict.get(\"post\", \"post\"))\n",
    "    id_field = field_dict.get(\"id\", \"\")\n",
    "    subreddit_id = field_dict.get(\"subreddit.id\", \"\")\n",
    "    subreddit_name = field_dict.get(\"subreddit.name\", \"\")\n",
    "    subreddit_nsfw = field_dict.get(\"subreddit.nsfw\", \"False\")  # Default to 'False'\n",
    "    created_utc = field_dict.get(\"created_utc\", \"\")\n",
    "    permalink = field_dict.get(\"permalink\", \"\")\n",
    "    sentiment = field_dict.get(\"sentiment\", \"NULL\")\n",
    "    body = field_dict.get(\"body\", \"\").replace(\"\\n\", \" \").strip()  # Join body\n",
    "    score = field_dict.get(\"score\", \"NULL\")\n",
    "\n",
    "    # Return as a structured tuple\n",
    "    return (type_field, id_field, subreddit_id, subreddit_name, subreddit_nsfw, created_utc, permalink, body, sentiment, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the scrambled data from HDFS (this is now an RDD)\n",
    "scrambled_rdd = sc.textFile(scrambled_data_path)\n",
    "\n",
    "# Clean the data using map and flatMap (to handle multiple lines returned by clean_data)\n",
    "cleaned_rdd = scrambled_rdd.flatMap(lambda line: clean_data(line))\n",
    "\n",
    "# Save cleaned data to the Silver layer on HDFS\n",
    "# cleaned_data_silver_path = \"hdfs://your_hdfs_path/data/silver_layer/\"\n",
    "\n",
    "cleaned_data_silver_path = \"/data/cleaned/\"\n",
    "\n",
    "cleaned_rdd.saveAsTextFile(cleaned_data_silver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 5: DataFrame for Gold Layer\n",
    "# -------------------------------------\n",
    "# Convert cleaned RDD back to DataFrame for potential SQL-based operations\n",
    "cleaned_df = cleaned_rdd.toDF([\"type\", \"id\", \"subreddit.id\", \"subreddit.name\", \"subreddit.nsfw\", \"created_utc\", \"permalink\", \"body\", \"score\"])\n",
    "\n",
    "# Save cleaned data to the Silver layer in Parquet format\n",
    "cleaned_data_silver_parquet_path = \"hdfs://your_hdfs_path/data/silver_layer_parquet/\"\n",
    "cleaned_df.write.mode(\"overwrite\").parquet(cleaned_data_silver_parquet_path)\n",
    "\n",
    "# Now the DataFrame is ready for future SQL or analytics in the Gold layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Part 6: Data Serving (Gold Layer)\n",
    "# -------------------------------------\n",
    "# You can load the cleaned Parquet data and perform advanced queries/analytics using Spark SQL for the Gold layer\n",
    "\n",
    "# Example: Analyze subreddit statistics\n",
    "cleaned_df.createOrReplaceTempView(\"reddit_comments\")\n",
    "# Example SQL operation: Sentiment analysis or any business logic\n",
    "\n",
    "# TODO: This is probably too simple, as it just counts the score (thumps up/down)\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT subreddit_name, COUNT(*) as comment_count\n",
    "    FROM reddit_comments\n",
    "    GROUP BY subreddit_name\n",
    "    HAVING COUNT(*) > 50\n",
    "    ORDER BY comment_count DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "result_df.show()\n",
    "\n",
    "# Save the final results to the Gold layer (for dashboard or further analysis)\n",
    "final_data_gold_path = \"hdfs://your_hdfs_path/data/gold_layer/\"\n",
    "result_df.write.mode(\"overwrite\").parquet(final_data_gold_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
