{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import StringType, BooleanType, IntegerType, FloatType\n",
    "import random\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 1: Initialize Spark\n",
    "# -------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Comments Scrambling\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 2: Data Ingestion (Bronze Layer)\n",
    "# -------------------------------------\n",
    "\n",
    "# TODO: Needs to have files on hdfs\n",
    "raw_data_path = \"hdfs://your_hdfs_path/data/subset/100000-reddit-covid-comments.csv\"  # Modify this path as needed\n",
    "raw_df = spark.read.option(\"header\", \"true\").csv(raw_data_path)\n",
    "\n",
    "# Display raw data sample\n",
    "print(\"Raw Data Sample:\")\n",
    "raw_df.show(5)\n",
    "\n",
    "# Save raw data into the Bronze layer\n",
    "raw_data_bronze_path = \"data/bronze_layer/\"\n",
    "raw_df.write.mode(\"overwrite\").parquet(raw_data_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 3: Data Scrambling\n",
    "# -------------------------------------\n",
    "def scramble_row(type_field, id_field, subreddit_id, subreddit_name, subreddit_nsfw, created_utc, permalink, body, sentiment, score):\n",
    "    # Replace '|' in body to avoid conflicts with new delimiter\n",
    "    body = body.replace('|', '__PIPE__')  \n",
    "    type_field = (type_field == \"comment\")  # Convert to boolean\n",
    "    subreddit_nsfw = \"nsfw\" if subreddit_nsfw == \"true\" else \"not_nsfw\"  # Convert to string\n",
    "\n",
    "    # Create a scrambled line with \"|\" as delimiter\n",
    "    row_str = f\"{type_field}|{id_field}|{subreddit_id}|{subreddit_name}|{subreddit_nsfw}|{created_utc}|{sentiment}|{permalink}|{body}|{score}\"\n",
    "    \n",
    "    # Randomly break the row into two parts\n",
    "    if random.random() > 0.7:  # 30% chance to split a row into two parts\n",
    "        split_point = len(row_str) // 2\n",
    "        row_str = row_str[:split_point] + '\\n' + row_str[split_point:]\n",
    "\n",
    "    return row_str\n",
    "\n",
    "# Register the scramble function as a UDF\n",
    "scramble_udf = udf(scramble_row, StringType())\n",
    "\n",
    "# Apply the scramble function\n",
    "scrambled_df = raw_df.select(scramble_udf(\n",
    "    col(\"type\"),\n",
    "    col(\"id\"),\n",
    "    col(\"subreddit.id\"),\n",
    "    col(\"subreddit.name\"),\n",
    "    col(\"subreddit.nsfw\"),\n",
    "    col(\"created_utc\"),\n",
    "    col(\"permalink\"),\n",
    "    col(\"body\"),\n",
    "    col(\"sentiment\"),\n",
    "    col(\"score\")\n",
    ").alias(\"scrambled_data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 4: Data Cleaning (Silver Layer)\n",
    "# -------------------------------------\n",
    "def clean_data(scrambled_data):\n",
    "    lines = scrambled_data.split(\"\\n\")\n",
    "    \n",
    "    # Step 1: Rejoin broken lines\n",
    "    cleaned_lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip():  # Ignore empty lines\n",
    "            current_line += line.strip() + \" \"  # Append with a space to avoid issues\n",
    "            # Check if we have a complete line\n",
    "            if current_line.count('|') == 9:  # Exactly 10 fields\n",
    "                cleaned_lines.append(current_line.strip())  # Add completed line\n",
    "                current_line = \"\"  # Reset for the next line\n",
    "\n",
    "    # Handle any remaining line that wasn't appended\n",
    "    if current_line:\n",
    "        cleaned_lines.append(current_line.strip())\n",
    "\n",
    "    # Step 2: Parse each cleaned line back into columns\n",
    "    parsed_data = []\n",
    "\n",
    "    for line in cleaned_lines:\n",
    "        fields = line.split('|')\n",
    "\n",
    "        if len(fields) < 10:\n",
    "            continue  # Skip bad rows\n",
    "        \n",
    "        # Extract fields\n",
    "        type_field = \"comment\" if fields[0] == \"True\" else \"post\"  # Convert back to string\n",
    "        id_field = fields[1]\n",
    "        subreddit_id = fields[2]\n",
    "        subreddit_name = fields[3]\n",
    "        subreddit_nsfw = \"nsfw\" if fields[4] == \"True\" else \"not_nsfw\"  # Convert to string\n",
    "        created_utc = fields[5]\n",
    "        permalink = fields[6]\n",
    "        body = fields[7].replace('__PIPE__', '|')  # Restore original pipe symbol\n",
    "        score = fields[8]\n",
    "\n",
    "        # Create the row dictionary\n",
    "        row_dict = {\n",
    "            'type': type_field,\n",
    "            'id': id_field,\n",
    "            'subreddit.id': subreddit_id,\n",
    "            'subreddit.name': subreddit_name,\n",
    "            'subreddit.nsfw': subreddit_nsfw,\n",
    "            'created_utc': created_utc,\n",
    "            'permalink': permalink,\n",
    "            'body': body,\n",
    "            'score': score\n",
    "        }\n",
    "        parsed_data.append(row_dict)\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "# TODO: Is this really using spark and hdfs???\n",
    "\n",
    "# Register the clean function as a UDF\n",
    "clean_udf = udf(lambda scrambled: clean_data(scrambled), StringType())\n",
    "\n",
    "# Apply the clean function\n",
    "cleaned_df = scrambled_df.select(clean_udf(col(\"scrambled_data\")).alias(\"cleaned_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 5: Flatten the cleaned DataFrame\n",
    "# -------------------------------------\n",
    "cleaned_df_flat = cleaned_df.selectExpr(\"explode(cleaned_data) as data\").select(\"data.*\")\n",
    "\n",
    "# Display cleaned data sample\n",
    "print(\"Cleaned Data Sample:\")\n",
    "cleaned_df_flat.show(5)\n",
    "\n",
    "# Save cleaned data to the Silver layer\n",
    "cleaned_data_silver_path = \"data/silver_layer/\"\n",
    "cleaned_df_flat.write.mode(\"overwrite\").parquet(cleaned_data_silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Step 6: Data Serving (Gold Layer)\n",
    "# -------------------------------------\n",
    "# Load the cleaned data from the Silver layer\n",
    "cleaned_df = spark.read.parquet(cleaned_data_silver_path)\n",
    "\n",
    "# For example, let's find the average sentiment by subreddit\n",
    "cleaned_df.createOrReplaceTempView(\"reddit_comments\")\n",
    "\n",
    "# TODO: This is probably too simple, as it just counts the score (thumps up/down)\n",
    "\n",
    "# Perform sentiment analysis using SQL\n",
    "sentiment_analysis_df = spark.sql(\"\"\"\n",
    "    SELECT subreddit_name, AVG(score) as avg_score, COUNT(*) as comment_count\n",
    "    FROM reddit_comments\n",
    "    GROUP BY subreddit_name\n",
    "    HAVING COUNT(*) > 50  -- Subreddits with more than 50 comments\n",
    "    ORDER BY avg_score DESC\n",
    "\"\"\")\n",
    "\n",
    "# Display sentiment analysis results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "sentiment_analysis_df.show(10)\n",
    "\n",
    "# Save the final data to the Gold layer\n",
    "final_data_gold_path = \"data/gold_layer/\"\n",
    "sentiment_analysis_df.write.mode(\"overwrite\").parquet(final_data_gold_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
