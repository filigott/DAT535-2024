{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/31 18:59:06 WARN Utils: Your hostname, FiligottLaptop resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/31 18:59:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/31 18:59:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Sentiment Analysis\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# File path for the Parquet file\n",
    "file_path = \"data/the-reddit-covid-comments-sample.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- subreddit_name: string (nullable = true)\n",
      " |-- subreddit_nsfw: string (nullable = true)\n",
      " |-- created_utc: integer (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- sentiment: double (nullable = false)\n",
      " |-- score: integer (nullable = true)\n",
      "\n",
      "+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "|   type|     id|subreddit_id|subreddit_name|subreddit_nsfw|created_utc|           permalink|                body|sentiment|score|\n",
      "+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "|comment|hi1vsag|       2riyy|          nova|         false| 1635206399|https://old.reddi...|When you schedule...|      0.0|    2|\n",
      "|comment|hi1vs7i|       2qhov|     vancouver|         false| 1635206397|https://old.reddi...|Didn't stop price...|   0.1887|   32|\n",
      "|comment|hi1vs5n|       2qwzb|      pregnant|         false| 1635206397|https://old.reddi...|Iâ€™m just waiting ...|    0.672|    1|\n",
      "|comment|hi1vs5v|       2qixm|      startrek|         false| 1635206397|https://old.reddi...|*The first duty o...|   0.9562|    1|\n",
      "|comment|hi1vs0l|       2qsf3|       ontario|         false| 1635206395|https://old.reddi...|Compare BC to Ont...|      0.0|   -2|\n",
      "+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming columns for clarity\n",
    "df = df.withColumnRenamed(\"subreddit.id\", \"subreddit_id\")\n",
    "df = df.withColumnRenamed(\"subreddit.name\", \"subreddit_name\")\n",
    "df = df.withColumnRenamed(\"subreddit.nsfw\", \"subreddit_nsfw\")\n",
    "\n",
    "# Calculate the mean sentiment\n",
    "mean_sentiment = df.agg(F.avg(\"sentiment\")).first()[0]\n",
    "\n",
    "# Fill null values in the sentiment column with the mean\n",
    "df_filled_mean = df.fillna({'sentiment': mean_sentiment})\n",
    "\n",
    "# Show the DataFrame schema and first few rows\n",
    "df_filled_mean.printSchema()  # Print the schema of the filled DataFrame\n",
    "df_filled_mean.show(5)        # Display the first few rows of the filled DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1777747"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and daily sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'created_utc' (epoch timestamp) to 'date' column in readable format\n",
    "df = df.withColumn(\"date\", F.from_unixtime(F.col(\"created_utc\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the overall daily average sentiment\n",
    "df_daily_sentiment_all = df.groupBy(\"date\") \\\n",
    "    .agg(F.avg(\"sentiment\").alias(\"avg_daily_sentiment_all\")) \\\n",
    "    .orderBy(\"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average sentiment per day per subreddit\n",
    "df_daily_sentiment_subreddit = df.groupBy(\"date\", \"subreddit_name\") \\\n",
    "    .agg(F.avg(\"sentiment\").alias(\"avg_daily_sentiment_subreddit\")) \\\n",
    "    .orderBy(\"date\", \"subreddit_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join overall and subreddit trends on date to compare subreddit vs. overall sentiment\n",
    "df_trend_comparison = df_daily_sentiment_subreddit.join(\n",
    "    df_daily_sentiment_all, on=\"date\", how=\"left\"\n",
    ").withColumn(\n",
    "    \"sentiment_diff\", \n",
    "    F.col(\"avg_daily_sentiment_subreddit\") - F.col(\"avg_daily_sentiment_all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for saved Parquet files\n",
    "file_path_overall = \"data/results/daily_sentiment_overall.parquet\"\n",
    "file_path_subreddit = \"data/results/daily_sentiment_subreddit.parquet\"\n",
    "file_path_comparison = \"data/results/sentiment_trend_comparison.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the trend data to Parquet files for future use\n",
    "df_daily_sentiment_all.write.mode(\"overwrite\").parquet(file_path_overall)\n",
    "df_daily_sentiment_subreddit.write.mode(\"overwrite\").parquet(file_path_subreddit)\n",
    "df_trend_comparison.write.mode(\"overwrite\").parquet(file_path_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each Parquet file back into a DataFrame\n",
    "df_overall = spark.read.parquet(file_path_overall)\n",
    "df_subreddit = spark.read.parquet(file_path_subreddit)\n",
    "df_comparison = spark.read.parquet(file_path_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Daily Sentiment Trend (sample):\n",
      "+----------+-----------------------+\n",
      "|      date|avg_daily_sentiment_all|\n",
      "+----------+-----------------------+\n",
      "|2021-09-06|   0.021666966678317287|\n",
      "|2021-09-07|   0.018972046900336423|\n",
      "|2021-09-08|   0.006638849958779908|\n",
      "|2021-09-09|    0.02029829139981659|\n",
      "|2021-09-10|   -0.00923499148336...|\n",
      "|2021-09-11|   -0.01285521969230...|\n",
      "|2021-09-12|   -0.00768669654569...|\n",
      "|2021-09-13|   0.008088828050187884|\n",
      "|2021-09-14|   -0.00375876786659...|\n",
      "|2021-09-15|   -0.01135965006415501|\n",
      "+----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total Count (Overall Daily Sentiment Trend): 51\n"
     ]
    }
   ],
   "source": [
    "# Show some sample results and counts for each DataFrame\n",
    "print(\"Overall Daily Sentiment Trend (sample):\")\n",
    "df_overall.show(10)\n",
    "print(\"Total Count (Overall Daily Sentiment Trend):\", df_overall.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-Subreddit Daily Sentiment Trend (sample):\n",
      "+----------+-------------------+-----------------------------+\n",
      "|      date|     subreddit_name|avg_daily_sentiment_subreddit|\n",
      "+----------+-------------------+-----------------------------+\n",
      "|2021-10-14|           business|          -0.7052499999999999|\n",
      "|2021-10-14|       buyitforlife|                       0.1879|\n",
      "|2021-10-14|          byebyejob|         -0.09062095588235293|\n",
      "|2021-10-14|              c_s_t|                      -0.7998|\n",
      "|2021-10-14|             caguns|                       0.1996|\n",
      "|2021-10-14|           calculus|                       0.5574|\n",
      "|2021-10-14|            calgary|          0.08762702702702703|\n",
      "|2021-10-14|california_politics|                     -0.33293|\n",
      "|2021-10-14|       callherdaddy|                       0.7269|\n",
      "|2021-10-14|   callofdutymobile|                      0.48075|\n",
      "+----------+-------------------+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total Count (Per-Subreddit Daily Sentiment Trend): 190597\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPer-Subreddit Daily Sentiment Trend (sample):\")\n",
    "df_subreddit.show(10)\n",
    "print(\"Total Count (Per-Subreddit Daily Sentiment Trend):\", df_subreddit.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subreddit Sentiment Compared to Overall Trend (sample):\n",
      "+----------+--------------+-----------------------------+-----------------------+--------------------+\n",
      "|      date|subreddit_name|avg_daily_sentiment_subreddit|avg_daily_sentiment_all|      sentiment_diff|\n",
      "+----------+--------------+-----------------------------+-----------------------+--------------------+\n",
      "|2021-10-25|       glasgow|         -0.12721904761904765|    0.03645235609738238|-0.16367140371643002|\n",
      "|2021-10-25|    seattler4r|           0.2911000000000001|    0.03645235609738238|  0.2546476439026177|\n",
      "|2021-10-25|       vechain|                        0.667|    0.03645235609738238|  0.6305476439026176|\n",
      "|2021-10-25|  kingcobrajfs|          0.07853809523809523|    0.03645235609738238| 0.04208573914071285|\n",
      "|2021-10-25|showerthoughts|          0.08346666666666665|    0.03645235609738238| 0.04701431056928427|\n",
      "|2021-10-25|          rome|                      -0.5719|    0.03645235609738238| -0.6083523560973824|\n",
      "|2021-10-25|     thetagang|                      0.25812|    0.03645235609738238| 0.22166764390261764|\n",
      "|2021-10-25|  palayeroyale|                       0.8786|    0.03645235609738238|  0.8421476439026176|\n",
      "|2021-10-25|    northkorea|                      -0.0998|    0.03645235609738238|-0.13625235609738237|\n",
      "|2021-10-25|         ffacj|                       0.8964|    0.03645235609738238|  0.8599476439026176|\n",
      "+----------+--------------+-----------------------------+-----------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total Count (Subreddit Sentiment Compared to Overall Trend): 190597\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSubreddit Sentiment Compared to Overall Trend (sample):\")\n",
    "df_comparison.show(10)\n",
    "print(\"Total Count (Subreddit Sentiment Compared to Overall Trend):\", df_comparison.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
