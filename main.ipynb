{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: simulate scrambling the original data\n",
    "# TODO: Could maybe scramble more?\n",
    "\n",
    "def scramble_data(df: pd.DataFrame):\n",
    "    scrambled_lines = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Extract each column's value\n",
    "        type_field = row['type']\n",
    "        id_field = row['id']\n",
    "        subreddit_id = row['subreddit.id']\n",
    "        subreddit_name = row['subreddit.name']\n",
    "        subreddit_nsfw = row['subreddit.nsfw']\n",
    "        created_utc = row['created_utc']\n",
    "        permalink = row['permalink']\n",
    "        # Replace '|' in body to avoid conflicts with new delimiter\n",
    "        body = row['body'].replace('|', '__PIPE__')  \n",
    "        sentiment = row['sentiment']\n",
    "        score = row['score']\n",
    "\n",
    "        # Create a scrambled line with the pipe as delimiter\n",
    "        row_str = f\"{type_field}|{id_field}|{subreddit_id}|{subreddit_name}|{subreddit_nsfw}|{created_utc}|{permalink}|{sentiment}|{score}|{body}\\n\"\n",
    "\n",
    "        # Randomly break the row into two parts\n",
    "        if random.random() > 0.7:  # 30% chance to split a row into two parts\n",
    "            split_point = len(row_str) // 2\n",
    "            row_str = row_str[:split_point] + '\\n' + row_str[split_point:]\n",
    "\n",
    "        scrambled_lines.append(row_str)\n",
    "\n",
    "    scrambled_data = \"\".join(scrambled_lines)\n",
    "    return scrambled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: cleaning the scrambled data\n",
    "# TODO: This should be done in SPARK? (Lecturer did something to the body field here for body text in emails)\n",
    "\n",
    "def clean_data(scrambled_data: str):\n",
    "    # Split the unstructured data into individual lines\n",
    "    lines = scrambled_data.split('\\n')\n",
    "\n",
    "    # Step 1: Rejoin broken lines\n",
    "    cleaned_lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip():  # Ignore empty lines\n",
    "            current_line += line.strip() + \" \"  # Append with a space to avoid issues\n",
    "            # Check if we have a complete line\n",
    "            if current_line.count('|') == 9:  # Exactly 10 fields\n",
    "                cleaned_lines.append(current_line.strip())  # Add completed line\n",
    "                current_line = \"\"  # Reset for the next line\n",
    "\n",
    "    # Handle any remaining line that wasn't appended\n",
    "    if current_line:\n",
    "        cleaned_lines.append(current_line.strip())\n",
    "\n",
    "    # Step 2: Parse each cleaned line back into columns\n",
    "    parsed_data = []\n",
    "    error_data = []  # List to hold error information\n",
    "\n",
    "    for i, line in enumerate(cleaned_lines):\n",
    "        fields = line.split('|')\n",
    "\n",
    "        # Ensure we have at least 10 fields before processing\n",
    "        if len(fields) < 10:\n",
    "            error_info = {\n",
    "                'line_index': i,\n",
    "                'fields_found': fields,\n",
    "                'original_line': line\n",
    "            }\n",
    "            error_data.append(error_info)\n",
    "            continue\n",
    "        \n",
    "        row_dict = {\n",
    "            'type': fields[0],\n",
    "            'id': fields[1],\n",
    "            'subreddit.id': fields[2],\n",
    "            'subreddit.name': fields[3],\n",
    "            'subreddit.nsfw': fields[4],\n",
    "            'created_utc': fields[5],\n",
    "            'permalink': fields[6],\n",
    "            'sentiment': fields[7],\n",
    "            'score': fields[8],\n",
    "            'body': fields[9].replace('__PIPE__', '|')  # Restore original pipe symbol\n",
    "        }\n",
    "        parsed_data.append(row_dict)\n",
    "\n",
    "    # Optionally, return both parsed data and error data\n",
    "    return pd.DataFrame(parsed_data), error_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "NUM_ROWS = 20\n",
    "\n",
    "# Load a small subset of the CSV (for testing)\n",
    "input_file = 'data/subset/100000-reddit-covid-comments.csv'\n",
    "df = pd.read_csv(input_file).head(NUM_ROWS)  # Load first N rows for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrambled text (first 20 lines):\n",
      "\n",
      "comment|hi0xdct|2qh7q|florida|False|1635191579|https://old.reddit.com/r/florida/comments/qf3xp4/desantis_recruiting_unvaccinated_out_of_state/hi0xdct/|-0.5666|-1|&gt; COVID-19 is legitimately dangerous*\n",
      "\n",
      "*for the old and/or unhealthy.\n",
      "\n",
      "Average age of death is like 80. 76% of all deaths are 65+.\n",
      "\n",
      "&gt; we STILL don't know what the long-term effects are\n",
      "\n",
      "[The CDC estimates 120.2 million cases from 2/20 to 5/21 with 6.2 million hospitalizations.](https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/burden.html)\n",
      "\n",
      "How many people do you think are walking around with \"long-term effects\"? How long is \"long-term\"?\n",
      "\n",
      "What if, much like every other disease, there are none, except from severe cases or being absurdly unlucky?\n",
      "\n",
      "&gt; ~750k wasn't enough\n",
      "\n",
      "Considering all the ignored red flags raised about [the PCR test](https://www.nytimes.com/2020/08/29/health/coronavirus-testing.html), we have very good reason to think that number is way too high.\n",
      "\n",
      "&gt; \"In Massachusetts, **from 85 to 90 percent** of people who tested positive in July with a cycle threshold of 40 would have been deemed negative if the threshold were 30 cycles, Dr. Mina said. “I would say that none of those people should be contact-traced, not one,” he said.\"\n",
      "comment|hi16118|2y77d|antiwork|False|1635195040|https://old.reddit.com/r/antiwork/comments/qfgkqn/my_friend_has_covid_and_complained_to_the_hr/hi16118/|-0.8634|1|Wtf ? What is wrong with that manager? For almost 2 years covid has been in the news everyday and he said that to a covid positive worker??!! Something seriously wrong with people nowadays! Haha !!! Crazy.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Scramble the dataset and save to a .txt file\n",
    "scrambled_text = scramble_data(df)\n",
    "scrambled_file = \"data/scrambled/scrambled-reddit-covid-comments.txt\"\n",
    "\n",
    "with open(scrambled_file, 'w') as file:\n",
    "    file.write(scrambled_text)\n",
    "\n",
    "print(f\"Scrambled text (first {NUM_ROWS} lines):\\n\")\n",
    "with open(scrambled_file, 'r') as file:\n",
    "    scrambled_data = file.readlines()\n",
    "    for line in scrambled_data[:NUM_ROWS]:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Data (first 20 rows):\n",
      "                                                 type       id subreddit.id  \\\n",
      "0                                             comment  hi0xdct        2qh7q   \n",
      "1   *for the old and/or unhealthy. Average age of ...  hi16118        2y77d   \n",
      "2                                             comment  hi1mkh7        2qhsa   \n",
      "3                                             comment  hi15pqu        2z2wm   \n",
      "4                                             comment  hi16y0z        2qqd2   \n",
      "5                                             comment  hi0wt1o        2wtmm   \n",
      "6   And tbh the vaccine is less effective than hav...  hi1sjao        2yrq6   \n",
      "7                                             comment  hi132e0        2qh1i   \n",
      "8   to www.vigiaccess.org scroll down and accept t...  hi0wo8l        2qtwb   \n",
      "9   But sadly, under normally circumstances (not a...  hi1t85o       2n4vyh   \n",
      "10  Is not the same as this paste: https://www.tra...  hi1gjs2        2tasy   \n",
      "11  One in Montreal; and one in Mississauga. The t...  hi0q4vl        2qh33   \n",
      "12  A message to all users: Please be aware that s...  hi12n1p        2r77k   \n",
      "13                                            comment  hi10jol        2qh4r   \n",
      "14                                            comment  hi19upp        2qm35   \n",
      "15                                            comment  hi0yj65        2y77d   \n",
      "16  My supervisor has responded to my call outs wi...  hi1si5f        3ax4r   \n",
      "17  Someone tells me I’m fat, I just tell them cal...  hi0d34w        2w844   \n",
      "18  About 5x as many cops died from COVID than gun...  hi0sper        2qh4r   \n",
      "19  https://www.usatoday.com/story/news/nation/202...  hi0mccb        2qh2t   \n",
      "\n",
      "           subreddit.name subreddit.nsfw created_utc  \\\n",
      "0                 florida          False  1635191579   \n",
      "1                antiwork          False  1635195040   \n",
      "2       interestingasfuck          False  1635202157   \n",
      "3           cryptomarkets          False  1635194914   \n",
      "4         greenbaypackers          False  1635195416   \n",
      "5              edcorlando          False  1635191360   \n",
      "6          publicfreakout          False  1635204886   \n",
      "7               askreddit          False  1635193856   \n",
      "8               childfree          False  1635191307   \n",
      "9           anime_titties          False  1635205204   \n",
      "10  personalfinancecanada          False  1635199494   \n",
      "11                  funny          False  1635188722   \n",
      "12                halifax          False  1635193685   \n",
      "13             conspiracy          False  1635192853   \n",
      "14                romania          False  1635196629   \n",
      "15               antiwork          False  1635192042   \n",
      "16        camgirlproblems          False  1635204870   \n",
      "17      nostupidquestions          False  1635183422   \n",
      "18             conspiracy          False  1635189728   \n",
      "19                chicago          False  1635187202   \n",
      "\n",
      "                                            permalink sentiment score  \\\n",
      "0   https://old.reddit.com/r/florida/comments/qf3x...   -0.5666    -1   \n",
      "1   https://old.reddit.com/r/antiwork/comments/qfg...   -0.8634     1   \n",
      "2   https://old.reddit.com/r/interestingasfuck/com...    0.4329     1   \n",
      "3   https://old.reddit.com/r/CryptoMarkets/comment...       0.0     1   \n",
      "4   https://old.reddit.com/r/GreenBayPackers/comme...       0.0     0   \n",
      "5   https://old.reddit.com/r/EDCOrlando/comments/q...    0.8077     3   \n",
      "6   https://old.reddit.com/r/PublicFreakout/commen...       0.0     2   \n",
      "7   https://old.reddit.com/r/AskReddit/comments/qf...    0.5106    -2   \n",
      "8   https://old.reddit.com/r/childfree/comments/qf...    0.9752    24   \n",
      "9   https://old.reddit.com/r/anime_titties/comment...   -0.8689     0   \n",
      "10  https://old.reddit.com/r/PersonalFinanceCanada...     -0.34     6   \n",
      "11  https://old.reddit.com/r/funny/comments/qfnv3e...    0.6124     1   \n",
      "12  https://old.reddit.com/r/halifax/comments/qfng...   -0.6598     1   \n",
      "13  https://old.reddit.com/r/conspiracy/comments/q...   -0.4143    12   \n",
      "14  https://old.reddit.com/r/Romania/comments/qfqi...       nan    18   \n",
      "15  https://old.reddit.com/r/antiwork/comments/qfg...     0.797     1   \n",
      "16  https://old.reddit.com/r/CamGirlProblems/comme...    0.8659     1   \n",
      "17  https://old.reddit.com/r/NoStupidQuestions/com...   -0.2023    -1   \n",
      "18  https://old.reddit.com/r/conspiracy/comments/q...   -0.0258     2   \n",
      "19  https://old.reddit.com/r/chicago/comments/qfmo...    0.5859    12   \n",
      "\n",
      "                                                 body  \n",
      "0            &gt; COVID-19 is legitimately dangerous*  \n",
      "1   Wtf ? What is wrong with that manager? For alm...  \n",
      "2   I thought the sherpas were working on that thr...  \n",
      "3   I mean…yea it was a gamble. But it’s paying my...  \n",
      "4   Fact is the “vaccine” doesn’t do anything. Fir...  \n",
      "5   Lol I’m vaccinated. But u sound like a dumbass...  \n",
      "6   This explains why he's also against the COVID ...  \n",
      "7                                                  Go  \n",
      "8   Fair question and true. Wanting is a enough of...  \n",
      "9   This pill: https://www.drugs.com/imprints/806-...  \n",
      "10              BMO has two Customer Contact Centres.  \n",
      "11                                                     \n",
      "12  I so feel you with this. Lost work from COVID,...  \n",
      "13  There are so many parents with no fucking clue...  \n",
      "14  Baga-mias pula in ea treaba, eu paranoicul pul...  \n",
      "15  I don't have COVID. But I am very sick today, ...  \n",
      "16  Trolls will attack you with anything they sens...  \n",
      "17                                  Any math on that?  \n",
      "18  There was already that story that came out a m...  \n",
      "19  steppenwolf is doing BUG again, it was amazing...  \n",
      "Cleaned data saved to data/cleaned/cleaned-reddit-covid-comments.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the scrambled .txt file (to simulate real-world scenario)\n",
    "with open(scrambled_file, 'r') as file:\n",
    "    scrambled_data = file.read()\n",
    "\n",
    "# Clean the dataset (bring it back to a clean state)\n",
    "df_cleaned, error_data = clean_data(scrambled_data)\n",
    "\n",
    "print(f\"\\nCleaned Data (first {NUM_ROWS} rows):\")\n",
    "print(df_cleaned.head(NUM_ROWS))\n",
    "\n",
    "# Step 3: Save the cleaned data back to a Parquet file\n",
    "cleaned_output_file_parquet = \"data/cleaned/cleaned-reddit-covid-comments.parquet\"\n",
    "df_cleaned.to_parquet(cleaned_output_file_parquet, index=False)\n",
    "\n",
    "# Also save the cleaned data back to a CSV for easier comparison\n",
    "cleaned_output_file = \"data/cleaned/cleaned-reddit-covid-comments.csv\"\n",
    "df_cleaned.to_csv(cleaned_output_file, index=True)\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found.\n"
     ]
    }
   ],
   "source": [
    "# Check for any errors\n",
    "if len(error_data) > 0:\n",
    "    print(f\"\\nTotal Errors Encountered: {len(error_data)}\")\n",
    "    for error in error_data:\n",
    "        print(f\"Error in line {error['line_index']}: {error['fields_found']}\")\n",
    "else:\n",
    "    print(\"No errors found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
