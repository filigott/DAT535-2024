{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with Dynamic Allocation\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final Gold Layer\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"3\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"9\") \\\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.extraPythonPackages\", \"vaderSentiment\") \\\n",
    "    .config(\"spark.driver.extraPythonPackages\", \"vaderSentiment\") \\\n",
    "    .config(\"spark.executorEnv.PYTHONPATH\", \":\".join(sys.path)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file from the silver layer\n",
    "file_path = \"hdfs://namenode:9000/data/cleaned_dataset.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# Select only columns needed for processing to minimize memory usage\n",
    "df = df.select(\"comment_id\", \"body\", \"created_utc\", \"subreddit_name\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean comments\n",
    "def clean_comment_spark(df, column):\n",
    "    return df.withColumn(\n",
    "        f\"{column}_clean\",\n",
    "        F.trim(\n",
    "            F.regexp_replace(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.lower(F.col(column)),\n",
    "                        r\"http\\S+|www\\S+|https\\S+\", \"\"),\n",
    "                    r\"@\\w+|#\", \"\"),\n",
    "                r\"[^\\w\\s]\", \"\")\n",
    "            )\n",
    "        )\n",
    "    \n",
    "df = clean_comment_spark(df, \"body\")\n",
    "\n",
    "# Cache cleaned dataset\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast Sentiment Analyzer\n",
    "analyzer_broadcast = sc.broadcast(SentimentIntensityAnalyzer())\n",
    "\n",
    "# Sentiment calculation using RDDs\n",
    "def calculate_sentiment(row):\n",
    "    analyzer = analyzer_broadcast.value\n",
    "    comment_id = row['comment_id']\n",
    "    text = row['body_clean']\n",
    "    sentiment_score = analyzer.polarity_scores(text)['compound'] if text else None\n",
    "    return (comment_id, sentiment_score)\n",
    "\n",
    "sentiment_rdd = df.rdd.map(calculate_sentiment)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"comment_id\", StringType(), True),\n",
    "    StructField(\"sentiment\", FloatType(), True)\n",
    "])\n",
    "\n",
    "sentiment_df = spark.createDataFrame(sentiment_rdd, schema)\n",
    "\n",
    "# Save only comment_id and sentiment to minimize storage\n",
    "sentiment_output_path = \"hdfs://namenode:9000/data/comment_sentiment.parquet\"\n",
    "sentiment_df.write.mode(\"overwrite\").parquet(sentiment_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use in-memory data to compute sentiment trends\n",
    "df_with_sentiment = df.join(sentiment_df, on=\"comment_id\", how=\"inner\") \\\n",
    "    .select(\"comment_id\", \"body_clean\", \"created_utc\", \"subreddit_name\", \"sentiment\")\n",
    "\n",
    "# Convert timestamp to date for trend analysis\n",
    "df_with_sentiment = df_with_sentiment.withColumn(\"date\", F.from_unixtime(F.col(\"created_utc\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Overall daily sentiment\n",
    "df_daily_sentiment_all = df_with_sentiment.groupBy(\"date\").agg(F.avg(\"sentiment\").alias(\"avg_daily_sentiment_all\"))\n",
    "\n",
    "# Daily sentiment per subreddit\n",
    "df_daily_sentiment_subreddit = df_with_sentiment.groupBy(\"date\", \"subreddit_name\") \\\n",
    "    .agg(F.avg(\"sentiment\").alias(\"avg_daily_sentiment_subreddit\"))\n",
    "\n",
    "# Join subreddit and overall trends\n",
    "df_trend_comparison = df_daily_sentiment_subreddit.join(\n",
    "    df_daily_sentiment_all, on=\"date\", how=\"left\"\n",
    ").withColumn(\n",
    "    \"sentiment_diff\",\n",
    "    F.col(\"avg_daily_sentiment_subreddit\") - F.col(\"avg_daily_sentiment_all\")\n",
    ")\n",
    "\n",
    "# Save sentiment trends with minimal columns\n",
    "trends_output_path = \"hdfs://namenode:9000/data/sentiment_trends.parquet\"\n",
    "df_trend_comparison.select(\"date\", \"subreddit_name\", \"avg_daily_sentiment_subreddit\", \"sentiment_diff\") \\\n",
    "    .write.mode(\"overwrite\").parquet(trends_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Body text processing pipeline\n",
    "# tokenizer = Tokenizer(inputCol=\"body_clean\", outputCol=\"words\")\n",
    "# stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# # TF-IDF\n",
    "# hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "# idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf])\n",
    "# model = pipeline.fit(df_with_sentiment)\n",
    "# tfidf_df = model.transform(df_with_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Body text processing pipeline with sentiment score\n",
    "tokenizer = Tokenizer(inputCol=\"body_clean\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "\n",
    "# MinMax Scaling for sentiment scores\n",
    "assembler_sentiment = VectorAssembler(inputCols=[\"sentiment\"], outputCol=\"sentiment_vector\")\n",
    "scaler = MinMaxScaler(inputCol=\"sentiment_vector\", outputCol=\"scaled_sentiment\")\n",
    "\n",
    "# Combine TF-IDF and sentiment into a single feature vector\n",
    "feature_assembler = VectorAssembler(inputCols=[\"tfidf_features\", \"scaled_sentiment\"], outputCol=\"final_features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stopwords_remover,\n",
    "    hashing_tf,\n",
    "    idf,\n",
    "    assembler_sentiment,\n",
    "    scaler,\n",
    "    feature_assembler\n",
    "])\n",
    "\n",
    "# Fit the pipeline and transform the data\n",
    "model = pipeline.fit(df_with_sentiment)\n",
    "features_df = model.transform(df_with_sentiment).select(\"subreddit_name\", \"final_features\", \"sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KMeans clustering\n",
    "# kmeans = KMeans(k=5, seed=123)\n",
    "# model = kmeans.fit(tfidf_df)\n",
    "# predictions = model.transform(tfidf_df)\n",
    "\n",
    "# # Cache clustering predictions\n",
    "# predictions.cache()\n",
    "\n",
    "# # Sentiment analysis by cluster\n",
    "# sentiment_analysis = predictions.groupBy(\"prediction\").agg(\n",
    "#     F.mean(\"sentiment\").alias(\"average_sentiment\"),\n",
    "#     F.count(\"subreddit_name\").alias(\"subreddit_count\")\n",
    "# )\n",
    "\n",
    "# # Clustering evaluation\n",
    "# evaluator = ClusteringEvaluator(featuresCol=\"features\")\n",
    "# silhouette = evaluator.evaluate(predictions)\n",
    "# print(f\"Silhouette Score: {silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans clustering\n",
    "kmeans = KMeans(featuresCol=\"final_features\", k=5, seed=123)\n",
    "kmeans_model = kmeans.fit(features_df)\n",
    "predictions = kmeans_model.transform(features_df).select(\"subreddit_name\", \"sentiment\", \"prediction\")\n",
    "\n",
    "# Sentiment analysis by cluster\n",
    "sentiment_analysis = predictions.groupBy(\"prediction\").agg(\n",
    "    F.mean(\"sentiment\").alias(\"average_sentiment\"),\n",
    "    F.count(\"subreddit_name\").alias(\"subreddit_count\")\n",
    ")\n",
    "\n",
    "# Clustering evaluation\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"final_features\")\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette Score: {silhouette}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustering results\n",
    "cluster_output_path = \"hdfs://namenode:9000/data/clustered_sentiment_text_results.parquet\"\n",
    "predictions.write.mode(\"overwrite\").parquet(cluster_output_path)\n",
    "\n",
    "# Save summarized sentiment analysis by cluster\n",
    "cluster_summary_output_path = \"hdfs://namenode:9000/data/cluster_summary_text_sentiment.parquet\"\n",
    "sentiment_analysis.write.mode(\"overwrite\").parquet(cluster_summary_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
