{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate Jupyter Notebook for Spark-based Data Pipeline Project\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType, FloatType, IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Part 1: Data Ingestion (Bronze Layer)\n",
    "# -------------------------------------\n",
    "\n",
    "# Initialize Spark\n",
    "conf = SparkConf().setAppName(\"E-commerce Clickstream Analysis\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Data Ingestion: Load the raw data (CSV, JSON, etc.) from cloud storage or local file system\n",
    "# Example: Loading a CSV file from local storage\n",
    "raw_data_path = \"/path/to/your/dataset.csv\"  # Modify this path\n",
    "raw_df = spark.read.option(\"header\", \"true\").csv(raw_data_path)\n",
    "\n",
    "# Display raw data sample\n",
    "print(\"Raw Data Sample:\")\n",
    "raw_df.show(5)\n",
    "\n",
    "# Save raw data into the Bronze layer (Uncleaned/Raw Data)\n",
    "# Optionally, you could save this to cloud storage, but for now, we'll store it locally\n",
    "raw_data_bronze_path = \"/path/to/bronze_layer/\"\n",
    "raw_df.write.mode(\"overwrite\").parquet(raw_data_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Part 2: Data Cleaning (Silver Layer)\n",
    "# -------------------------------------\n",
    "\n",
    "# Define cleaning and transformation functions (using Spark RDDs and MapReduce approach)\n",
    "\n",
    "# Function to clean and process the raw data\n",
    "def clean_data(line):\n",
    "    fields = line.split(\",\")  # Assuming the data is CSV, adjust as needed\n",
    "    try:\n",
    "        # Extract relevant fields\n",
    "        # For example, assume fields like id, subreddit, body, sentiment, etc.\n",
    "        id = fields[0]\n",
    "        subreddit_id = fields[1]\n",
    "        subreddit_name = fields[2]\n",
    "        nsfw = fields[3]\n",
    "        created_utc = int(fields[4])  # Convert to integer\n",
    "        permalink = fields[5]\n",
    "        body = fields[6]\n",
    "        sentiment = float(fields[7])  # Convert to float\n",
    "        score = int(fields[8])  # Convert to integer\n",
    "\n",
    "        # Return cleaned data\n",
    "        return (id, subreddit_id, subreddit_name, nsfw, created_utc, permalink, body, sentiment, score)\n",
    "    except Exception as e:\n",
    "        return None  # Skip bad rows\n",
    "\n",
    "# Apply cleaning function using Spark RDD\n",
    "raw_rdd = sc.textFile(raw_data_bronze_path)\n",
    "cleaned_rdd = raw_rdd.map(clean_data).filter(lambda x: x is not None)\n",
    "\n",
    "# Convert cleaned RDD back to DataFrame\n",
    "cleaned_df = cleaned_rdd.toDF([\"id\", \"subreddit_id\", \"subreddit_name\", \"nsfw\", \"created_utc\", \"permalink\", \"body\", \"sentiment\", \"score\"])\n",
    "\n",
    "# Display cleaned data sample\n",
    "print(\"Cleaned Data Sample:\")\n",
    "cleaned_df.show(5)\n",
    "\n",
    "# Save cleaned data to the Silver layer\n",
    "cleaned_data_silver_path = \"/path/to/silver_layer/\"\n",
    "cleaned_df.write.mode(\"overwrite\").parquet(cleaned_data_silver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Part 3: Data Serving (Gold Layer)\n",
    "# -------------------------------------\n",
    "\n",
    "# Business Use Case 1: Sentiment Analysis (Example using SQL)\n",
    "# Load the cleaned data from the Silver layer\n",
    "cleaned_df = spark.read.parquet(cleaned_data_silver_path)\n",
    "\n",
    "# For example, let's find the average sentiment by subreddit\n",
    "cleaned_df.createOrReplaceTempView(\"reddit_comments\")\n",
    "\n",
    "# Perform sentiment analysis using SQL\n",
    "sentiment_analysis_df = spark.sql(\"\"\"\n",
    "    SELECT subreddit_name, AVG(sentiment) as avg_sentiment, COUNT(*) as comment_count\n",
    "    FROM reddit_comments\n",
    "    GROUP BY subreddit_name\n",
    "    HAVING COUNT(*) > 50  -- Subreddits with more than 50 comments\n",
    "    ORDER BY avg_sentiment DESC\n",
    "\"\"\")\n",
    "\n",
    "# Display sentiment analysis results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "sentiment_analysis_df.show(10)\n",
    "\n",
    "# Save the final data to the Gold layer (for further analysis or serving to a dashboard)\n",
    "final_data_gold_path = \"/path/to/gold_layer/\"\n",
    "sentiment_analysis_df.write.mode(\"overwrite\").parquet(final_data_gold_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Part 4: Optional - Machine Learning Model (Bonus)\n",
    "# -------------------------------------\n",
    "\n",
    "# Example: Building a simple model to predict whether a comment will have a high score (classification task)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create features column (example: based on sentiment, created_utc)\n",
    "assembler = VectorAssembler(inputCols=[\"sentiment\", \"created_utc\"], outputCol=\"features\")\n",
    "ml_df = assembler.transform(cleaned_df)\n",
    "\n",
    "# Define a binary label (e.g., high score or not)\n",
    "ml_df = ml_df.withColumn(\"label\", (col(\"score\") > 5).cast(IntegerType()))\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(ml_df)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model.transform(ml_df)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model for future use\n",
    "model_path = \"/path/to/saved_model/\"\n",
    "lr_model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Clean up Spark resources\n",
    "# -------------------------------------\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
