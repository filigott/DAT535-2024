{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Sentiment Analysis\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the Parquet file\n",
    "file_path = \"data/the-reddit-covid-comments-sample.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- subreddit_name: string (nullable = true)\n",
      " |-- subreddit_nsfw: string (nullable = true)\n",
      " |-- created_utc: integer (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- sentiment: double (nullable = false)\n",
      " |-- score: integer (nullable = true)\n",
      "\n",
      "+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "|   type|     id|subreddit_id|subreddit_name|subreddit_nsfw|created_utc|           permalink|                body|sentiment|score|\n",
      "+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "|comment|hi1vsag|       2riyy|          nova|         false| 1635206399|https://old.reddi...|When you schedule...|      0.0|    2|\n",
      "|comment|hi1vs7i|       2qhov|     vancouver|         false| 1635206397|https://old.reddi...|Didn't stop price...|   0.1887|   32|\n",
      "|comment|hi1vs5n|       2qwzb|      pregnant|         false| 1635206397|https://old.reddi...|Iâ€™m just waiting ...|    0.672|    1|\n",
      "|comment|hi1vs5v|       2qixm|      startrek|         false| 1635206397|https://old.reddi...|*The first duty o...|   0.9562|    1|\n",
      "|comment|hi1vs0l|       2qsf3|       ontario|         false| 1635206395|https://old.reddi...|Compare BC to Ont...|      0.0|   -2|\n",
      "+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming columns for clarity\n",
    "df = df.withColumnRenamed(\"subreddit.id\", \"subreddit_id\")\n",
    "df = df.withColumnRenamed(\"subreddit.name\", \"subreddit_name\")\n",
    "df = df.withColumnRenamed(\"subreddit.nsfw\", \"subreddit_nsfw\")\n",
    "\n",
    "# Calculate the mean sentiment\n",
    "mean_sentiment = df.agg(F.avg(\"sentiment\")).first()[0]\n",
    "\n",
    "# Fill null values in the sentiment column with the mean\n",
    "df = df.fillna({'sentiment': mean_sentiment})\n",
    "\n",
    "# Show the DataFrame schema and first few rows\n",
    "df.printSchema()  # Print the schema of the filled DataFrame\n",
    "df.show(5)        # Display the first few rows of the filled DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1777747"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average sentiment Per Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|  subreddit_name|       avg_sentiment|\n",
      "+----------------+--------------------+\n",
      "|          travel| 0.34720585424110667|\n",
      "|            snhu| 0.22637999999999997|\n",
      "|      traderjoes|  0.2185318181818182|\n",
      "|debitismus_forum|-0.04006802554303...|\n",
      "|       methadone| 0.14535849673202614|\n",
      "|           anime| 0.24444842555210985|\n",
      "|    couchsurfing| 0.42876000000000003|\n",
      "|   gastricsleeve|  0.2631261174590532|\n",
      "|          scjerk|-0.00946678554058...|\n",
      "|   crohnsdisease| 0.10366811023622047|\n",
      "+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Average Sentiment per Subreddit\n",
    "avg_sentiment_per_subreddit = df.groupBy(\"subreddit_name\").agg(\n",
    "    F.avg(\"sentiment\").alias(\"avg_sentiment\")\n",
    ")\n",
    "avg_sentiment_per_subreddit.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Average Sentiment per Subreddit\n",
    "df = df.groupBy(\"subreddit_name\").agg(\n",
    "    F.avg(\"sentiment\").alias(\"avg_sentiment\"),\n",
    "    F.count(\"id\").alias(\"comment_count\"),  # Count of posts per subreddit\n",
    "    F.expr(\"percentile_approx(sentiment, 0.5)\").alias(\"median_sentiment\")  # Median sentiment\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+----------------+\n",
      "|      subreddit_name|avg_sentiment|comment_count|median_sentiment|\n",
      "+--------------------+-------------+-------------+----------------+\n",
      "|u_rowan-the-girlf...|       0.9999|            1|          0.9999|\n",
      "|       thegreatgasly|       0.9997|            1|          0.9997|\n",
      "|              umiami|       0.9997|            1|          0.9997|\n",
      "|               puffy|       0.9996|            1|          0.9996|\n",
      "|     u_mylastchapter|       0.9995|            1|          0.9995|\n",
      "|               bones|       0.9995|            1|          0.9995|\n",
      "|          irishmusic|       0.9995|            1|          0.9995|\n",
      "|             vieques|       0.9995|            1|          0.9995|\n",
      "|           dcuonline|       0.9994|            1|          0.9994|\n",
      "|       hotspringspas|       0.9994|            1|          0.9994|\n",
      "+--------------------+-------------+-------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the top 10 subreddits with the average sentiment and post counts\n",
    "df.orderBy(\"avg_sentiment\", ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-------------+----------------+\n",
      "|     subreddit_name|avg_sentiment|comment_count|median_sentiment|\n",
      "+-------------------+-------------+-------------+----------------+\n",
      "|         anticommie|      -0.9998|            2|         -0.9998|\n",
      "|   falseadvertising|      -0.9992|            1|         -0.9992|\n",
      "|        whitepride_|      -0.9992|            1|         -0.9992|\n",
      "|   asksocialscience|      -0.9991|            1|         -0.9991|\n",
      "|   covidateyourface|       -0.999|            1|          -0.999|\n",
      "|          artadvice|     -0.99895|            2|         -0.9991|\n",
      "|          meatballs|      -0.9989|            1|         -0.9989|\n",
      "|          atfopenup|      -0.9985|            1|         -0.9985|\n",
      "|shittymobilegameads|      -0.9985|            1|         -0.9985|\n",
      "|  iforgorthesubname|      -0.9985|            1|         -0.9985|\n",
      "+-------------------+-------------+-------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the top 10 subreddits with the average sentiment and post counts\n",
    "df.orderBy(\"avg_sentiment\", ascending=True).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Filter out subreddits with fewer than a certain number of posts for more meaningful analysis\n",
    "min_comment_count = 50  # Change this threshold as needed\n",
    "filtered_avg_sentiment = df.filter(F.col(\"comment_count\") >= min_comment_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:======================================>                 (11 + 5) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------+----------------+\n",
      "|   subreddit_name|     avg_sentiment|comment_count|median_sentiment|\n",
      "+-----------------+------------------+-------------+----------------+\n",
      "|         r4rasian|0.9680937308868541|         1308|          0.9712|\n",
      "|         abortion|0.9418763432237328|         1247|          0.9688|\n",
      "|         startrek| 0.938179064056261|         2515|          0.9562|\n",
      "|baltimoreanddcr4r| 0.928099999999983|         4094|          0.9281|\n",
      "|          bourbon|0.9043954110898693|         1046|          0.9704|\n",
      "|           borrow|0.8941376623376622|           77|          0.9565|\n",
      "|  healthinsurance|0.8833896805896826|          814|           0.953|\n",
      "|           bizsmg|0.8433243512974052|          501|          0.9969|\n",
      "|     irishtourism|0.7979388392857141|          224|          0.9454|\n",
      "|       nycmeetups|0.7672459016393461|          610|          0.7685|\n",
      "+-----------------+------------------+-------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the filtered results\n",
    "filtered_avg_sentiment.orderBy(\"avg_sentiment\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+----------------+\n",
      "|      subreddit_name|       avg_sentiment|comment_count|median_sentiment|\n",
      "+--------------------+--------------------+-------------+----------------+\n",
      "|        covid19_ohio| -0.4679525575253793|          179|         -0.8624|\n",
      "|         letterkenny| -0.3409493463543523|          453|         -0.6191|\n",
      "|      redditsecurity|  -0.338291935483871|           62|         -0.5994|\n",
      "|        gradeaundera| -0.3330597560975609|          164|         -0.4939|\n",
      "|           flatearth| -0.3312639061310989|           65|         -0.6249|\n",
      "| subredditsummarybot|-0.32729189615137366|         1196|         -0.6124|\n",
      "|hermancainawardsucks|-0.30271234767714283|          195|         -0.6059|\n",
      "|   politicalopinions| -0.2965837837837837|           74|         -0.5778|\n",
      "|      hankaaronaward| -0.2892778910002566|          334|            -0.5|\n",
      "|    coronavirusidaho|           -0.288692|           50|         -0.4939|\n",
      "+--------------------+--------------------+-------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_avg_sentiment.orderBy(\"avg_sentiment\", ascending=True).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "per_subreddit_stats_path = \"data/results/per_subreddit_stats.parquet\"\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(per_subreddit_stats_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-------------+--------------------+\n",
      "|subreddit_name|       avg_sentiment|comment_count|    median_sentiment|\n",
      "+--------------+--------------------+-------------+--------------------+\n",
      "| 12step_escape|              0.2763|            1|              0.2763|\n",
      "|  1500isplenty| 0.46612142857142863|           14|              0.6222|\n",
      "|         18_19|              0.8338|            1|              0.8338|\n",
      "|        2000ad|             -0.4215|            1|             -0.4215|\n",
      "|       2asia4u|  0.1213054160099026|           44|              0.0516|\n",
      "|   2balkan4you| -0.1201626082922619|           36|             -0.4717|\n",
      "|          350z|              0.4064|            9|                0.34|\n",
      "|      3atatime|0.010246101478571794|            1|0.010246101478571794|\n",
      "|         4chan|  -0.164401238990117|         1087|             -0.2023|\n",
      "|          4ktv| 0.22467826086956522|           23|              0.4215|\n",
      "+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23265"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File path for the Parquet file\n",
    "file_path = \"data/results/sentiment_calculations.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df_result = spark.read.parquet(file_path)\n",
    "\n",
    "df.show(10)\n",
    "df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
