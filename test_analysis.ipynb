{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime, avg, date_format, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Sentiment Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/cleaned/cleaned-reddit-covid-comments.csv\"\n",
    "parquet_file_path = \"data/cleaned/cleaned-reddit-covid-comments.parquet\" \n",
    "df = spark.read.option(\"header\", \"true\").parquet(parquet_file_path, inferSchema=True)\n",
    "#df = df.withColumn(\"date\", date_format(col(\"created_utc\"), \"yyyy-MM-dd\"))\n",
    "#df = df.withColumn(\"sentiment\", col(\"sentiment\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Group by subreddit_name, calculate the average sentiment, and count the number of comments\n",
    "result_df = df.groupBy(col(\"subreddit_name\").alias(\"subreddit_name\")) \\\n",
    "    .agg(\n",
    "        avg(\"sentiment\").alias(\"average_sentiment\"),\n",
    "        count(\"id\").alias(\"comment_count\")  # Count the number of comments per subreddit\n",
    "    ) \\\n",
    "    .filter(col(\"comment_count\") >= 100).orderBy(col(\"average_sentiment\").asc()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select rows where subreddit_name is \"conspiracy\"\n",
    "\n",
    "conspiracy_df = df.filter(col(\"subreddit_name\") == \"conspiracy\")\n",
    "conspiracy_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'\\@w+|\\#','', text)  # Remove @mentions and #hashtags\n",
    "    text = re.sub(r'\\s+', ' ', text)    # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    return text\n",
    "\n",
    "clean_text_udf = F.udf(clean_comment, F.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to clean the comments\n",
    "cleaned_comments_df = df.withColumn('cleaned_comment', clean_text_udf(F.col('body')))\n",
    "\n",
    "# Show the cleaned comments\n",
    "cleaned_comments_df.select('body', 'cleaned_comment').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return float(score['compound'])\n",
    "\n",
    "sentiment_udf = F.udf(get_vader_sentiment, FloatType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_with_sentiment = cleaned_comments_df.withColumn('sentimentTest', sentiment_udf(F.col('cleaned_comment')))\n",
    "\n",
    "# Show the result\n",
    "comments_with_sentiment.show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
