{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler, PCA\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/31 21:19:44 WARN Utils: Your hostname, FiligottLaptop resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/31 21:19:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/31 21:19:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Sentiment Analysis\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# File path for the Parquet file\n",
    "file_path = \"data/the-reddit-covid-comments-sample.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- subreddit_name: string (nullable = true)\n",
      " |-- subreddit_nsfw: string (nullable = true)\n",
      " |-- created_utc: integer (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- sentiment: double (nullable = false)\n",
      " |-- score: integer (nullable = true)\n",
      "\n",
      "+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "|   type|     id|subreddit_id|subreddit_name|subreddit_nsfw|created_utc|           permalink|                body|sentiment|score|\n",
      "+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "|comment|hi1vsag|       2riyy|          nova|         false| 1635206399|https://old.reddi...|When you schedule...|      0.0|    2|\n",
      "|comment|hi1vs7i|       2qhov|     vancouver|         false| 1635206397|https://old.reddi...|Didn't stop price...|   0.1887|   32|\n",
      "|comment|hi1vs5n|       2qwzb|      pregnant|         false| 1635206397|https://old.reddi...|Iâ€™m just waiting ...|    0.672|    1|\n",
      "|comment|hi1vs5v|       2qixm|      startrek|         false| 1635206397|https://old.reddi...|*The first duty o...|   0.9562|    1|\n",
      "|comment|hi1vs0l|       2qsf3|       ontario|         false| 1635206395|https://old.reddi...|Compare BC to Ont...|      0.0|   -2|\n",
      "+-------+-------+------------+--------------+--------------+-----------+--------------------+--------------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming columns for clarity\n",
    "df = df.withColumnRenamed(\"subreddit.id\", \"subreddit_id\")\n",
    "df = df.withColumnRenamed(\"subreddit.name\", \"subreddit_name\")\n",
    "df = df.withColumnRenamed(\"subreddit.nsfw\", \"subreddit_nsfw\")\n",
    "\n",
    "# Calculate the mean sentiment\n",
    "mean_sentiment = df.agg(F.avg(\"sentiment\")).first()[0]\n",
    "\n",
    "# Fill null values in the sentiment column with the mean\n",
    "df_filled_mean = df.fillna({'sentiment': mean_sentiment})\n",
    "\n",
    "# Show the DataFrame schema and first few rows\n",
    "df_filled_mean.printSchema()  # Print the schema of the filled DataFrame\n",
    "df_filled_mean.show(5)        # Display the first few rows of the filled DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1777747"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize and remove stopwords\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pipeline for processing\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf])\n",
    "model = pipeline.fit(df)\n",
    "tfidf_df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Average TF-IDF Vector per Subreddit\n",
    "# Summarizer provides a way to calculate mean for vector columns\n",
    "subreddit_vectors = tfidf_df.groupBy(\"subreddit_name\").agg(\n",
    "    Summarizer.mean(F.col(\"features\")).alias(\"tfidf_vector\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = float(vec1.dot(vec2))\n",
    "    norm1 = float(np.sqrt(vec1.dot(vec1)))\n",
    "    norm2 = float(np.sqrt(vec2.dot(vec2)))\n",
    "    return dot_product / (norm1 * norm2) if norm1 != 0 and norm2 != 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDF\n",
    "cosine_similarity_udf = F.udf(cosine_similarity, T.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Self-join with distinct aliases for each subreddit name\n",
    "# subreddit_pairs = subreddit_vectors.alias(\"a\") \\\n",
    "#     .crossJoin(subreddit_vectors.alias(\"b\")) \\\n",
    "#     .filter(F.col(\"a.subreddit_name\") < F.col(\"b.subreddit_name\")) \\\n",
    "#     .withColumn(\"cosine_similarity\", cosine_similarity_udf(F.col(\"a.tfidf_vector\"), F.col(\"b.tfidf_vector\"))) \\\n",
    "#     .select(\n",
    "#         F.col(\"a.subreddit_name\").alias(\"subreddit_name_a\"),\n",
    "#         F.col(\"b.subreddit_name\").alias(\"subreddit_name_b\"),\n",
    "#         F.col(\"cosine_similarity\")\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter pairs with high similarity for easier analysis\n",
    "# similar_subreddits = subreddit_pairs.filter(F.col(\"cosine_similarity\") > 0.8)\n",
    "\n",
    "# # Show similar subreddit pairs, ordering by cosine similarity\n",
    "# similar_subreddits \\\n",
    "#     .select(\"subreddit_name_a\", \"subreddit_name_b\", \"cosine_similarity\") \\\n",
    "#     .orderBy(F.col(\"cosine_similarity\").desc()) \\\n",
    "#     .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'tfidf_vector' to 'features'\n",
    "subreddit_vectors = subreddit_vectors.withColumnRenamed(\"tfidf_vector\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming 'subreddit_vectors' contains the TF-IDF vectors for each subreddit\n",
    "kmeans = KMeans(k=5, seed=123)  # You can change the number of clusters (k) as needed\n",
    "\n",
    "# Fit the model\n",
    "model = kmeans.fit(subreddit_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(subreddit_vectors)\n",
    "\n",
    "predictions = predictions.join(df_filled_mean.select(\"subreddit_name\", \"sentiment\"), on=\"subreddit_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|subreddit_name|prediction|\n",
      "+--------------+----------+\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|  1500isplenty|         0|\n",
      "|         18_19|         0|\n",
      "|        2000ad|         0|\n",
      "|   2balkan4you|         0|\n",
      "|   2balkan4you|         0|\n",
      "|   2balkan4you|         0|\n",
      "|   2balkan4you|         0|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the predictions (clusters assigned to each subreddit)\n",
    "predictions.select(\"subreddit_name\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sentiment` cannot be resolved. Did you mean one of the following? [`features`, `prediction`, `subreddit_name`].;\n'Aggregate [prediction#795], [prediction#795, avg('sentiment) AS average_sentiment#1734, count(subreddit_name#32) AS subreddit_count#1736L]\n+- Project [subreddit_name#32, features#300, UDF(features#300) AS prediction#795]\n   +- Project [subreddit_name#32, tfidf_vector#297 AS features#300]\n      +- Aggregate [subreddit_name#32], [subreddit_name#32, aggregate_metrics(Mean, ComputeMean, ComputeWeightSum, features#263, 1.0, 0, 0).mean AS tfidf_vector#297]\n         +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9, words#210, filtered_words#228, raw_features#246, UDF(raw_features#246) AS features#263]\n            +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9, words#210, filtered_words#228, UDF(filtered_words#228) AS raw_features#246]\n               +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9, words#210, UDF(words#210) AS filtered_words#228]\n                  +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9, UDF(body#7) AS words#210]\n                     +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit.nsfw#4 AS subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9]\n                        +- Project [type#0, id#1, subreddit_id#20, subreddit.name#3 AS subreddit_name#32, subreddit.nsfw#4, created_utc#5, permalink#6, body#7, sentiment#8, score#9]\n                           +- Project [type#0, id#1, subreddit.id#2 AS subreddit_id#20, subreddit.name#3, subreddit.nsfw#4, created_utc#5, permalink#6, body#7, sentiment#8, score#9]\n                              +- Relation [type#0,id#1,subreddit.id#2,subreddit.name#3,subreddit.nsfw#4,created_utc#5,permalink#6,body#7,sentiment#8,score#9] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 5: Analyze Sentiment by Cluster\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sentiment_analysis \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maverage_sentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubreddit_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubreddit_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m sentiment_analysis\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/sql/group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[0;32m~/dat535-2024/project/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/dat535-2024/project/venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sentiment` cannot be resolved. Did you mean one of the following? [`features`, `prediction`, `subreddit_name`].;\n'Aggregate [prediction#795], [prediction#795, avg('sentiment) AS average_sentiment#1734, count(subreddit_name#32) AS subreddit_count#1736L]\n+- Project [subreddit_name#32, features#300, UDF(features#300) AS prediction#795]\n   +- Project [subreddit_name#32, tfidf_vector#297 AS features#300]\n      +- Aggregate [subreddit_name#32], [subreddit_name#32, aggregate_metrics(Mean, ComputeMean, ComputeWeightSum, features#263, 1.0, 0, 0).mean AS tfidf_vector#297]\n         +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9, words#210, filtered_words#228, raw_features#246, UDF(raw_features#246) AS features#263]\n            +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9, words#210, filtered_words#228, UDF(filtered_words#228) AS raw_features#246]\n               +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9, words#210, UDF(words#210) AS filtered_words#228]\n                  +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9, UDF(body#7) AS words#210]\n                     +- Project [type#0, id#1, subreddit_id#20, subreddit_name#32, subreddit.nsfw#4 AS subreddit_nsfw#43, created_utc#5, permalink#6, body#7, sentiment#8, score#9]\n                        +- Project [type#0, id#1, subreddit_id#20, subreddit.name#3 AS subreddit_name#32, subreddit.nsfw#4, created_utc#5, permalink#6, body#7, sentiment#8, score#9]\n                           +- Project [type#0, id#1, subreddit.id#2 AS subreddit_id#20, subreddit.name#3, subreddit.nsfw#4, created_utc#5, permalink#6, body#7, sentiment#8, score#9]\n                              +- Relation [type#0,id#1,subreddit.id#2,subreddit.name#3,subreddit.nsfw#4,created_utc#5,permalink#6,body#7,sentiment#8,score#9] parquet\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Analyze Sentiment by Cluster\n",
    "sentiment_analysis = predictions.groupBy(\"prediction\").agg(\n",
    "    F.mean(\"sentiment\").alias(\"average_sentiment\"),\n",
    "    F.count(\"subreddit_name\").alias(\"subreddit_count\")\n",
    ").orderBy(\"average_sentiment\")\n",
    "\n",
    "sentiment_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:================================================>      (15 + 2) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9250503133607962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette with squared euclidean distance = {silhouette}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save the results\n",
    "output_path = \"data/results/similar_subreddits_by_text.parquet\"\n",
    "similar_subreddits.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark.read.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample results\n",
    "df_read.show(10)\n",
    "print(\"Total Count:\", df_read.count())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
